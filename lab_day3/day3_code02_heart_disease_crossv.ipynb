{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCbL-3UsFO3l"
      },
      "source": [
        "# You are breaking my heart\n",
        "\n",
        "This dataset contains information on 303 patients. Several medically relevant data are available (age, sex, cholesterol, resting blood pressure...). Our task is to predict the presence of heart disease (column \"target\", 0 means healty, 1 means sick).\n",
        "\n",
        "This dataset is described in detail:\n",
        "\n",
        "* on Kaggle datasets: https://www.kaggle.com/ronitf/heart-disease-uci\n",
        "* on its original webpage: https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
        "\n",
        "I've downloaded a copy of the data and made it available at the following url:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKkpvbRpxMY7"
      },
      "source": [
        "DATASET_URL = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/datasets_33180_43520_heart.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zNple3YGNn"
      },
      "source": [
        "# Battleplan\n",
        "\n",
        "* load dataset\n",
        "* a bit of data visualization/exploration\n",
        "* a baseline classifier: logistic regression\n",
        "* improve the data, improve the classifier\n",
        "* a better NN classifier: let's add a layer\n",
        "  * doing a proper crossvalidation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whmOWIRgR15i"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0ryu2cR3OS"
      },
      "source": [
        "#let's fix already our desired number of epochs\n",
        "EPOCHS = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OECkDKCOHdl_"
      },
      "source": [
        "To be sure to have the same results, we can fix the random seeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAv0wbk_HksK"
      },
      "source": [
        "#resetting the seeds\n",
        "!wget -O support_code.py https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/lab_day1/support_code.py\n",
        "%run support_code.py\n",
        "n1 = 10\n",
        "reset_random_seeds(n1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQLaXLYf0Fh6"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R90Prp_wAF-Z"
      },
      "source": [
        "import pandas\n",
        "\n",
        "#pandas can read a csv directly from a url\n",
        "heart_data = pandas.read_csv(DATASET_URL)\n",
        "print(heart_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oorqzi51Cjw3"
      },
      "source": [
        "#splitting features and target\n",
        "features = heart_data.iloc[:,:-1] ## takes all columns except the last one (starts counting from 0)\n",
        "target = heart_data.iloc[:,-1] ## takes last column (starts counting from 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KwlarWPInSz"
      },
      "source": [
        "#take a look at what we have done\n",
        "print(heart_data.columns)\n",
        "print(features.shape)\n",
        "print(target.shape) #beware of rank 1 arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ISOT2GBg66"
      },
      "source": [
        "## Train and Validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkudbVoiBmI4"
      },
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, target):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target[train_index]\n",
        "    target_val     = target[val_index]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSUQjM0xAevZ"
      },
      "source": [
        "# Baseline predictor: logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pbfwsHGDry5"
      },
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT. In other words, there\n",
        "#we will not be any loops, i.e. our output will never be recycled as\n",
        "#input for the first layer. That would make the architecture\n",
        "#recurrent, which we don't want since the data is tabular and\n",
        "#recurrent networks are more suited for streams and sequences of data\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes. In our case there is only one node in the layer, and\n",
        "#it receives all the features.\n",
        "#Doc: https://keras.io/api/layers/core_layers/dense/\n",
        "from keras.layers import Dense\n",
        "#Doc: https://keras.io/api/layers/core_layers/input/\n",
        "from keras.layers import Input\n",
        "\n",
        "\n",
        "#this is optional: we'll add some extra metrics which are more human\n",
        "#readable than crossentropy, so to have a practical feeling about\n",
        "#how well the model is doing\n",
        "import keras.metrics\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model = Sequential()\n",
        "model.add(Input((features_train.shape[1],))) #13\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the internal data structures. Moreover, we still need to\n",
        "#specify optimizer, loss function and, optionally, extra metrics\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "      keras.metrics.BinaryAccuracy(),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BPlTywjtoiK"
      },
      "source": [
        "At this point we are ready to train. You may have noticed that so far we just described the architecture. It's now time to have the network meed train and validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8OZBvPnEBjE"
      },
      "source": [
        "#doc: https://keras.io/api/models/model_training_apis/#fit-method\n",
        "history = model.fit(\n",
        "    x = features_train,\n",
        "    y = target_train,\n",
        "    epochs = 10,\n",
        "    validation_data = (features_val, target_val),\n",
        "    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-LmEK9hIPNQ"
      },
      "source": [
        "#Taking a look to what's inside the returned history dictionary\n",
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AZztca9EGXq"
      },
      "source": [
        "#function to plot losses and metrics evolution\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_loss_history(h, title):\n",
        "  for metric in h.history.keys():\n",
        "    #ignoring metrics on validation set, which are implied when\n",
        "    #plotting on training set\n",
        "    if metric.startswith('val_'):\n",
        "      continue\n",
        "\n",
        "    #if we get here we found a metric on the training set,\n",
        "    #let's plot it\n",
        "    plt.plot(h.history[metric], label = \"Train set\")\n",
        "    plt.plot(h.history[\"val_\" + metric], label = \"Validation set\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title + ' - ' + metric)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h719dXPAEJCy"
      },
      "source": [
        "plot_loss_history(history, 'Logistic (10 epochs)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhlpWS5BEaCR"
      },
      "source": [
        "#putting verbose to 0 to avoid filling the screen\n",
        "history2 = model.fit(features_train, target_train, epochs=(EPOCHS - 10),\n",
        "                     validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPFR7y3Ed5W"
      },
      "source": [
        "#putting together the whole history\n",
        "for k in history.history.keys():\n",
        "  history.history[k] += history2.history[k]\n",
        "\n",
        "#and plotting again\n",
        "plot_loss_history(history, 'Logistic (' + str(EPOCHS) + ' epochs)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s928wjyvQw7"
      },
      "source": [
        "Taking a look to the [confusion matrix](https://www.analyticssteps.com/blogs/what-confusion-matrix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1RfTZ8wS0_8"
      },
      "source": [
        "#sklearn provides a function to compute the confusion\n",
        "#matrix given two numpy arrays: the true labels and\n",
        "#the predicted labels\n",
        "#doc: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#first: let's get the predictions for our validation set\n",
        "predictions = model.predict(features_val)\n",
        "\n",
        "#taking a look at the first few ones\n",
        "print(predictions[1:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmDbgRA1wSyR"
      },
      "source": [
        "At this stage predictions are numbers in the [0,1] range. That's expected, since it's the normal output of a sigmoid (a.k.a. a logistic function).\n",
        "\n",
        "To change them in labels (healty/sick) we need to discretyze. Everything above 0.5 goes to one, everything below goes to zero. In this way it will be comparable to true labels, which are naturally in the {zero, one} set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tyVH96pv_Gb"
      },
      "source": [
        "import numpy\n",
        "\n",
        "#using numpy \"where\" function to do substitutions\n",
        "#doc: https://numpy.org/doc/stable/reference/generated/numpy.where.html\n",
        "predicted_labels = numpy.where(predictions > 0.5, 1, 0)\n",
        "\n",
        "#computing and printing the matrix\n",
        "con_mat_df = confusion_matrix(target_val, predicted_labels)\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t0i7TBSAoa2"
      },
      "source": [
        "# Improvement: data normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDGtI4Am1IAx"
      },
      "source": [
        "#getting an idea about features averages, sd\n",
        "avg = features_train.mean()\n",
        "std = features_train.std()\n",
        "print('Feature means')\n",
        "print(avg)\n",
        "print('\\nFeature standard deviations')\n",
        "print(std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_UHLi322CCV"
      },
      "source": [
        "#IMPORTANT: normalizing features using the same weights for both\n",
        "#train and validation sets. This is the hardest condition. For a\n",
        "#simpler problem compute avg and std on the full dataset.\n",
        "#QUESTION: why?\n",
        "features_train = (features_train - avg)/std\n",
        "features_val = (features_val - avg)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWvJ7KTArHi"
      },
      "source": [
        "# Improvement: class balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYzLcD0a2yt4"
      },
      "source": [
        "#comparing the number of samples for each class\n",
        "N_TOT = target_train.shape[0]\n",
        "N_DISEASE = target_train.sum()\n",
        "N_NORMAL = N_TOT - N_DISEASE\n",
        "\n",
        "print('Total samples: ' + str(N_TOT) + ' (normal:' + str(N_NORMAL) + ' diseases:' + str(N_DISEASE) + ')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSdQP8Xszzoo"
      },
      "source": [
        "We propose a \"in-house\" class balancing function. There's a more complicated (and effective) one in sklearn called [compute_class_weight()](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7CZtUmV30yr"
      },
      "source": [
        "#https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit and\n",
        "#whatch for argument class_weight\n",
        "\n",
        "#handy function, to be reused later\n",
        "def get_weights(target):\n",
        "  #counting the number of instances for each class\n",
        "  class_1_cnt = target_train.sum()\n",
        "  class_0_cnt = target.shape[0] - target_train.sum()\n",
        "\n",
        "  #we get weights as inverse of class ratios\n",
        "  weight_for_0 = (class_0_cnt + class_1_cnt) / class_0_cnt\n",
        "  weight_for_1 = (class_0_cnt + class_1_cnt) / class_1_cnt\n",
        "\n",
        "  #and we are done\n",
        "  return({0: weight_for_0, 1: weight_for_1})\n",
        "\n",
        "\n",
        "#let's we invoke the function right away\n",
        "class_weight = get_weights(target_train)\n",
        "print('Computed weights:' + str(class_weight))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rarkB4BMAuJy"
      },
      "source": [
        "# Improvement: better model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR0128KS-9Wr"
      },
      "source": [
        "#let's keep it simple: adding a Dense ReLU layer\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model2 = Sequential()\n",
        "model2.add(Input((features_train.shape[1],)))\n",
        "model2.add(Dense(10, activation='relu'))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model2.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "      keras.metrics.BinaryAccuracy(),\n",
        "      keras.metrics.AUC(),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rViMBXmPCrwp"
      },
      "source": [
        "# Train again!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_train)"
      ],
      "metadata": {
        "id": "WpFvSLIqCyIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM06qx5M_o6A"
      },
      "source": [
        "#train with normalized data, class weights, improved model, same epochs\n",
        "history_m2 = model2.fit(\n",
        "    x = features_train,\n",
        "    y = target_train.to_numpy(),\n",
        "    epochs=EPOCHS,\n",
        "    validation_data = (features_val, target_val.to_numpy()),\n",
        "    class_weight = class_weight,\n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(target_train.to_list()))"
      ],
      "metadata": {
        "id": "d907b99lFkcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ol8xJhW0uvA"
      },
      "source": [
        "Confusion matrix for second model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4VMZEVETf9K"
      },
      "source": [
        "predictions = model2.predict(features_val)\n",
        "predicted_labels = numpy.where(predictions > 0.5, 1, 0)\n",
        "con_mat_df = confusion_matrix(target_val, predicted_labels)\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzSFNZaGCzhQ"
      },
      "source": [
        "#a direct comparison of loss functions\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Model 1')\n",
        "plt.plot(history_m2.history['loss'], label='Model 2')\n",
        "plt.title('Train set loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['val_loss'], label='Model 1')\n",
        "plt.plot(history_m2.history['val_loss'], label='Model 2')\n",
        "plt.title('Validation set loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN4B0g4oRup6"
      },
      "source": [
        "#taking a look at final val accuracy\n",
        "\n",
        "print('Binary accuracy on validation set')\n",
        "print('Model 1) ' + str(history.history['val_binary_accuracy'][-1]))\n",
        "print('Model 2) ' + str(history_m2.history['val_binary_accuracy'][-1]))\n",
        "\n",
        "print('\\nLoss on validation set')\n",
        "print('Model 1) ' + str(history.history['val_loss'][-1]))\n",
        "print('Model 2) ' + str(history_m2.history['val_loss'][-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ4uU7yNmt6o"
      },
      "source": [
        "---\n",
        "# END OF INTERACTIVE PART\n",
        "\n",
        "(code below this point is a bit too slow for class)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cxmrd-mnMG0"
      },
      "source": [
        "# Improvement: fine tuning number of units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iruyLVdC2nxs"
      },
      "source": [
        "We want to add more units to our hidden layer. How many? This is our first true HYPERPARAMETER. To tune it we'll implement a five-folds crossvalidation scheme. BUT we'll need a new, pristine piece of data to actually evaluate the performances. It's time to introduce..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xS7weS-nhGt"
      },
      "source": [
        "## Train, validation and TEST sets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHzWpgr1q-FF"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1-9uP7NfHGUx-TtZzKcil3W6ehx28YRCI\" width=600/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vlYTzcZx_Cl"
      },
      "source": [
        "#support function to have a leaner code down below:\n",
        "#input: train set, test set, number of units in the hidden layer\n",
        "#output: train history object\n",
        "\n",
        "def train_NN(feat_tr, feat_val, tar_tr, tar_val, n_units):\n",
        "    #same 1 hidden layer model as above\n",
        "    m = Sequential()\n",
        "    m.add(Input((feat_tr.shape[1],)))\n",
        "    m.add(Dense(units = n_units, activation='relu'))\n",
        "    m.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    #the model is declared, but we still need to compile it to actually\n",
        "    #build all the data structures\n",
        "    m.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "          keras.metrics.BinaryAccuracy(),\n",
        "          keras.metrics.AUC(name='auc'),\n",
        "        ])\n",
        "\n",
        "    #compute class weights for this specific split\n",
        "    cw = get_weights(tar_tr)\n",
        "\n",
        "    #ready to train!\n",
        "    h = m.fit(\n",
        "      feat_tr, tar_tr.to_numpy(),\n",
        "      epochs=EPOCHS,\n",
        "      validation_data=(feat_val, tar_val.to_numpy()),\n",
        "      class_weight = cw,\n",
        "      verbose=0)\n",
        "\n",
        "    #and we are done\n",
        "    return(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdRotcjonlmf"
      },
      "source": [
        "#we now further split the train set to do a proper crossvalidation using\n",
        "#again scikit-learn, but this time we want the indexes for each split\n",
        "#so we are going to instantiate a StratifiedKFold object\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "#as done above, the .split() method returns (an iterable over) two lists which\n",
        "#can be used to index the samples that go into train and test sets\n",
        "\n",
        "#a loop tracker, useful for indexing and printing messages\n",
        "fold = 0\n",
        "\n",
        "#room to store all the training histories\n",
        "all_histories = {}\n",
        "\n",
        "#let's explore these possible number of units\n",
        "layer_units = [2, 4, 8, 16, 32]\n",
        "\n",
        "#loop over folds\n",
        "for train_index_cv, test_index_cv in skf.split(features_train, target_train):\n",
        "    features_train_cv = features_train.iloc[train_index_cv, :]\n",
        "    features_test_cv  = features_train.iloc[test_index_cv, :]\n",
        "    target_train_cv   = target_train.iloc[train_index_cv]\n",
        "    target_test_cv    = target_train.iloc[test_index_cv]\n",
        "\n",
        "    #user interface\n",
        "    fold += 1\n",
        "    print('Doing fold ' + str(fold))\n",
        "\n",
        "    #room to store all the training histories\n",
        "    all_histories[fold] = {}\n",
        "\n",
        "    #loop over considered number of units\n",
        "    for lu in layer_units:\n",
        "      #user interface\n",
        "      print(' - training with lu=' + str(lu))\n",
        "\n",
        "      #training the network, storing the training history\n",
        "      all_histories[fold][lu] = train_NN(\n",
        "          features_train_cv, features_test_cv,\n",
        "          target_train_cv, target_test_cv, lu)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtoxNR_mAGLw"
      },
      "source": [
        "#all_histories is a bit messy, order of indexing is:\n",
        "#fold -> number of units -> actual history object\n",
        "#let's take a look at all these indexes:\n",
        "print(all_histories.keys())\n",
        "print(all_histories[1].keys())\n",
        "print(all_histories[1][16].history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0XM5AkGBxNe"
      },
      "source": [
        "#for each considered number of units we have five different executions\n",
        "#let's take a look at one\n",
        "lu = 16\n",
        "plt.plot(all_histories[1][lu].history['val_loss'], label = 'Fold 1')\n",
        "plt.plot(all_histories[2][lu].history['val_loss'], label = 'Fold 2')\n",
        "plt.plot(all_histories[3][lu].history['val_loss'], label = 'Fold 3')\n",
        "plt.plot(all_histories[4][lu].history['val_loss'], label = 'Fold 4')\n",
        "plt.plot(all_histories[5][lu].history['val_loss'], label = 'Fold 5')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Validation set Loss for ' + str(lu) + ' units')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqYk90EOJJ78"
      },
      "source": [
        "#let's forget history and focus on metrics for last epoch\n",
        "#also: let's average over folds\n",
        "\n",
        "rows = []\n",
        "for lu in layer_units:\n",
        "  for fold in all_histories.keys():\n",
        "    row = []\n",
        "    row.append(lu)\n",
        "    row.append(all_histories[fold][lu].history['val_loss'][-1])\n",
        "    row.append(all_histories[fold][lu].history['val_binary_accuracy'][-1])\n",
        "    row.append(all_histories[fold][lu].history['val_auc'][-1])\n",
        "    rows.append(row)\n",
        "\n",
        "#converting to pandas\n",
        "df = pandas.DataFrame(rows, columns=[\"LU\", \"loss\", \"accuracy\", \"AUC\"])\n",
        "\n",
        "#average over folds\n",
        "df = df.groupby(df['LU']).aggregate('mean')\n",
        "\n",
        "#and take a look to the numbers\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vExr8fdZ60ln"
      },
      "source": [
        "#selecting LU with best loss\n",
        "LU_selected = df.loc[:, 'loss'].idxmin()\n",
        "print('Best performance with ' + str(LU_selected) + ' units in the hidden layer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMNuxwgy9If8"
      },
      "source": [
        "#train again, this time using the full train set, and evaluating in the validation set\n",
        "history_m3 = train_NN(\n",
        "    features_train, features_val,\n",
        "    target_train, target_val,\n",
        "    LU_selected)\n",
        "\n",
        "#taking a look at the final validation loss, even if the comparison it's a\n",
        "#bit unfair (can you tell why?)\n",
        "print('Model 1) ' + str(history.history['val_loss'][-1]))\n",
        "print('Model 2) ' + str(history_m2.history['val_loss'][-1]))\n",
        "print('Model 3) ' + str(history_m3.history['val_loss'][-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TODO\n",
        "the `features_val` should be renamed `features_test` and so forth"
      ],
      "metadata": {
        "id": "EExwzXZUVGZI"
      }
    }
  ]
}