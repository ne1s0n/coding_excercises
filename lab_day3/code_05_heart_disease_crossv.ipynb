{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code_05_heart_disease_crossv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCbL-3UsFO3l"
      },
      "source": [
        "# You are breaking my heart\n",
        "\n",
        "This dataset contains information on 303 patients. Several medically relevant data are available (age, sex, cholesterol, resting blood pressure...). Our task is to predict the presence of heart disease (column \"target\", 0 means healty, 1 means sick).\n",
        "\n",
        "This dataset is described in detail:\n",
        "\n",
        "* on Kaggle datasets: https://www.kaggle.com/ronitf/heart-disease-uci\n",
        "* on its original webpage: https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
        "\n",
        "I've downloaded a copy of the data and made it available at the following url:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKkpvbRpxMY7"
      },
      "source": [
        "DATASET_URL = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/datasets_33180_43520_heart.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2zNple3YGNn"
      },
      "source": [
        "# Battleplan\n",
        "\n",
        "* load dataset\n",
        "* a bit of data visualization/exploration\n",
        "* a baseline classifier: logistic regression\n",
        "* improve the data, improve the classifier\n",
        "* a better NN classifier: let's add a layer\n",
        "  * doing a proper crossvalidation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whmOWIRgR15i"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR0ryu2cR3OS"
      },
      "source": [
        "#let's fix already our desired number of epochs\n",
        "EPOCHS = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OECkDKCOHdl_"
      },
      "source": [
        "To be sure to have the same results, we can fix the random seeds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAv0wbk_HksK"
      },
      "source": [
        "#general random seed\n",
        "from numpy.random import seed\n",
        "seed(0)\n",
        "\n",
        "#tensorflow-specific seed\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQLaXLYf0Fh6"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R90Prp_wAF-Z"
      },
      "source": [
        "import pandas\n",
        "\n",
        "#pandas can read a csv directly from a url\n",
        "heart_data = pandas.read_csv(DATASET_URL)\n",
        "print(heart_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oorqzi51Cjw3"
      },
      "source": [
        "#splitting features and target\n",
        "features = heart_data.iloc[:,:-1]\n",
        "target = heart_data.iloc[:,-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KwlarWPInSz"
      },
      "source": [
        "#take a look at what we have done\n",
        "print(heart_data.columns)\n",
        "print(features.shape)\n",
        "print(target.shape) #beware of rank 1 arrays"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ISOT2GBg66"
      },
      "source": [
        "## Train and Validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkudbVoiBmI4"
      },
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, target):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target[train_index]\n",
        "    target_val     = target[val_index]\n",
        "    \n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSUQjM0xAevZ"
      },
      "source": [
        "# Baseline predictor: logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pbfwsHGDry5"
      },
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will \n",
        "#flow like INPUT -> ELABORATION -> OUTPUT. In particular, we will\n",
        "#not have any loops, i.e. our output will never be recycled as\n",
        "#input for the first layer\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes. In our case there is only one node in the layer, and\n",
        "#it receives all the features\n",
        "from keras.layers import Dense\n",
        "\n",
        "import keras.metrics\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=features_train.shape[1]))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
        "    metrics=[\n",
        "      keras.metrics.BinaryAccuracy(),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8OZBvPnEBjE"
      },
      "source": [
        "history = model.fit(features_train, target_train, epochs=10, validation_data=(features_val, target_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-LmEK9hIPNQ"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AZztca9EGXq"
      },
      "source": [
        "#function to take a look at losses and metrics evolution\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_loss_history(h, title):\n",
        "  for metric in h.history.keys():\n",
        "    #ignoring metrics on validation set, which are implied when\n",
        "    #plotting on training set\n",
        "    if metric.startswith('val_'):\n",
        "      continue\n",
        "    \n",
        "    #if we get here we found a metric on the training set,\n",
        "    #let's plot it\n",
        "    plt.plot(h.history[metric], label = \"Train set\")\n",
        "    plt.plot(h.history[\"val_\" + metric], label = \"Validation set\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title + ' - ' + metric)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h719dXPAEJCy"
      },
      "source": [
        "plot_loss_history(history, 'Logistic (10 epochs)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhlpWS5BEaCR"
      },
      "source": [
        "#putting verbose to 0 to avoid filling the screen\n",
        "history2 = model.fit(features_train, target_train, epochs=(EPOCHS - 10), \n",
        "                     validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPFR7y3Ed5W"
      },
      "source": [
        "#putting together the whole history\n",
        "for k in history.history.keys():\n",
        "  history.history[k] += history2.history[k]\n",
        "\n",
        "#and plotting again\n",
        "plot_loss_history(history, 'Logistic (' + str(EPOCHS) + ' epochs)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t0i7TBSAoa2"
      },
      "source": [
        "# Improvement: data normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDGtI4Am1IAx"
      },
      "source": [
        "#getting an idea about features averages, sd\n",
        "avg = features_train.mean()\n",
        "std = features_train.std()\n",
        "print('Feature means')\n",
        "print(avg)\n",
        "print('\\nFeature standard deviations')\n",
        "print(std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_UHLi322CCV"
      },
      "source": [
        "#normalizing features, using the same weights for both\n",
        "#train and validation test\n",
        "features_train = (features_train - avg)/std\n",
        "features_val = (features_val - avg)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWvJ7KTArHi"
      },
      "source": [
        "# Improvement: class balancing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYzLcD0a2yt4"
      },
      "source": [
        "#comparing the number of samples for each class\n",
        "N_TOT = target_train.shape[0]\n",
        "N_DISEASE = target_train.sum()\n",
        "N_NORMAL = N_TOT - N_DISEASE\n",
        "\n",
        "print('Total samples: ' + str(N_TOT) + ' (normal:' + str(N_NORMAL) + ' diseases:' + str(N_DISEASE) + ')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7CZtUmV30yr"
      },
      "source": [
        "#https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit and\n",
        "#whatch for argument class_weight\n",
        "\n",
        "#handy function, to be reused later\n",
        "def get_weights(target):\n",
        "  #counting the number of instances for each class\n",
        "  class_1_cnt = target_train.sum()\n",
        "  class_0_cnt = target.shape[0] - target_train.sum()\n",
        "\n",
        "  #we get weights as inverse of class ratios\n",
        "  weight_for_0 = (class_0_cnt + class_1_cnt) / class_0_cnt\n",
        "  weight_for_1 = (class_0_cnt + class_1_cnt) / class_1_cnt\n",
        "\n",
        "  #and we are done\n",
        "  return({0: weight_for_0, 1: weight_for_1})\n",
        "\n",
        "\n",
        "#let's we invoke the function right away\n",
        "class_weight = get_weights(target_train)\n",
        "print('Computed weights:' + str(class_weight))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rarkB4BMAuJy"
      },
      "source": [
        "# Improvement: better model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR0128KS-9Wr"
      },
      "source": [
        "#let's keep it simple: adding a Dense ReLU layer\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(10, activation='relu', input_dim=features_train.shape[1]))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model2.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
        "    metrics=[\n",
        "      keras.metrics.BinaryAccuracy(),\n",
        "      keras.metrics.AUC(),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rViMBXmPCrwp"
      },
      "source": [
        "# Train again!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM06qx5M_o6A"
      },
      "source": [
        "#train with normalized data, class weights, improved model, same epochs\n",
        "history_m2 = model2.fit(\n",
        "    features_train, target_train, \n",
        "    epochs=EPOCHS, \n",
        "    validation_data=(features_val, target_val), \n",
        "    class_weight = class_weight,\n",
        "    verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3AbaH0cA0BP"
      },
      "source": [
        "#take a look at our results\n",
        "plot_loss_history(history_m2, 'Improved NN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzSFNZaGCzhQ"
      },
      "source": [
        "#a direct comparison of loss functions\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Model 1')\n",
        "plt.plot(history_m2.history['loss'], label='Model 2')\n",
        "plt.title('Train set loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['val_loss'], label='Model 1')\n",
        "plt.plot(history_m2.history['val_loss'], label='Model 2')\n",
        "plt.title('Validation set loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN4B0g4oRup6"
      },
      "source": [
        "#taking a look at final val accuracy\n",
        "\n",
        "print('Binary accuracy on validation set')\n",
        "print(history.history['val_binary_accuracy'][-1])\n",
        "print(history_m2.history['val_binary_accuracy'][-1])\n",
        "\n",
        "print('\\nLoss on validation set')\n",
        "print(history.history['val_loss'][-1])\n",
        "print(history_m2.history['val_loss'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cxmrd-mnMG0"
      },
      "source": [
        "# Improvement: fine tuning number of units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xS7weS-nhGt"
      },
      "source": [
        "## Train, validation and TEST sets!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHzWpgr1q-FF"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1-9uP7NfHGUx-TtZzKcil3W6ehx28YRCI\" width=600/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vlYTzcZx_Cl"
      },
      "source": [
        "#support function to have a leaner code down below:\n",
        "#input: train set, test set, number of units in the hidden layer\n",
        "#output: train history object\n",
        "\n",
        "def train_NN(feat_tr, feat_val, tar_tr, tar_val, n_units):\n",
        "    #same 1 hidden layer model as above\n",
        "    m = Sequential()\n",
        "    m.add(Dense(units = n_units, activation='relu', input_dim=feat_tr.shape[1]))\n",
        "    m.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    #the model is declared, but we still need to compile it to actually\n",
        "    #build all the data structures\n",
        "    m.compile(optimizer='rmsprop', loss='binary_crossentropy', \n",
        "        metrics=[\n",
        "          keras.metrics.BinaryAccuracy(),\n",
        "          keras.metrics.AUC(name='auc'),\n",
        "        ])\n",
        "    \n",
        "    #compute class weights for this specific split\n",
        "    cw = get_weights(tar_tr)\n",
        "\n",
        "    #ready to train!\n",
        "    h = m.fit(\n",
        "      feat_tr, tar_tr, \n",
        "      epochs=EPOCHS, \n",
        "      validation_data=(feat_val, tar_val), \n",
        "      class_weight = cw,\n",
        "      verbose=0)\n",
        "    \n",
        "    #and we are done\n",
        "    return(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdRotcjonlmf"
      },
      "source": [
        "#we now further split the train set to do a proper crossvalidation using\n",
        "#again scikit-learn, but this time we want the indexes for each split\n",
        "#so we are going to instantiate a StratifiedKFold object\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "#as done above, the .split() method returns (an iterable over) two lists which \n",
        "#can be used to index the samples that go into train and test sets\n",
        "\n",
        "#a loop tracker, useful for indexing and printing messages\n",
        "fold = 0\n",
        "\n",
        "#room to store all the training histories\n",
        "all_histories = {}\n",
        "\n",
        "#let's explore these possible number of units\n",
        "layer_units = [2, 4, 8, 16, 32]\n",
        "\n",
        "#loop over folds\n",
        "for train_index_cv, test_index_cv in skf.split(features_train, target_train):\n",
        "    features_train_cv = features_train.iloc[train_index_cv, :]\n",
        "    features_test_cv  = features_train.iloc[test_index_cv, :]\n",
        "    target_train_cv   = target_train.iloc[train_index_cv]\n",
        "    target_test_cv    = target_train.iloc[test_index_cv]\n",
        "\n",
        "    #user interface\n",
        "    fold += 1\n",
        "    print('Doing fold ' + str(fold))\n",
        "\n",
        "    #room to store all the training histories\n",
        "    all_histories[fold] = {}\n",
        "\n",
        "    #loop over considered number of units\n",
        "    for lu in layer_units:\n",
        "      #user interface\n",
        "      print(' - training with lu=' + str(lu))\n",
        "\n",
        "      #training the network, storing the training history\n",
        "      all_histories[fold][lu] = train_NN(\n",
        "          features_train_cv, features_test_cv, \n",
        "          target_train_cv, target_test_cv, lu)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtoxNR_mAGLw"
      },
      "source": [
        "#all_histories is a bit messy, order of indexing is:\n",
        "#fold -> number of units -> actual history object\n",
        "#let's take a look at all these indexes:\n",
        "print(all_histories.keys())\n",
        "print(all_histories[1].keys())\n",
        "print(all_histories[1][16].history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0XM5AkGBxNe"
      },
      "source": [
        "#for each considered number of units we have five different executions\n",
        "#let's take a look at one\n",
        "lu = 16\n",
        "plt.plot(all_histories[1][lu].history['val_loss'], label = 'Fold 1')\n",
        "plt.plot(all_histories[2][lu].history['val_loss'], label = 'Fold 2')\n",
        "plt.plot(all_histories[3][lu].history['val_loss'], label = 'Fold 3')\n",
        "plt.plot(all_histories[4][lu].history['val_loss'], label = 'Fold 4')\n",
        "plt.plot(all_histories[5][lu].history['val_loss'], label = 'Fold 5')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Test set Loss for ' + str(lu) + ' units')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqYk90EOJJ78"
      },
      "source": [
        "#let's forget history and focus on metrics for last epoch\n",
        "#also: let's average over folds\n",
        "\n",
        "rows = []\n",
        "for lu in layer_units:\n",
        "  for fold in all_histories.keys():\n",
        "    row = []\n",
        "    row.append(lu)\n",
        "    row.append(all_histories[fold][lu].history['val_loss'][-1])\n",
        "    row.append(all_histories[fold][lu].history['val_binary_accuracy'][-1])\n",
        "    row.append(all_histories[fold][lu].history['val_auc'][-1])\n",
        "    rows.append(row)\n",
        "\n",
        "#converting to pandas\n",
        "df = pandas.DataFrame(rows, columns=[\"LU\", \"loss\", \"accuracy\", \"AUC\"])\n",
        "\n",
        "#average over folds\n",
        "df = df.groupby(df['LU']).aggregate('mean')\n",
        "\n",
        "#and take a look to the numbers\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vExr8fdZ60ln"
      },
      "source": [
        "#selecting LU with best loss\n",
        "LU_selected = df.loc[:, 'loss'].idxmin()\n",
        "print('Best performance with ' + str(LU_selected) + ' units in the hidden layer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMNuxwgy9If8"
      },
      "source": [
        "#train again, this time using the full train set, and evaluating in the validation set\n",
        "history_m3 = train_NN(\n",
        "    features_train, features_val,\n",
        "    target_train, target_val,\n",
        "    LU_selected)\n",
        "\n",
        "#taking a look at final validation loss\n",
        "print(history.history['val_loss'][-1])\n",
        "print(history_m2.history['val_loss'][-1])\n",
        "print(history_m3.history['val_loss'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}