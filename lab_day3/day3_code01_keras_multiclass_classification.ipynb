{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYvC99Qgqt6L"
      },
      "source": [
        "\n",
        "\n",
        "# Multiclass classification using Softmax\n",
        "\n",
        "We now extend logistic regression to multiclas classification using [softmax](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "The problem requires us to do three-classes classification using a Softmax function, which can be easily considered as an extension of logistic regression over three (or more) classes. We use the neural network-like implementation as with binary classification.\n",
        "\n",
        "Luckily, Keras provides a [softmax activation function](https://keras.io/api/layers/activations/#softmax-function), which we will use instead of the logistic we previously used.\n",
        "\n",
        "The structure of our network will be similar, but the output goes from a single number to **three** numbers, one per class, and we thus need three nodes:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/softmax_neuron.png\">\n",
        "\n",
        "As a result, the loss function will need to change. Remember, loss represents a measure of how good the predictions are. Previously we used binary_crossentropy, but since now predictions are multiclass we need to change function. Luckily Keras provides a natural extension for the multiclass case with [CategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV7d3uTErn8z"
      },
      "source": [
        "## Categorical data\n",
        "\n",
        "As usual, we first import some necessary libraries and load the data: as we did with binary classification, we first take only two features from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPv0zhcYsMgV"
      },
      "source": [
        "## import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setting seeds\n",
        "from numpy.random import seed\n",
        "seed(180)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.keras.utils.set_random_seed(121)\n",
        "tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "zfIo6F9kHCXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YROzh8LRrxpk"
      },
      "source": [
        "iris = sklearn.datasets.load_iris()\n",
        "iris.data = pd.DataFrame(iris.data, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n",
        "iris.target = pd.Series(iris.target) #converting numpy array -> pandas Series\n",
        "iris.target = iris.target.to_frame() #converting Pandas series to dataframe\n",
        "\n",
        "features = iris.data.iloc[:,0:2] ## which features / columns are we selectiong?\n",
        "target = iris.target\n",
        "\n",
        "feature_x = 0\n",
        "feature_y = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxsFtmDp8yC3"
      },
      "source": [
        "We can re-plot the data to check their distribution in the two-feature space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHepzfwF8oTx"
      },
      "source": [
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in three bunches of 50, once per class\n",
        "ax.scatter(x=iris.data.iloc[0:50,feature_x],    y=iris.data.iloc[0:50,feature_y],    c='red',   label=iris.target_names[0])\n",
        "ax.scatter(x=iris.data.iloc[50:100,feature_x],  y=iris.data.iloc[50:100,feature_y],  c='green', label=iris.target_names[1])\n",
        "ax.scatter(x=iris.data.iloc[100:150,feature_x], y=iris.data.iloc[100:150,feature_y], c='blue',  label=iris.target_names[2])\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrkgfuYx9CwZ"
      },
      "source": [
        "We now check the dimensions of our input arrays (a sanity check which is important with neural networks and deep learning):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOCu4sUD8wRM"
      },
      "source": [
        "print('Shape of the feature table: ' + str(features.shape))\n",
        "print('Shape of the target array: ' + str(target.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VX1jVMWsmwq"
      },
      "source": [
        "The problem is that our target array `iris.target` is a numeric array. But those numbers we used (0, 1, and 2) do not represent real values. In other words, \"virginica\" is not twice \"versicolor\". Numbers here are used as labels, not as quantities.\n",
        "\n",
        "In fact, to properly train a model the structure of the target array must change to [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). In simple terms, it needs to become a table with one row per sample (150 in total) and one column per class (three in total). Something like:\n",
        "\n",
        "| Setosa | Versicolor | Virginica |\n",
        "|------|------|------|\n",
        "|   0  |   1  |   0  |\n",
        "|   1  |   0  |   0  |\n",
        "|   1  |   0  |   0  |\n",
        "|   0  |   0  |   1  |\n",
        "\n",
        "As you can see the first sample is Versicolor, the second and third are Setosa, the last one is Virginica. Note that there is only a single \"one\" per row.\n",
        "\n",
        "Luckily, it's easy to pass to one-hot encoding using keras function [to_categorical](https://keras.io/api/utils/python_utils/#to_categorical-function):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnoxh-5vs7_F"
      },
      "source": [
        "#the \"utils\" subpackage is very useful, take a look to it when you have time\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#converting to categorical\n",
        "target_multi_cat = tf.keras.utils.to_categorical(target)\n",
        "\n",
        "#since everything else is a Pandas dataframe, let's stick to the format\n",
        "#for consistency\n",
        "target_multi_cat = pd.DataFrame(target_multi_cat)\n",
        "\n",
        "#let's take a look\n",
        "print(target_multi_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITSVqAr7tQyF"
      },
      "source": [
        "## Training and validation sets\n",
        "\n",
        "We are now ready to create our training and validation sets, as done above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u_Io-6ftVUc"
      },
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "test_pct = 0.2\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "#random_state is used to control class balance between training and test sets (None to switch to random behavior)\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size= test_pct, random_state=0)\n",
        "\n",
        "for train_index, val_index in sss.split(features, target_multi_cat):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target_multi_cat.iloc[train_index, :]\n",
        "    target_val     = target_multi_cat.iloc[val_index, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7FWwIxGt1a1"
      },
      "source": [
        "Just a little check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWYIPsoDt3cv"
      },
      "source": [
        "#shapes\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)\n",
        "\n",
        "#number of classes per split\n",
        "print('\\nClasses in train set:')\n",
        "print(target_train.sum())\n",
        "print('\\nClasses in validation set:')\n",
        "print(target_val.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EQXmDHjuJ_C"
      },
      "source": [
        "We have now a balanced dataset, with 40 instances for each class in the training set and 10 in the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature normalization\n",
        "\n",
        "We calculate the average and standard deviation on the training data: then we use these values to normalize the features both in the training and in the test datasets <!-- (!! **IMPORTANT** !!) -->"
      ],
      "metadata": {
        "id": "rr_IaQaCrhFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating features averages and std devs\n",
        "avg = features_train.mean()\n",
        "std = features_train.std()\n",
        "\n",
        "#standardizing the data (mean 0, std 1)\n",
        "features_train = (features_train - avg)/std\n",
        "features_val = (features_val - avg)/std"
      ],
      "metadata": {
        "id": "Qt35OrgprQa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train.head()"
      ],
      "metadata": {
        "id": "0J7nvFtYry4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_val.head()"
      ],
      "metadata": {
        "id": "eaF24iS4r42g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train.describe()"
      ],
      "metadata": {
        "id": "jnIOLyLyA-kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_val.describe()"
      ],
      "metadata": {
        "id": "RMfhIuo6BFcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMYSgINdNGHI"
      },
      "source": [
        "## Set up\n",
        "\n",
        "We define here the hyperparameters of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B88ql-inMRxE"
      },
      "source": [
        "num_classes = 3\n",
        "input_shape = features_train.shape[1]\n",
        "activation_function = 'softmax'\n",
        "optimising_method = 'rmsprop'\n",
        "loss_function = 'categorical_crossentropy'\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVZZ1dFarX10"
      },
      "source": [
        "## A multiclass model\n",
        "\n",
        "We are now ready to declare our multiclass classification model: we use `Keras`, but this is equivalent to multiclass logistic regression (only with neural networks-like representation). The output layer has three units, corresponding to the three classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fic85viHqtVB"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# 3-class softmax regression in Keras\n",
        "model_multi = Sequential()\n",
        "model_multi.add(Dense(units = num_classes, activation=activation_function, input_dim=input_shape))\n",
        "\n",
        "#compile the model specifying the new multiclass loss\n",
        "model_multi.compile(optimizer=optimising_method, loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVDzz9Hnrh8W"
      },
      "source": [
        "Let's take a look under the hood:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcP7R4JrrbKr"
      },
      "source": [
        "print(model_multi.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX2raO_-rl3F"
      },
      "source": [
        "We now have to train 9 parameters: 3 coefficients (W1, W2 B) times three nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT912ksQuUxf"
      },
      "source": [
        "## Fitting the model\n",
        "\n",
        "We are ready to fit the model. This time we go directly to 500 epochs, trained in silent mode. We then plot the loss function evolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvtbANHaulJl"
      },
      "source": [
        "history_multi = model_multi.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUiWK0hxvX3V"
      },
      "source": [
        "#function to take a look at losses evolution\n",
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history_multi, 'Softmax multiclass ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH_hiUKttHqo"
      },
      "source": [
        "This looks promising. There's the clear same pattern we saw with logistic regression, with a strong improvement in the first hundred epochs (and then things become slow...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwJcq2WytQcH"
      },
      "source": [
        "## Decision boundary\n",
        "\n",
        "We want now to plot again the decision boundary. Unfortunately `plot_decision_regions` function from [mlxtend](http://rasbt.github.io/mlxtend/) module does not support one-hot encoded multiclasses natively. Luckily [there's a quick workaround](https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/#visualizing-the-decision-boundary), but if you get lost in the code don't worry and just look at the plot :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySRsRvoktXdc"
      },
      "source": [
        "#we define a class to take the Keras model and convert its predictions\n",
        "#from \"one probability per iris type\" to \"just the iris type with the highest probability\"\n",
        "class Onehot2Int(object):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.model.predict(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        "#we wrap our trained model, instantiating a new object\n",
        "keras_model_no_ohe = Onehot2Int(model_multi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More on the function below here: https://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/"
      ],
      "metadata": {
        "id": "POsFKEntCVXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#and we can now plot the decision boundary safely (we still need to convert\n",
        "#the target one-hot-encoded matrix to int, though)\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "\n",
        "plot_decision_regions(features_val.to_numpy(), np.argmax(target_val.to_numpy(), axis=1),\n",
        "                      clf=keras_model_no_ohe) ## clf = classifier object\n",
        "plt.title('Decision boundary for 0 (setosa) vs 1 (versicolor) vs 2 (virginica)')\n",
        "plt.xlabel(iris.feature_names[feature_x])\n",
        "plt.ylabel(iris.feature_names[feature_y])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XUzJNbLEAvQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOZHtkJ3vid2"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "We now look at predictions in the **test set**: each test sample is assigned to the class with the highest probability.\n",
        "\n",
        "Predicted classes are then compared to true classes in a confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh5rmApBvkxQ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "predictions = model_multi.predict(features_val)\n",
        "print(predictions)\n",
        "\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n",
        "\n",
        "target_classes = target.iloc[val_index].to_numpy()\n",
        "\n",
        "### for later use ###\n",
        "predicted_classes_logistic = predicted_classes\n",
        "target_classes_logistic = target_classes\n",
        "### --- ###\n",
        "\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2])\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O6Qmf3L7y6a"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yn965L876q-"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(target_classes_logistic, predicted_classes_logistic)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "confusion_matrix(target_classes_logistic, predicted_classes_logistic, normalize='true', labels=[0,1,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95rhqPISbszq"
      },
      "source": [
        "# Shallow neural networks for softmax regression\n",
        "\n",
        "We now move on from multinomial logistic regression to softmax regression with neural networks: again, we start by implementing a shallow neural netowrk model.\n",
        "\n",
        "First, we select all four features from the dataset and configure some basic parameters:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## setting seeds\n",
        "seed(123)\n",
        "tf.keras.utils.set_random_seed(777)"
      ],
      "metadata": {
        "id": "BbstDzSdVkis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTXwA5oBflKf"
      },
      "source": [
        "## select all 4 features\n",
        "features = iris.data.iloc[:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## normalize the data\n",
        "#calculating features averages and std devs\n",
        "avg = features_train.mean()\n",
        "std = features_train.std()\n",
        "\n",
        "#standardizing the data (mean 0, std 1)\n",
        "features_train = (features_train - avg)/std\n",
        "features_val = (features_val - avg)/std"
      ],
      "metadata": {
        "id": "Ai9bWy4rsa3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dKMvwggcGe0"
      },
      "source": [
        "## # Configuration options\n",
        "num_classes = 3\n",
        "input_shape = (features.shape[1],)\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'softmax'\n",
        "loss_function = 'categorical_crossentropy'\n",
        "optimizer_used = 'rmsprop' ## or keras.optimizers.adam(lr=0.001)? maybe for softmax regression?\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSPBwa-FfdmB"
      },
      "source": [
        "# 3-class softmax regression in Keras\n",
        "model_multi = Sequential()\n",
        "model_multi.add(Dense(units=hidden_nodes, input_shape=input_shape, activation=hidden_activation))\n",
        "model_multi.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#compile the model specifying the new multiclass loss\n",
        "model_multi.compile(optimizer='rmsprop', loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ARQwD-DfgHj"
      },
      "source": [
        "print(model_multi.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJRXmm9YglNd"
      },
      "source": [
        "We see that we have 67 parameters to train: 5 parameters (w1 ... w4, b) times 8 nodes in the hidden layer; 9 parameters (8 units + b) times three nodes in the output layer.\n",
        "\n",
        "## Training and test sets\n",
        "\n",
        "We split the data in the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8yMNyp3gkNJ"
      },
      "source": [
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, target_multi_cat):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target_multi_cat.iloc[train_index,:]\n",
        "    target_val     = target_multi_cat.iloc[val_index,:]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(\"Training features size: \", features_train.shape)\n",
        "print(\"Test features size: \", features_val.shape)\n",
        "print(\"Training targets size: \", target_train.shape)\n",
        "print(\"Test targets size: \", target_val.shape)\n",
        "\n",
        "print(\"Type of the training features object: \", type(features_train))\n",
        "print(\"Type of the training targets object: \", type(target_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRG3rxbnhFPm"
      },
      "source": [
        "#number of classes per split\n",
        "print('\\nClasses in train set:')\n",
        "print(target_train.sum())\n",
        "print('\\nClasses in validation set:')\n",
        "print(target_val.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp4EALtwhfUL"
      },
      "source": [
        "history_multi = model_multi.fit(features_train, target_train, epochs=num_epochs,\n",
        "                     validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_history(history_multi, 'Softmax multiclass ({} epochs)'.format(num_epochs))"
      ],
      "metadata": {
        "id": "cE9f6F59s4Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZJG_vlRhlwa"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "predictions = model_multi.predict(features_val)\n",
        "print(predictions)\n",
        "\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "target_classes = target.iloc[val_index,:].to_numpy()\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels=[0,1,2])\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl1XGbfHh4jS"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5af7Zgdch8ED"
      },
      "source": [
        "accuracy = accuracy_score(target_classes, predicted_classes)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "confusion_matrix(target_classes, predicted_classes, normalize='true', labels=[0,1,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cohen's k (snippet)\n",
        "\n",
        "We briefly introduce one additional metric to evaluate multiclass classification models\n",
        "\n",
        "For multiclass classification problems we can also use other metrics to measure performance, like for instance `Cohen's kappa` (or `k` coefficient) (more info <a href='https://en.wikipedia.org/wiki/Cohen%27s_kappa'>here</a>).\n",
        "\n",
        "We start from the results of our first model (multinomial logistic regression, output layer only - because we have some errors there ...):\n",
        "\n",
        "`predicted_classes_logistic`\n",
        "\n",
        "`target_classes_logistic`\n"
      ],
      "metadata": {
        "id": "4XpoVUv341RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes_logistic = predicted_classes_logistic.reshape(len(predicted_classes_logistic))\n",
        "target_classes_logistic = target_classes_logistic.reshape(len(target_classes_logistic))\n",
        "df = pd.DataFrame({'obs':target_classes_logistic, 'preds':predicted_classes_logistic})\n",
        "df['result'] = df.apply(lambda row: row.obs == row.preds, axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tN8OWZ4jcX6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that:\n",
        "\n",
        "- `0` = `setosa`\n",
        "- `1` = `versicolor`\n",
        "- `2` = `virginica`"
      ],
      "metadata": {
        "id": "PMAcQWrh5KD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tbl = df.drop(columns = ['preds']).groupby('obs').sum('result') ## summing True occurrences --> correct predictions\n",
        "tbl"
      ],
      "metadata": {
        "id": "bH6GK05_gxMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_setosa = tbl.loc[[0]]['result'].item()\n",
        "true_versicolor = tbl.loc[[1]]['result'].item()\n",
        "true_virginica = tbl.loc[[2]]['result'].item()"
      ],
      "metadata": {
        "id": "GZ7ropitibsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now calculate the total accuracy (n. of correct predictions over total number of predictions):"
      ],
      "metadata": {
        "id": "TaSo6_j_7m-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (true_setosa + true_versicolor + true_virginica)/len(predicted_classes_logistic)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "5eM-9o05buQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this the best metric to evaluate the performance of multiclass classification models?\n",
        "\n",
        "We already saw with binary classification that it is important also to look at **TPR** and **TNR**:\n",
        "- unbalanced data\n",
        "- specific classes may be more relevant\n",
        "\n",
        "Furthermore, the total (\"raw\") accuracy does not consider that correct predictions may also be obtained by **chance**!\n",
        "In binary classification, we know that (if data are balanced) the chance accuracy is 0.5.\n",
        "\n",
        "But what about multiclass classification? This is where **Cohen's kappa** comes into play.\n",
        "The `kappa coefficient` tries to consider how much better the predictive performance is over chance accuracy.\n",
        "\n",
        "To do so, we need to get some measure of the **expected value for chance accuracy**.\n"
      ],
      "metadata": {
        "id": "h5zukZuX9fNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's calculate chance accuracy!\n",
        "\n",
        "We use the frequentist definition of probabilities:\n",
        "\n",
        "- chance predictions (relative frequencies of predictions, per class)\n",
        "- chance observations (relative class frequencies from the observed values)"
      ],
      "metadata": {
        "id": "6CBsfHdoPxFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = df.groupby('preds').size()\n",
        "nums"
      ],
      "metadata": {
        "id": "QGBqN3aiE7Ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chance_preds_setosa = nums.iloc[[0]].item()/len(df)\n",
        "chance_preds_versicolor = nums.iloc[[1]].item()/len(df)\n",
        "chance_preds_virginica = nums.iloc[[2]].item()/len(df)"
      ],
      "metadata": {
        "id": "6hH1ZFGAR9mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chance_preds_setosa)\n",
        "print(chance_preds_versicolor)\n",
        "print(chance_preds_virginica)"
      ],
      "metadata": {
        "id": "T6JpDzwdWzUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the test dataset is balanced, and all three classes have equal size (10 examples each): therefore chance observations are the same for all classes ($1/3$):"
      ],
      "metadata": {
        "id": "iNXm2fGocLd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = df.groupby('obs').size()\n",
        "nums"
      ],
      "metadata": {
        "id": "L8hEo-WuYl7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chance_obs_setosa = 10/len(df)\n",
        "chance_obs_versicolor = 10/len(df)\n",
        "chance_obs_virginica = 10/len(df)"
      ],
      "metadata": {
        "id": "PYY78YkqYKYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now all set to calculate **chance accuracy**"
      ],
      "metadata": {
        "id": "Bsls5dQvdI6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chance_accuracy = chance_preds_setosa*chance_obs_setosa + chance_preds_versicolor*chance_obs_versicolor + chance_preds_virginica*chance_obs_virginica\n",
        "print(chance_accuracy)"
      ],
      "metadata": {
        "id": "9uCIbkvtrE6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cohen's k\n",
        "\n",
        "$$\n",
        "k = \\frac{\\text{accuracy} - \\text{chance_accuracy}}{1 - \\text{chance_accuracy}}\n",
        "$$"
      ],
      "metadata": {
        "id": "gARn04CKdPWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kappa = (accuracy-chance_accuracy) / (1 - chance_accuracy)\n",
        "print(kappa)"
      ],
      "metadata": {
        "id": "oai36DhrvPIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- the numerator calculates the difference between accuracy and chance accuracy\n",
        "- if accuracy = 1, we have perfect predictive performance (the confusion matrix is diagonal) $\\rightarrow$ $ k = 1$, regardless of chance accuracy\n",
        "- if accuracy = chance accuracy, $k=0$ $\\rightarrow$ correct predictions are by chance.\n",
        "- if accuracy < chance accuracy, $k < 0$ (negative) $\\rightarrow$ accuracy is lower than what it would be by chance."
      ],
      "metadata": {
        "id": "VaJA2ZEngyb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "## test set\n",
        "k_test = cohen_kappa_score(predicted_classes_logistic,target_classes_logistic)\n",
        "print(\"Cohen kappa in the test set is: \", k_test)"
      ],
      "metadata": {
        "id": "xNBseyzX4-PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What about the (shallow) neural network model?"
      ],
      "metadata": {
        "id": "YYfRkKrlkO2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## test set\n",
        "k_test = cohen_kappa_score(predicted_classes,target_classes)\n",
        "print(\"Cohen kappa in the test set is: \", k_test)"
      ],
      "metadata": {
        "id": "TG8LGTuRj_aC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}