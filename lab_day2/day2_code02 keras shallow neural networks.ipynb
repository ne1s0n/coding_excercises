{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXVGYykpZXPp"
      },
      "source": [
        "## Shallow neural network models\n",
        "\n",
        "We revise here the classification exercises on the `iris` dataset moving from logistic regression to shallow neural networks.\n",
        "Initially, we'll address binary classification.\n",
        "\n",
        "We first import some libraries and the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPlZ98cNOJne"
      },
      "source": [
        "## import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setting seeds\n",
        "from numpy.random import seed\n",
        "seed(132)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(111)\n",
        "tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "U8ZBWpa7T9rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCFcGq29OWV9"
      },
      "source": [
        "iris = sklearn.datasets.load_iris()\n",
        "iris.data = pd.DataFrame(iris.data, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n",
        "iris.target = pd.Series(iris.target) #converting numpy array -> pandas Series\n",
        "iris.target = iris.target.to_frame() #converting Pandas series to dataframe\n",
        "print('Shape of the feature table: ' + str(iris.data.shape))\n",
        "print('Shape of the target array: ' + str(iris.target.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOWIuMHigcdQ"
      },
      "source": [
        "We are now using all four features of the `iris` dataset: `sepal length (cm)`, `sepal width (cm)`, `petal length (cm)` and `petal width (cm)`. As before, we first reduce the problem to binary classification, by merging two classes together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai_XE2gkOd5z"
      },
      "source": [
        "features = iris.data.iloc[:,:]\n",
        "target = iris.target\n",
        "\n",
        "#updating class labels. To makes things difficult we put together old classes 0 and 1\n",
        "#in a new class (non virginica) and keep old class 2 (virginica) as new class 1.\n",
        "#For an easier problems put together versicolor and virginica and keep setosa by itself\n",
        "n1 = 100 ## split: 50 for setosa vs versicolor+virginica, 100 for setos+versicolor vs virginica\n",
        "target[0:n1] = 0\n",
        "target[n1:150] = 1\n",
        "\n",
        "print(iris.target.iloc[:,0].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJCWlvUNtR7d"
      },
      "source": [
        "As before, a visual check of the two classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BPh7_4vOgW5"
      },
      "source": [
        "feature_x = 2\n",
        "feature_y = 3\n",
        "\n",
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in two bunches\n",
        "ax.scatter(x=features.iloc[0:n1,0],   y=features.iloc[0:n1,1],   c='red',  label='Not virginica')\n",
        "ax.scatter(x=features.iloc[n1:150,0], y=features.iloc[n1:150,1], c='blue', label='virginica')\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAOwHJmxO1-D"
      },
      "source": [
        "### Neural Network model\n",
        "\n",
        "In the previous exercise we used logistic regression implemented as a simple neural network model with just the output layer: this output layer had only one node (binary classification) which performed both the regression (linear combination of input variables + bias) and sigmoid activation steps:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PRc719uT1kOUuCMbpHML2sEk7qp6UJnm\">\n",
        "\n",
        "We are now building a **shallow neural network model**, by adding **one hidden layer** with **u nodes** (units):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QROz9pFnMoqTeqrFbele8pFz8qXDSckq\">\n",
        "\n",
        "We now have to set a number of `hyperparameters` (the **building blocks** -or 'ingredients'- of a neural network model):\n",
        "\n",
        "- the **number of hidden nodes** (number of units in the hidden layer)\n",
        "- the **type of activation function** in the hidden layer\n",
        "- the **output activation function**\n",
        "- the **loss function** (for backpropagation)\n",
        "- the **optimizer** (for gradient descent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWTGCvZbPAk"
      },
      "source": [
        "## # Configuration options\n",
        "input_shape = (features.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'SGD' ##stochastic gradient descent\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question for you: what is the size of the input matrix $\\mathbf{X}$, the matrix of coefficients $\\mathbf{W}$ and the vector of bias terms $\\mathbf{b}$ in the hidden layer (layer n. 1)?**"
      ],
      "metadata": {
        "id": "iFPgNU-tt11J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IbRkmOS-HHZ"
      },
      "source": [
        "We chose <a href='https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a'>binary cross-entropy</a> for binary classification problems: this is the loss function we have been discussing in the slides.\n",
        "As for the optimizer, we use `rmsprop`: more details on the available otimizers in Keras can be found <a href='https://keras.io/api/optimizers/'>here</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQDDNpS_czT6"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question for you: what does the `input_shape` indicate?**"
      ],
      "metadata": {
        "id": "3To4nBQAt_ZN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjRDH3RhvcJ7"
      },
      "source": [
        "#### Training and test sets\n",
        "\n",
        "We now prepare the training and test set for correct evaulation of the neural network model performance: we make one split (one training set and one test set), and assign 80% of the data to the training set and 20% of the data to the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMPW3k6_OlVd"
      },
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, target):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target.iloc[train_index,:]\n",
        "    target_val     = target.iloc[val_index,:]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(\"Training features size: \", features_train.shape)\n",
        "print(\"Test features size: \", features_val.shape)\n",
        "print(\"Training targets size: \", target_train.shape)\n",
        "print(\"Test targets size: \", target_val.shape)\n",
        "\n",
        "print(\"Type of the training features object: \", type(features_train))\n",
        "print(\"Type of the training targets object: \", type(target_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwsSzWgnwgFL"
      },
      "source": [
        "#### Building the neural network model\n",
        "\n",
        "We now build the (shallow) **neural network model** using the `Keras` framework: we choose a `sequential` architecture and add `dense` (fully connected) layers (**1 hidden layer**, **1 output layer**).\n",
        "\n",
        "We chose `ReLU` activation functions for the units in the hidden layer, and `sigmoid` activation function for the output layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHMzMrHVOpB4"
      },
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT.\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes.\n",
        "from keras.layers import Dense\n",
        "\n",
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(units=hidden_nodes, input_shape=input_shape, activation=hidden_activation))\n",
        "model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4O2yFSjc-Wh"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfZaLf2L0jI3"
      },
      "source": [
        "The `summary()` method of the Keras model tells us that there are 49  parameters to train:\n",
        "- w1, w2, w3, w4, b (weights for the 4 features + bias term) for each of the 8 nodes in the hidden layer ($\\rightarrow$ 40 parameters);\n",
        "- w1 - w8 + b (weights for the 8 nodes results + bias term) for the output layer ($\\rightarrow$ 9 parameters) [&#161; you may remember from the matrix dimensions we discussed in the slides !]\n",
        "\n",
        "#### Training the neural network model\n",
        "\n",
        "We have now prepared everything we need and are ready to train the model on our data. It's an iterative process that cycles many times through what are called `epochs` (~ iterations). We'll start with using the parameter set above:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0UlsKrDdCiK"
      },
      "source": [
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fclNIr25eTGi"
      },
      "source": [
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym491pXNlplI"
      },
      "source": [
        "#### Confusion matrix\n",
        "\n",
        "With more than two features we can't plot the decision boundary; however, we can have an idea of the performance of our classification model by looking at the <a href=\"https://en.wikipedia.org/wiki/Confusion_matrix\">confusion matrix</a>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRZFu9myaV-A"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "predictions = model.predict(features_val)\n",
        "predicted_labels = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_labels = target_val.to_numpy().reshape((len(target_val),1))\n",
        "target_labels = np.where(target_labels > 0.5, \"virginica\", \"non-virginica\")\n",
        "\n",
        "con_mat_df = confusion_matrix(target_labels, predicted_labels, labels=[\"non-virginica\",\"virginica\"])\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wWK_HCChKYk"
      },
      "source": [
        "import seaborn as sn\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijg0GxZYzBO6"
      },
      "source": [
        "We can now calculate the **overall accuracy** of the model, and the **TPR** (true positive rate) and the **TNR** (true negative rate):\n",
        "\n",
        "$$\n",
        "\\text{TPR}=\\frac{TP}{TP+FN}\\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{TNR}=\\frac{TN}{TN+FP}\\\\\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy_z4Jzyv3jC"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(target_labels, predicted_labels)\n",
        "tn, fp, fn, tp = con_mat_df.ravel()\n",
        "tpr = tp/(tp+fn)\n",
        "tnr = tn/(tn+fp)\n",
        "\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "print(\"TPR is: \", tpr)\n",
        "print(\"TNR is: \", tnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHsYTUKOJlPT"
      },
      "source": [
        "- going deeper\n",
        "- dl for logistic regression is a bit of an overkill (Ng's slide below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvbdISzP4cZd"
      },
      "source": [
        "## A deeper neural networks model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GS_S3ADGezR"
      },
      "source": [
        "input_shape = (features_train.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes_1 = 8\n",
        "hidden_nodes_2 = 5\n",
        "hidden_activation_1 = 'relu'\n",
        "hidden_activation_2 = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'rmsprop' ## Root Mean Square Propagation\n",
        "num_epochs = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5LeIcq8GUzO"
      },
      "source": [
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(units=hidden_nodes_1, input_shape=input_shape, activation=hidden_activation_1))\n",
        "model.add(Dense(units=hidden_nodes_2, activation=hidden_activation_2))\n",
        "model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsNG5urlOOPm"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6KZ6yFNGvpQ"
      },
      "source": [
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImvhF0tfOaqh"
      },
      "source": [
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqRn6lZUOiE1"
      },
      "source": [
        "predictions = model.predict(features_val)\n",
        "predicted_labels = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_labels = target_val.to_numpy().reshape((len(target_val),1))\n",
        "target_labels = np.where(target_labels > 0.5, \"virginica\", \"non-virginica\")\n",
        "\n",
        "con_mat_df = confusion_matrix(target_labels, predicted_labels, labels=[\"non-virginica\",\"virginica\"])\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbhLwnMoOl6K"
      },
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzQwUbZqOq4s"
      },
      "source": [
        "accuracy = accuracy_score(target_labels, predicted_labels)\n",
        "tn, fp, fn, tp = con_mat_df.ravel()\n",
        "tpr = tp/(tp+fn)\n",
        "tnr = tn/(tn+fp)\n",
        "\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "print(\"TPR is: \", tpr)\n",
        "print(\"TNR is: \", tnr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRmVTlUySgOR"
      },
      "source": [
        "## A side note\n",
        "\n",
        "Neural networks models and deep learning models may be an overkill when applied to a classification problem on 150 samples and 4 features: it is not even guaranteed that they will perform better than simpler machine learning methods like logistic regression. Indeed at times deep learning may perform worse than simpler approaches.\n",
        "As a matter of fact, deep learning is known to work best when the size of the problem is very large (both in terms of amount of data and order of computations needed), and the advent of *Big Data* is exactly one of the drivers behind the rise of deep learning.\n",
        "\n",
        "The Figure below comes from slides by <a href='https://en.wikipedia.org/wiki/Andrew_Ng'>Andrew Ng</a> and illustrates this point: when the amount of data is limited, traditional machine learning methods and small and large neural networks have similar performance. It is only when the size of the problem increases that deep learning shows its potential and consistently outperforms other methods/algorithms.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXL0otE9IyX2"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1_Nom99R963AhM30UbbDLg4-u-mPSiX_L\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSS4Mt2NcNeZ"
      },
      "source": [
        "As a matter of fact, a reliable way to get better predictive performance is often to either **train a bigger network** or feed **more data** to it.\n",
        "Eventually you'll hit the limit: i) run out of training examples; or ii) network so big that it is too slow to train"
      ]
    }
  ]
}