{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"day2_code03 keras multiclass classification.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPFJxBEVbebMPYFCJ3LEHti"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oYvC99Qgqt6L"},"source":["\n","\n","# Multiclass classification using Softmax\n","\n","We now extend logistic regression to multiclas classification using [softmax](https://en.wikipedia.org/wiki/Softmax_function).\n","\n","The problem requires us to do three-classes classification using a Softmax function, which can be easily considered as an extension of logistic regression over three (or more) classes. We use the neural network-like implementation as with binary classification.\n","\n","Luckily, Keras provides a [softmax activation function](https://keras.io/api/layers/activations/#softmax-function), which we will use instead of the logistic we previously used.\n","\n","The structure of our network will be similar, but the output goes from a single number to **three** numbers, one per class, and we thus need three nodes:\n","\n","<img src=\"https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/softmax_neuron.png\">\n","\n","As a result, the loss function will need to change. Remember, loss represents a measure of how good the predictions are. Previously we used binary_crossentropy, but since now predictions are multiclass we need to change function. Luckily Keras provides a natural extension for the multiclass case with [CategoricalCrossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)\n"]},{"cell_type":"markdown","metadata":{"id":"mV7d3uTErn8z"},"source":["## Categorical data\n","\n","As usual, we first import some necessary libraries and load the data: as we did with binary classification, we first take only two features from the dataset."]},{"cell_type":"code","metadata":{"id":"IPv0zhcYsMgV"},"source":["## import libraries\n","import numpy as np\n","import tensorflow as tf\n","import pandas as pd\n","import sklearn.datasets\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YROzh8LRrxpk"},"source":["iris = sklearn.datasets.load_iris()\n","iris.data = pd.DataFrame(iris.data, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n","iris.target = pd.Series(iris.target) #converting numpy array -> pandas Series\n","iris.target = iris.target.to_frame() #converting Pandas series to dataframe\n","\n","features = iris.data.iloc[:,0:2]\n","target = iris.target\n","\n","feature_x = 0\n","feature_y = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VxsFtmDp8yC3"},"source":["We can re-plot the data to check their distribution in the two-feature space:"]},{"cell_type":"code","metadata":{"id":"bHepzfwF8oTx"},"source":["#starting a new plot\n","fig, ax = plt.subplots()\n","\n","#adding data in three bunches of 50, once per class\n","ax.scatter(x=iris.data.iloc[0:50,feature_x],    y=iris.data.iloc[0:50,feature_y],    c='red',   label=iris.target_names[0])\n","ax.scatter(x=iris.data.iloc[50:100,feature_x],  y=iris.data.iloc[50:100,feature_y],  c='green', label=iris.target_names[1])\n","ax.scatter(x=iris.data.iloc[100:150,feature_x], y=iris.data.iloc[100:150,feature_y], c='blue',  label=iris.target_names[2])\n","\n","#the axis names are taken from feature names\n","ax.set_xlabel(iris.feature_names[feature_x])\n","ax.set_ylabel(iris.feature_names[feature_y])\n","\n","#adding the legend and printing the plot\n","ax.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UrkgfuYx9CwZ"},"source":["We now check the dimensions of our input arrays (a sanity check which is important with neural networks and deep learning):"]},{"cell_type":"code","metadata":{"id":"KOCu4sUD8wRM"},"source":["print('Shape of the feature table: ' + str(features.shape))\n","print('Shape of the target array: ' + str(target.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VX1jVMWsmwq"},"source":["The problem is that our target array `iris.target` is a numeric array. But those numbers we used (0, 1, and 2) do not represent real values. In other words, \"virginica\" is not twice \"versicolor\". Numbers here are used as labels, not as quantities.\n","\n","In fact, to properly train a model the structure of the target array must change to [one-hot encoding](https://en.wikipedia.org/wiki/One-hot). In simple terms, it needs to become a table with one row per sample (150 in total) and one column per class (three in total). Something like:\n","\n","| Setosa | Versicolor | Virginica |\n","|------|------|------|\n","|   0  |   1  |   0  |\n","|   1  |   0  |   0  |\n","|   1  |   0  |   0  |\n","|   0  |   0  |   1  |\n","\n","As you can see the first sample is Versicolor, the second and third are Setosa, the last one is Virginica. Note that there is only a single \"one\" per row.\n","\n","Luckily, it's easy to pass to one-hot encoding using keras function [to_categorical](https://keras.io/api/utils/python_utils/#to_categorical-function):\n"]},{"cell_type":"code","metadata":{"id":"fnoxh-5vs7_F"},"source":["#the \"utils\" subpackage is very useful, take a look to it when you have time\n","from tensorflow.keras.utils import to_categorical\n","\n","#converting to categorical\n","target_multi_cat = tf.keras.utils.to_categorical(target)\n","\n","#since everything else is a Pandas dataframe, let's stick to the format\n","#for consistency\n","target_multi_cat = pd.DataFrame(target_multi_cat)\n","\n","#let's take a look\n","print(target_multi_cat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ITSVqAr7tQyF"},"source":["## Training and validation sets\n","\n","We are now ready to create our training and validation sets, as done above:"]},{"cell_type":"code","metadata":{"id":"9u_Io-6ftVUc"},"source":["#we want to have the same proportion of classes in both train and validation sets\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","test_pct = 0.2\n","\n","#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n","#assigned to validation set (here called \"test\")\n","#random_state is used to control class balance between training and test sets (None to switch to random behavior)\n","sss = StratifiedShuffleSplit(n_splits=1, test_size= test_pct, random_state=0)\n","\n","for train_index, val_index in sss.split(features, target_multi_cat):\n","    features_train = features.iloc[train_index, :]\n","    features_val   = features.iloc[val_index, :]\n","    target_train   = target_multi_cat.iloc[train_index, :]\n","    target_val     = target_multi_cat.iloc[val_index, :]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7FWwIxGt1a1"},"source":["Just a little check:"]},{"cell_type":"code","metadata":{"id":"tWYIPsoDt3cv"},"source":["#shapes\n","print(features_train.shape)\n","print(features_val.shape)\n","print(target_train.shape)\n","print(target_val.shape)\n","\n","#number of classes per split\n","print('\\nClasses in train set:')\n","print(target_train.sum())\n","print('\\nClasses in validation set:')\n","print(target_val.sum())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5EQXmDHjuJ_C"},"source":["We have now a balanced dataset, with 40 instances for each class in the training set and 10 in the validation set."]},{"cell_type":"markdown","metadata":{"id":"MMYSgINdNGHI"},"source":["## Set up\n","\n","We define here the hyperparameters of the model"]},{"cell_type":"code","metadata":{"id":"B88ql-inMRxE"},"source":["num_classes = 3\n","input_shape = features_train.shape[1]\n","activation_function = 'softmax'\n","optimising_method = 'rmsprop'\n","loss_function = 'categorical_crossentropy'\n","num_epochs = 200"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVZZ1dFarX10"},"source":["## A multiclass model\n","\n","We are now ready to declare our multiclass classification model: we use `Keras`, but this is equivalent to multiclass logistic regression (only with neural networks-like representation). The output layer has three units, corresponding to the three classes:"]},{"cell_type":"code","metadata":{"id":"fic85viHqtVB"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# 3-class softmax regression in Keras\n","model_multi = Sequential()\n","model_multi.add(Dense(units = num_classes, activation=activation_function, input_dim=input_shape))\n","\n","#compile the model specifying the new multiclass loss\n","model_multi.compile(optimizer=optimising_method, loss=loss_function)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVDzz9Hnrh8W"},"source":["Let's take a look under the hood:"]},{"cell_type":"code","metadata":{"id":"VcP7R4JrrbKr"},"source":["print(model_multi.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GX2raO_-rl3F"},"source":["We now have to train 9 parameters: 3 coefficients (W1, W2 B) times three nodes."]},{"cell_type":"markdown","metadata":{"id":"XT912ksQuUxf"},"source":["## Fitting the model \n","\n","We are ready to fit the model. This time we go directly to 500 epochs, trained in silent mode. We then plot the loss function evolution."]},{"cell_type":"code","metadata":{"id":"DvtbANHaulJl"},"source":["history_multi = model_multi.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OUiWK0hxvX3V"},"source":["#function to take a look at losses evolution\n","def plot_loss_history(h, title):\n","    plt.plot(h.history['loss'], label = \"Train loss\")\n","    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n","    plt.xlabel('Epochs')\n","    plt.title(title)\n","    plt.legend()\n","    plt.show()\n","\n","plot_loss_history(history_multi, 'Softmax multiclass ({} epochs)'.format(num_epochs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hH_hiUKttHqo"},"source":["This looks promising. There's the clear same pattern we saw with logistic regression, with a strong improvement in the first hundred epochs (and then things become slow...)"]},{"cell_type":"markdown","metadata":{"id":"WwJcq2WytQcH"},"source":["## Decision boundary\n","\n","We want now to plot again the decision boundary. Unfortunately `plot_decision_regions` function from [mlxtend](http://rasbt.github.io/mlxtend/) module does not support one-hot encoded multiclasses natively. Luckily [there's a quick workaround](https://www.machinecurve.com/index.php/2019/10/17/how-to-use-categorical-multiclass-hinge-with-keras/#visualizing-the-decision-boundary), but if you get lost in the code don't worry and just look at the plot :) "]},{"cell_type":"code","metadata":{"id":"ySRsRvoktXdc"},"source":["#we define a class to take the Keras model and convert its predictions\n","#from \"one probability per iris type\" to \"just the iris type with the highest probability\"\n","class Onehot2Int(object):\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def predict(self, X):\n","        y_pred = self.model.predict(X)\n","        return np.argmax(y_pred, axis=1)\n","\n","#we wrap our trained model, instantiating a new object\n","keras_model_no_ohe = Onehot2Int(model_multi)\n","\n","#and we can now plot the decision boundary safely (we still need to convert\n","#the target one-hot-encoded matrix to int, though)\n","from mlxtend.plotting import plot_decision_regions\n","\n","plot_decision_regions(features_train.to_numpy(), np.argmax(target_train.to_numpy(), axis=1), \n","                      clf=keras_model_no_ohe)\n","plt.title('Decision boundary for 0 (setosa) vs 1 (versicolor) vs 2 (virginica)')\n","plt.xlabel(iris.feature_names[feature_x])\n","plt.ylabel(iris.feature_names[feature_y])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOZHtkJ3vid2"},"source":["### Predictions\n","\n","We now look at predictions in the **test set**: each test sample is assigned to the class with the highest probability.\n","\n","Predicted classes are then compared to true classes in a confusion matrix"]},{"cell_type":"code","metadata":{"id":"Eh5rmApBvkxQ"},"source":["from sklearn.metrics import confusion_matrix\n","\n","predictions = model_multi.predict(features_val)\n","print(predictions)\n","\n","predicted_classes = np.argmax(predictions,axis=1)\n","predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n","\n","target_classes = target.iloc[val_index].to_numpy()\n","\n","con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2])\n","print(\"\\nConfusion matrix:\")\n","print(con_mat_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2O6Qmf3L7y6a"},"source":["import seaborn as sn\n","\n","figure = plt.figure(figsize=(8, 8))\n","sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n","plt.tight_layout()\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_yn965L876q-"},"source":["from sklearn.metrics import accuracy_score\n","accuracy = accuracy_score(target_classes, predicted_classes)\n","print(\"Overall accuracy is: \", accuracy)\n","\n","confusion_matrix(target_classes, predicted_classes, normalize='true', labels=[0,1,2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95rhqPISbszq"},"source":["# Shallow neural networks for softmax regression\n","\n","We now move on from multinomial logistic regression to softmax regression with neural networks: again, we start by implementing a shallow neural netowrk model.\n","\n","First, we select all four features from the dataset and configure some basic parameters:"]},{"cell_type":"code","metadata":{"id":"dTXwA5oBflKf"},"source":["## select all 4 features\n","features = iris.data.iloc[:,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dKMvwggcGe0"},"source":["## # Configuration options\n","num_classes = 3\n","input_shape = (features.shape[1],)\n","hidden_nodes = 8\n","hidden_activation = 'relu'\n","output_activation = 'softmax'\n","loss_function = 'categorical_crossentropy'\n","optimizer_used = 'rmsprop' ## or keras.optimizers.adam(lr=0.001)? maybe for softmax regression?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSPBwa-FfdmB"},"source":["# 3-class softmax regression in Keras\n","model_multi = Sequential()\n","model_multi.add(Dense(units=hidden_nodes, input_shape=input_shape, activation=hidden_activation))\n","model_multi.add(Dense(num_classes, activation='softmax'))\n","\n","#compile the model specifying the new multiclass loss\n","model_multi.compile(optimizer='rmsprop', loss=loss_function)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ARQwD-DfgHj"},"source":["print(model_multi.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FJRXmm9YglNd"},"source":["We see that we have 67 parameters to train: 5 parameters (w1 ... w4, b) times 8 nodes in the hidden layer; 9 parameters (8 units + b) times three nodes in the output layer.\n","\n","## Training and test sets\n","\n","We split the data in the training and test sets:"]},{"cell_type":"code","metadata":{"id":"u8yMNyp3gkNJ"},"source":["#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n","#assigned to validation set (here called \"test\")\n","sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n","\n","#the .split() method returns (an iterable over) two lists which can be\n","#used to index the samples that go into train and validation sets\n","for train_index, val_index in sss.split(features, target_multi_cat):\n","    features_train = features.iloc[train_index, :]\n","    features_val   = features.iloc[val_index, :]\n","    target_train   = target_multi_cat.iloc[train_index,:]\n","    target_val     = target_multi_cat.iloc[val_index,:]\n","    \n","#let's print some shapes to get an idea of the resulting data structure\n","print(\"Training features size: \", features_train.shape)\n","print(\"Test features size: \", features_val.shape)\n","print(\"Training targets size: \", target_train.shape)\n","print(\"Test targets size: \", target_val.shape)\n","\n","print(\"Type of the training features object: \", type(features_train))\n","print(\"Type of the training targets object: \", type(target_train))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRG3rxbnhFPm"},"source":["#number of classes per split\n","print('\\nClasses in train set:')\n","print(target_train.sum())\n","print('\\nClasses in validation set:')\n","print(target_val.sum())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vp4EALtwhfUL"},"source":["history_multi = model_multi.fit(features_train, target_train, epochs=100, \n","                     validation_data=(features_val, target_val), verbose=0)\n","\n","plot_loss_history(history_multi, 'Softmax multiclass (100 epochs)')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GZJG_vlRhlwa"},"source":["from sklearn.metrics import confusion_matrix\n","\n","predictions = model_multi.predict(features_val)\n","print(predictions)\n","\n","predicted_classes = np.argmax(predictions,axis=1)\n","target_classes = target.iloc[val_index,:].to_numpy()\n","con_mat_df = confusion_matrix(target_classes, predicted_classes, labels=[0,1,2])\n","print(con_mat_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nl1XGbfHh4jS"},"source":["import seaborn as sn\n","\n","figure = plt.figure(figsize=(8, 8))\n","sn.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n","plt.tight_layout()\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5af7Zgdch8ED"},"source":["accuracy = accuracy_score(target_classes, predicted_classes)\n","print(\"Overall accuracy is: \", accuracy)\n","\n","confusion_matrix(target_classes, predicted_classes, normalize='true', labels=[0,1,2])"],"execution_count":null,"outputs":[]}]}