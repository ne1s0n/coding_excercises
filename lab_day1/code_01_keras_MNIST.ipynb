{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"code_01_keras_MNIST.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tUy9M38H59ta"},"source":["# A deep-learning neural network for image recognition\n","We present here a `Python Keras` implementation of a deep learning neural network for image recognition.\n","The publicly available `MNIST` dataset is used.\n","\n","`Keras` is a popular `Python` library for deep learning models:\n","- wrapper for `TensorFlow`\n","- minimalistic\n","- modular\n","- easy to implement\n","\n","The `MNIST` database (Modified National Institute of Standards and Technology database) is a large database of hand-written digits (details and data [here](http://yann.lecun.com/exdb/mnist/)):\n","\n","![mnist](https://drive.google.com/uc?id=1KNK3-8qahQixvL-StpDAs6GoOUAHKSDy)\n","\n","Deep learning consists of neural networks with multiple hidden layers that learn increasingly abstract and complex representations of the input data.\n","For instance, if we train a deep learning model to recognize hand-written digits (images):\n","\n","- the first hidden layers might only learn local edge patterns;\n","- subsequent layers learns more complex representations of the data;\n","- the last layer will classify the image as one of ten digits.\n","\n","For image recognition we use a specific deep learning architecture: **convolutional neural networks** (*CNN*), which assume that input data are images, thereby greatly reducing the number of model parameters to be tuned (more on *CNN's* later in the course).\n"]},{"cell_type":"markdown","metadata":{"id":"EG18PhUOLMFk"},"source":["## Importing libraries"]},{"cell_type":"markdown","metadata":{"id":"fqG72eSnLJYC"},"source":["We import the necessary libraries to build a DL NN for image recognition:\n","\n","- import the Sequential model type from Keras: linear stack of neural network layers, to be used to build a feed-forward CNN\n","-  import the 'core' layers from Keras: layers that are used in almost any neural network\n","- import the CNN layers from Keras: convolutional layers to train the model on image data\n","- load the MNIST dataset"]},{"cell_type":"code","metadata":{"id":"bh-2gj8A5WAM","executionInfo":{"status":"ok","timestamp":1601208201995,"user_tz":-120,"elapsed":2881,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}}},"source":["import keras.utils\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K # needed for image_data_format()\n","from keras.datasets import mnist"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MbGoAqrV55W5"},"source":["#Configuration parameters\n","\n","Define model parameters:\n","\n","- batch size: DL models typically do not process the entire dataset at once, rather break it in **batches**\n","- n. of classes: n. of classes to predict (10 digits, in the MNIST problem)\n","- n. of epochs: n. of **iterations** over the entire dataset"]},{"cell_type":"code","metadata":{"id":"AGrzGdEC5wjm","executionInfo":{"status":"ok","timestamp":1601208202007,"user_tz":-120,"elapsed":2886,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}}},"source":["batch_size = 128 # n. of samples/records in each batch\n","num_classes = 10 # ten digits -> ten classes\n","epochs = 12\n","\n","# input image dimensions (pixels)\n","img_rows, img_cols = 28, 28"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rG7RUXrA52gj"},"source":["# Input data\n","\n","We load the data from the MNIST dataset, and assign them to the training and testing sets. \n","\n","Image data is generally harder to work with than flat relational data. The MNIST dataset is a beginner-friendly intoduction to working with image data: it contains $70\\,000$ labeled images of handwritten digits. These are grey-scale images, 28 x 28 pixels.\n","\n","The MNIST dataset comprises $60\\,000$ training observations and $10\\,000$ test observations: the function `load_data()` automatically assigns these to the training and testing sets."]},{"cell_type":"code","metadata":{"id":"uaLJe1Vn0lXI","executionInfo":{"status":"ok","timestamp":1601208435282,"user_tz":-120,"elapsed":1177,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"2e9df4ad-b092-4614-d68c-e318c1a8438c","colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["# the data, split between train and test sets\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","print(\"Size of the training set\")\n","print(X_train.shape)\n","print(\"Size of the test set\")\n","print(X_test.shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Size of the training set\n","(60000, 28, 28)\n","Size of the test set\n","(10000, 28, 28)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rgVK5wI4GetE"},"source":["Data have been split into a **training** and a **testing set**, and within these into a **three-dimensional array** $X$ of **features** (samples x pixels x pixels) and a vector $y$ of labels (0-9 digits).\n","\n","Each record in the 3-D array $X$ is a 28 x 28 matrix of grayscale intensities (1 byte = 8 bits = 0 - 255 values). Grayscale (black-n-white) images only use one color channel. Colour images use three channels (e.g. RGB) and each image (record) is therefore a 3-D matrix (pixels x pixels x 3)."]},{"cell_type":"code","metadata":{"id":"Q7wusZdU5FDX","executionInfo":{"status":"ok","timestamp":1601137862974,"user_tz":-120,"elapsed":1255,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"74130445-f329-4255-b9a7-37d48b73b3fe","colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["print(\"First training label: \",y_train[0])\n","\n","from matplotlib import pyplot as plt\n","plt.imshow(X_train[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First training label:  5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f5490e85a58>"]},"metadata":{"tags":[]},"execution_count":5},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"-kfxPKriPOjO"},"source":["By default the matplotlib function `imshow()` uses pseudocolors to plot grayscale images; if you want to display the actual grayscale image, you can specify the color mapping parameters:"]},{"cell_type":"code","metadata":{"id":"RFn4vTd3PEwl","executionInfo":{"status":"ok","timestamp":1601137877396,"user_tz":-120,"elapsed":1220,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"d737bc1b-f824-4b26-9396-9d498affc1d7","colab":{"base_uri":"https://localhost:8080/","height":301}},"source":["print(\"First training label: \",y_train[0])\n","plt.imshow(X_train[0], cmap='gray', vmin=0, vmax=255)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["First training label:  5\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f549095ad68>"]},"metadata":{"tags":[]},"execution_count":6},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"_1i4lpSr9IbD"},"source":["# Data preprocessing\n","\n","First, we need to explicitly declare the depth of the image representation array: in the case of grayscale images there is only one channel, and this dimension is 1.\n","\n","We use the utility function [image_data_format()](https://keras.io/api/utils/backend_utils#imagedataformat-function) from keras [backend utilities](https://keras.io/api/utils/backend_utils/) to discover the convention ('channels_first' or 'channels_last') of our current system.\n","\n","Depending on the backend (Theano or TensorFlow), the depth dimension is either the first or the last to be declared:"]},{"cell_type":"code","metadata":{"id":"Y0iC2KdoCyGv","executionInfo":{"status":"ok","timestamp":1601208461080,"user_tz":-120,"elapsed":921,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"3cf11129-1e35-4bd3-aad1-bbd6faeb94a2","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["if K.image_data_format() == 'channels_first':\n","    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n","    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n","    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","print(\"Modified array dimensions:\")\n","print(X_train.shape) \n","print(input_shape)  \n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Modified array dimensions:\n","(60000, 28, 28, 1)\n","(28, 28, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uvOBH3WoLy6p"},"source":["We then convert the input data type to `float32` and normalize the data values to the range $[0, 1]$.\n","These are operational modifications necessary to speed up and optimize the calculations.\n","\n","Finally, label vectors are converted to binary class matrices. This serves to convert a vector of numerical digits to a matrix of ten classes per observation, which is a better suited representation for a classification problem."]},{"cell_type":"code","metadata":{"id":"ER8vhNco50si","executionInfo":{"status":"ok","timestamp":1601137886744,"user_tz":-120,"elapsed":1068,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"25d95214-f7a7-47db-9e29-747a88cff275","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["X_train = X_train.astype('float32')\n","X_test = X_test.astype('float32')\n","X_train /= 255 #max value of pixel intensity\n","X_test /= 255 #max value of pixel intensity\n","print('x_train shape:', X_train.shape)\n","print(X_train.shape[0], 'train samples')\n","print(X_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices (also known as OHE - One Hot Encoding)\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8pg9tUjtQX2d","executionInfo":{"status":"ok","timestamp":1601137942772,"user_tz":-120,"elapsed":513,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"d3d75be0-1d42-4cda-b5ae-1b1714d166a1","colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["print(y_train[0:4]) ## print first four training examples (labels): which digits are these?"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KlUJzLw46FvK"},"source":[" # Model building\n","\n","We now define our deep-learning **neural network architecture**, and start building our model for image recognition.\n","\n","First, we declare a [sequential model](https://keras.io/guides/sequential_model/), that is a sequence of layers each with one input tensor and one output tensor. \n","Then we add a first convolutional layer ([Conv2D](https://keras.io/api/layers/convolution_layers/convolution2d/)) to our model; parameters are:\n","\n","- number of convolution filters (n. of kernels to convolve with the input data)\n","- number of rows and columns in each convolution kernel\n","- type of activation function\n","- shape of the input array\n"]},{"cell_type":"code","metadata":{"id":"udCTMi2JWaY3","executionInfo":{"status":"ok","timestamp":1601140676422,"user_tz":-120,"elapsed":875,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"593fc628-89b8-4dc8-b17e-055a0f3dcd1d","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model = Sequential()\n","model.add(\n","          Conv2D(32, kernel_size=(3, 3),\n","          activation='relu',\n","          input_shape=input_shape))\n","\n","print(model.output_shape) ## convolutional \"padding\" (28-2 x 28-2) + 32 kernels"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(None, 26, 26, 32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"y68JuKReaLOg"},"source":["The input shape is (60000, 28, 28, 1): 28 x 28 pixels, times 1 channel (grayscale), per 60,000 training samples.\n","The convolutional output shape is:\n","\n","- None: not yet any samples trained (to be added later)\n","- 26 x 26: convolutional padding (3x3 kernel size $\\rightarrow$ 28-2 x 28-2)\n","- 32: n. of convolutional filters (kernels)"]},{"cell_type":"markdown","metadata":{"id":"yC8qRgETmd00"},"source":["Then we can add more layers to the deep-learning model:\n","\n","- the [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) layer is a way to regularize our model to prevent overfitting\n","- [MaxPooling2D](https://keras.io/api/layers/pooling_layers/max_pooling2d/) is a way to reduce the number of model parameters by sliding a 2x2 pooling filter across the previous layer and taking the max of the 4 values\n","- a [Dense](https://keras.io/api/layers/core_layers/dense/) layer whose first parameter is the output size of the layer (weights from the Convolution layers must be flattened -made 1-dimensional- before being passed on to the fully connected Dense layer)\n","- the final layer has an output size of 10 (the 10 classes of digits): the activation function here is [softmax](https://keras.io/api/layers/activations/#softmax-function) (the multiclass analog of the logistic function) which returns a probability for each class, e.g. 10% of chance of the sample belonging to class 1, 15% for class 2 and so forth. The sum of all probabilities adds to 100%"]},{"cell_type":"code","metadata":{"id":"_wPciJ0ambwV"},"source":["model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BF9yHLD4sl4T"},"source":["### Compiling the model\n","When compiling the model we specify the **loss function** (here: [categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)) and the **optimizer** (here: [Adadelta](https://keras.io/api/optimizers/adadelta/))"]},{"cell_type":"code","metadata":{"id":"9COW8p3Z6IBQ"},"source":["model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FKeXuJK86QCJ"},"source":["# Training the deep-learning model\n","\n","We then fit the model on the training data, specifying:\n","\n","- the batch size\n","- the number of epochs to train the model"]},{"cell_type":"code","metadata":{"id":"tv_dGPGu6RWx","executionInfo":{"status":"ok","timestamp":1601140776309,"user_tz":-120,"elapsed":83058,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"673dc9cd-0cd2-44c4-8531-dd8661cdb04a","colab":{"base_uri":"https://localhost:8080/","height":467}},"source":["model.fit(X_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/12\n","469/469 [==============================] - 6s 14ms/step - loss: 2.2681 - accuracy: 0.1697\n","Epoch 2/12\n","469/469 [==============================] - 6s 13ms/step - loss: 2.1905 - accuracy: 0.2946\n","Epoch 3/12\n","469/469 [==============================] - 6s 13ms/step - loss: 2.0861 - accuracy: 0.4054\n","Epoch 4/12\n","469/469 [==============================] - 6s 13ms/step - loss: 1.9511 - accuracy: 0.4871\n","Epoch 5/12\n","469/469 [==============================] - 6s 13ms/step - loss: 1.7834 - accuracy: 0.5530\n","Epoch 6/12\n","469/469 [==============================] - 6s 14ms/step - loss: 1.6010 - accuracy: 0.5991\n","Epoch 7/12\n","469/469 [==============================] - 7s 14ms/step - loss: 1.4271 - accuracy: 0.6317\n","Epoch 8/12\n","469/469 [==============================] - 6s 13ms/step - loss: 1.2763 - accuracy: 0.6591\n","Epoch 9/12\n","469/469 [==============================] - 6s 13ms/step - loss: 1.1550 - accuracy: 0.6791\n","Epoch 10/12\n","469/469 [==============================] - 6s 13ms/step - loss: 1.0561 - accuracy: 0.6985\n","Epoch 11/12\n","469/469 [==============================] - 6s 14ms/step - loss: 0.9792 - accuracy: 0.7147\n","Epoch 12/12\n","469/469 [==============================] - 6s 13ms/step - loss: 0.9177 - accuracy: 0.7283\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5480447f98>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"G7n-6rZQ6TWk"},"source":["# Model evaluation\n","\n","We can now measure the performance (in terms of prediction accuracy) of the trained deep-learning model for image recognition. \n","To measure the performance, we applied our trained model to independent test data."]},{"cell_type":"code","metadata":{"id":"e0ZyOafK6UnN","executionInfo":{"status":"ok","timestamp":1601140884708,"user_tz":-120,"elapsed":2497,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"8a192df4-4c39-4744-8b7e-ad884a442e49","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["score = model.evaluate(X_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test loss: 0.6744098663330078\n","Test accuracy: 0.8392000198364258\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k3Rx5ww3ZfsO"},"source":["# Confusion matrix\n","\n","A [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is another way to express the accuracy of your predictions. It's a square matrix, with as many rows (and columns) as your classes. Rows represent *true values* and columns represent *predicted values*. On the main diagonal are thus reported the correct predictions, while off-diagonal elements represent errors.\n","\n","We'll use the [confusion_matrix()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) function part of [scikit-learn library](https://scikit-learn.org/stable/)."]},{"cell_type":"code","metadata":{"id":"bHY9mr6OXvDh","executionInfo":{"status":"ok","timestamp":1601140891551,"user_tz":-120,"elapsed":2275,"user":{"displayName":"Filippo Biscarini","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg7jR0XlLn_xe6aJQRo3t-i8pd96QODPh7zNo_l=s64","userId":"09590256246541826137"}},"outputId":"319084a8-7aaa-4b3d-aae5-3160232082bc","colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["#asking our model to return its predictions for the test set\n","predictions = model.predict(X_test)\n","\n","#confusion_matrix function requires actual classes labels (expressed as int)\n","#and not probabilities as we handled so far\n","predicted_classes = predictions.argmax(axis=1)\n","true_classes = y_test.argmax(axis=1)\n","\n","#rows are true values, columns are predicted values, numbering starts from zero\n","import sklearn.metrics\n","sklearn.metrics.confusion_matrix(true_classes, predicted_classes)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 930,    0,    1,    3,    0,    2,   26,    1,   17,    0],\n","       [   0, 1085,    8,   14,    0,    1,    5,    0,   22,    0],\n","       [  19,   10,  866,   23,   14,    0,   42,   24,   32,    2],\n","       [   3,   11,   29,  884,    0,   17,    6,   27,   20,   13],\n","       [   3,    9,    4,    0,  768,    0,   22,    1,   12,  163],\n","       [  24,   14,    4,  149,   30,  570,   38,    9,   45,    9],\n","       [  25,   14,    7,    0,    9,    9,  890,    0,    4,    0],\n","       [   2,   40,   28,    3,   11,    0,    0,  886,    7,   51],\n","       [  15,   34,   11,   78,   11,   24,   24,   23,  712,   42],\n","       [  19,   20,    6,   15,   88,    9,    1,   41,    9,  801]])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"FSFXg6obcHl4"},"source":["Can you spot the most ambiguous, often confounded classes?"]}]}