Things I (Nelson) would improve:

* more focus on learning rate, show the effect of too big/too small
	* since we are at it, give [learning rate autofinder](https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/ ) a go 
* show practical examples of bad cases:
    * optimizer is not working, loss keeps growing (what happens changing)
    * actual overfitting, effect of regularization
* have a single dataset that should be used along the whole course (or a couple: an easy one and a difficult one)
* consider these topics:
    * batch normalization
    * learning rate decay
    * 1D convolution for time-series and genotypes
* keep the drawing with the pen agile. One student told that sometimes it took too much time
* more guided/shared exercises. The teacher writes, the student tell what to do
* a fifth day?
* complete prep exercises on normalization (and maybe others)
