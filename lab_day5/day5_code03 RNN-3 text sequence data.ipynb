{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"day5_code03 RNN-3 text sequence data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO6zQ0/4/kqoyem/rnHxqap"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZvwnP6Dky_8W"},"source":["## RNN models for text data\n","\n","We analyse here data from the Internet Movie Database (IMDB: https://www.imdb.com/).\n","\n","We use RNN to build a classifier for movie reviews: given the text of a review, the model will predict whether it is a positive or negative review.\n","\n","#### Steps\n","\n","1. Load the dataset (50K IMDB Movie Review)\n","2. Clean the dataset\n","3. Encode the data\n","4. Split into training and testing sets\n","5. Tokenize and pad/truncate reviews\n","6. Build the RNN model\n","7. Train the model\n","8. Test the model\n","9. Applications\n"]},{"cell_type":"code","metadata":{"id":"NnRXCH49y6_Q"},"source":["## import relevant libraries\n","\n","import re\n","import nltk\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import itertools\n","import matplotlib.pyplot as plt\n","\n","from scipy import stats\n","from keras.datasets import imdb\n","\n","from nltk.corpus import stopwords   # to get collection of stopwords\n","from sklearn.model_selection import train_test_split       # for splitting dataset\n","from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n","from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n","from tensorflow.keras.models import Sequential     # the model\n","from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n","from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n","from tensorflow.keras.models import load_model   # load saved model\n","\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7LGxCn73Try"},"source":["#### Reading the data\n","\n","We raw an extract from IMDB hosted on a Github page:"]},{"cell_type":"code","metadata":{"id":"EkJTNbf1gMOk"},"source":["DATAURL = 'https://raw.githubusercontent.com/hansmichaels/sentiment-analysis-IMDB-Review-using-LSTM/master/IMDB%20Dataset.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuDgk_M8g8SJ"},"source":["\n","data = pd.read_csv(DATAURL)\n","print(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"meRL6w6Z5HTp"},"source":["## alternative way of getting the data, already preprocessed\n","# (X_train,Y_train),(X_test,Y_test) = imdb.load_data(path=\"imdb.npz\",num_words=None,skip_top=0,maxlen=None,start_char=1,seed=13,oov_char=2,index_from=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D2yaSrpD5PRu"},"source":["#### Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"-_TkfRtV5AAV"},"source":["The original reviews are \"dirty\", they contain html tags, punctuation, uppercase, stop words etc. which are not good for model training. \n","Therefore, we now need to clean the dataset.\n","\n","**Stop words** are commonly used words in a sentence, usually to be ignored in the analysis (i.e. \"the\", \"a\", \"an\", \"of\", etc.)"]},{"cell_type":"code","metadata":{"id":"0OOi_ROPhGWA"},"source":["english_stops = set(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0uB2FTq_3xH7"},"source":["[x[1] for x in enumerate(itertools.islice(english_stops, 10))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dB2lpEssuVI6"},"source":["def prep_dataset():\n","    x_data = data['review']       # Reviews/Input\n","    y_data = data['sentiment']    # Sentiment/Output\n","\n","    # PRE-PROCESS REVIEW\n","    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n","    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n","    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n","    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n","    \n","    # ENCODE SENTIMENT -> 0 & 1\n","    y_data = y_data.replace('positive', 1)\n","    y_data = y_data.replace('negative', 0)\n","\n","    return x_data, y_data\n","\n","x_data, y_data = prep_dataset()\n","\n","print('Reviews')\n","print(x_data, '\\n')\n","print('Sentiment')\n","print(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fEoL34Peu5F0"},"source":["#### Split dataset\n","\n","`train_test_split()` function to partition the data in 80% training and 20% test sets"]},{"cell_type":"code","metadata":{"id":"NyAK4VQnu9eb"},"source":["x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JrvyhnH17hDu"},"source":["#### A little bit of EDA"]},{"cell_type":"code","metadata":{"id":"7VvJCCug5bNE"},"source":["print(\"x train shape: \",x_train.shape)\n","print(\"y train shape: \",y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqxSIAPO8vuo"},"source":["print(\"x test shape: \",x_test.shape)\n","print(\"y test shape: \",y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbNWCRqD82Fe"},"source":["Distribution of classes in the training set"]},{"cell_type":"code","metadata":{"id":"yiu43jBxyXqK"},"source":["plt.figure();\n","sns.countplot(y_train);\n","plt.xlabel(\"Classes\");\n","plt.ylabel(\"Frequency\");\n","plt.title(\"Y Train\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8cFqAVr01iul"},"source":["review_len_train = []\n","review_len_test = []\n","for i,j in zip(x_train,x_test):\n","    review_len_train.append(len(i))\n","    review_len_test.append(len(j))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AOeC4snE2lvz"},"source":["print(\"min train: \", min(review_len_train), \"max train: \", max(review_len_train))\n","print(\"min test: \", min(review_len_test), \"max test: \", max(review_len_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTFU56qPxbsT"},"source":["#### Tokenize and pad/truncate\n","\n","RNN models only accept numeric data, so we need to encode the reviews. `tensorflow.keras.preprocessing.text.Tokenizer` is used to encode the reviews into integers, where each unique word is automatically indexed (using `fit_on_texts`) based on the training data\n","\n","x_train and x_test are converted to integers using `texts_to_sequences`\n","\n","Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length): `tensorflow.keras.preprocessing.sequence.pad_sequences`\n","\n"]},{"cell_type":"code","metadata":{"id":"5YJKZkiX9WlC"},"source":["def get_max_length():\n","    review_length = []\n","    for review in x_train:\n","        review_length.append(len(review))\n","\n","    return int(np.ceil(np.mean(review_length)))\n","\n","# ENCODE REVIEW\n","token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n","token.fit_on_texts(x_train)\n","x_train = token.texts_to_sequences(x_train)\n","x_test = token.texts_to_sequences(x_test)\n","\n","max_length = get_max_length()\n","\n","x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n","x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n","\n","## size of vocabulary\n","total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n","\n","print('Encoded X Train\\n', x_train, '\\n')\n","print('Encoded X Test\\n', x_test, '\\n')\n","print('Maximum review length: ', max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvmcsvj0hb5E"},"source":["x_train[0,0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f0m3Tsp6xyO-"},"source":["#### Build model\n","\n","**Embedding Layer**: it creates word vectors of each word in the vocabulary, and group words that are related or have similar meaning by analyzing other words around them\n","\n","**LSTM Layer**: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n","\n","- *Forget Gate*, decides information is to be kept or thrown away\n","- *Input Gate*, updates cell state by passing previous output and current input into sigmoid activation function\n","- *Cell State*, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.\n","- *Ouput Gate*, decides the next hidden state and used for predictions\n","\n","**Dense Layer**: compute the input from the LSTM layer and uses the sigmoid activation function because the output is only 0 or 1\n","\n"]},{"cell_type":"code","metadata":{"id":"oxifTWPk9jVa"},"source":["# ARCHITECTURE\n","\n","model = Sequential()\n","model.add(Embedding(total_words, 32, input_length = max_length))\n","model.add(LSTM(64))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAbxJ5gxyC3e"},"source":["#### Training the model\n","\n","For training we fit the x_train (input) and y_train (output/label) data to the RNN model. \n","We use a mini-batch learning method with a batch_size of 128 and 5 epochs\n"]},{"cell_type":"code","metadata":{"id":"eSbW8xEo9l40"},"source":["num_epochs = 5\n","batch_size = 128\n","\n","checkpoint = ModelCheckpoint(\n","    'models/LSTM.h5',\n","    monitor='accuracy',\n","    save_best_only=True,\n","    verbose=1\n",")\n","\n","history = model.fit(x_train, y_train, batch_size = batch_size, epochs = num_epochs, callbacks=[checkpoint])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Be28nXrNzPne"},"source":["plt.figure()\n","plt.plot(history.history[\"accuracy\"],label=\"Train\");\n","plt.title(\"Accuracy\")\n","plt.ylabel(\"Accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.legend()\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KriRus7uGo3I"},"source":["#### Testing"]},{"cell_type":"code","metadata":{"id":"5XZo7UlazYRO"},"source":["from sklearn.metrics import confusion_matrix\n","\n","predictions = model.predict(x_test)\n","predicted_labels = np.where(predictions > 0.5, \"good review\", \"bad review\")\n","\n","target_labels = y_test\n","target_labels = np.where(target_labels > 0.5, \"good review\", \"bad review\")\n","\n","con_mat_df = confusion_matrix(target_labels, predicted_labels, labels=[\"bad review\",\"good review\"])\n","print(con_mat_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BscjHxahGmUn"},"source":["y_pred = np.where(predictions > 0.5, 1, 0)\n","\n","true = 0\n","for i, y in enumerate(y_test):\n","    if y == y_pred[i]:\n","        true += 1\n","\n","print('Correct Prediction: {}'.format(true))\n","print('Wrong Prediction: {}'.format(len(y_pred) - true))\n","print('Accuracy: {}'.format(true/len(y_pred)*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1DxZQ-WhHJiz"},"source":["### A little application\n","\n","Now we feed a new review to the trained RNN model, to see whether it will be classified positive or negative.\n","\n","We go through the same preprocessing (cleaning, tokenizing, encoding), and then move directly to the predcition step (the RNN model has already been trained, and it has high accuracy from cross-validation)."]},{"cell_type":"code","metadata":{"id":"WA3ydUMDHJN6"},"source":["loaded_model = load_model('models/LSTM.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lohg7VySVcqj"},"source":["review = 'Movie Review: Nothing was typical about this. Everything was beautifully done in this movie, the story, the flow, the scenario, everything. I highly recommend it for mystery lovers, for anyone who wants to watch a good movie!'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n9KPYoZJWVv2"},"source":["# Pre-process input\n","regex = re.compile(r'[^a-zA-Z\\s]')\n","review = regex.sub('', review)\n","print('Cleaned: ', review)\n","\n","words = review.split(' ')\n","filtered = [w for w in words if w not in english_stops]\n","filtered = ' '.join(filtered)\n","filtered = [filtered.lower()]\n","\n","print('Filtered: ', filtered)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpqzYgtqXr4-"},"source":["tokenize_words = token.texts_to_sequences(filtered)\n","tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n","print(tokenize_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lz3AXgFuXute"},"source":["result = loaded_model.predict(tokenize_words)\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrMAYJV8XzF3"},"source":["if result >= 0.7:\n","    print('positive')\n","else:\n","    print('negative')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wiFNST4Hiypu"},"source":["## Exercise\n","\n","Try to write your own movie review, and then have the deep learning model classify it.\n","\n","0. write your review\n","1. clean the text data\n","2. tokenize it\n","3. predict and evaluate"]},{"cell_type":"code","metadata":{"id":"ak9BaixwjTYj"},"source":[""],"execution_count":null,"outputs":[]}]}