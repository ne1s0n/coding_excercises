{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"1h7v_W0Tct13"},"source":["# Segmentation metrics\n","\n","In this excercise we'll explore data typical of segmentation problems, plus two very common metrics used to measure segmentation performances.\n","\n","Assignments:\n","\n","* load the four images (URLs are provided)\n","* inspect the images\n","* transform the three masks to binary (zero/one) matrices\n","* measure accuracy for truth-algo1 and truth-algo2\n","* measure IoU (intersection over union)\n","* measure Dice coefficient"]},{"cell_type":"markdown","metadata":{"id":"_TOMbQ6uXZuQ"},"source":["# Image paths\n","\n","We'll work with the following files:"]},{"cell_type":"code","metadata":{"id":"_8i_7qY7WSkA"},"source":["#the original image\n","original_url = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/segmentation/segmentation_original.jpg'\n","\n","#the truth mask - what a segmentation algorithm should learn\n","truth_url = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/segmentation/segmentation_truth.png'\n","\n","#the output mask of a (bad) algorithm\n","segm1_url = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/segmentation/segmentation_algo1.png'\n","\n","#the output mask of a (slightly better) algorithm\n","segm2_url = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/segmentation/segmentation_algo2.png'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLQ3OQVoszF4"},"source":["# Loading and preprocessing\n","\n","This section contains the code to load RGB/RGBA images and transform them to binary images."]},{"cell_type":"markdown","metadata":{"id":"J6cGXkahhTz6"},"source":["## Loading the original image\n","\n","We start by loading and displaying the original image. We'll use the [requests module](https://requests.readthedocs.io/en/master/) for remote access and the [Pillow module](https://python-pillow.org/) (a PIL fork) for image manipulation."]},{"cell_type":"code","metadata":{"id":"qV2pDcd0hcNP"},"source":["import requests          #required for remote access via urls\n","from PIL import Image    # PIL is Pillow Module, for image manipulation\n","\n","#loading the image into a PIL object\n","response = requests.get(original_url, stream=True) #you may want to check response.status_code\n","original_PIL = Image.open(response.raw)\n","\n","#printing some image metadata\n","print(original_PIL.format)\n","print(original_PIL.size)\n","print(original_PIL.mode)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VI7krdy0j6nl"},"source":["So the original image is a 1024x683 jpg image, three channels (RGB). Let's have a look:"]},{"cell_type":"code","metadata":{"id":"dJYLC1lCj4xU"},"source":["from IPython.display import display\n","display(original_PIL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FXdqNBAfkww2"},"source":["## Loading the three masks\n","\n","The next part of the assignment requires that we load and explore the mask files. Let's start loading everything. The response variable is only used once and can be recycled."]},{"cell_type":"code","metadata":{"id":"F2h719aLlQe_"},"source":["#thruth\n","response = requests.get(truth_url, stream=True)\n","truth_PIL = Image.open(response.raw)\n","\n","#mask from segmentation algorithm 1\n","response = requests.get(segm1_url, stream=True)\n","segm1_PIL = Image.open(response.raw)\n","\n","#mask from segmentation algorithm 2\n","response = requests.get(segm2_url, stream=True)\n","segm2_PIL = Image.open(response.raw)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aI6OB7RamRES"},"source":["It's handy to take a look to all images together. To do that matplotlib and numpy modules will help:"]},{"cell_type":"code","metadata":{"id":"k24nCMrSme_M"},"source":["from matplotlib.pyplot import imshow\n","from matplotlib import pyplot\n","import numpy as np\n","\n","#building a multi subplot image\n","fig=pyplot.figure(figsize=(15, 10))\n","\n","fig.add_subplot(2,2,1)\n","pyplot.imshow(np.asarray(original_PIL))\n","\n","fig.add_subplot(2,2,2)\n","pyplot.imshow(np.asarray(truth_PIL))\n","\n","fig.add_subplot(2,2,3)\n","pyplot.imshow(np.asarray(segm1_PIL))\n","\n","fig.add_subplot(2,2,4)\n","pyplot.imshow(np.asarray(segm2_PIL))\n","\n","pyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pd0CJuw4otb8"},"source":["The top figures are the original one and the ground truth mask. The bottom figures are the result of algorithm 1 (a very poor approximation of a cat, on the left) and algorithm 2 (a slightly better cat).\n","\n","Let's take a look at the metadata for the masks:"]},{"cell_type":"code","metadata":{"id":"RiKoXiFzpFO4"},"source":["#let's define a handy printing function\n","def print_image_metadata(img, img_name):\n","  print(\"Metadata for image: \" + img_name)\n","  print(img.format)\n","  print(img.size)\n","  print(img.mode + '\\n')\n","\n","#printing everythin\n","print_image_metadata(truth_PIL, 'Truth')\n","print_image_metadata(segm1_PIL, 'Algorithm 1')\n","print_image_metadata(segm2_PIL, 'Algorithm 2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dr4_UNg3ptTQ"},"source":["The good news is that all images have the same size, format (PNG) and mode (RGBA). The bad news is that the fourth channel (A = Alpha = transparency channel) is useless for this exercise and we'll need to remove it."]},{"cell_type":"markdown","metadata":{"id":"fu9eWYtPqphi"},"source":["## Remove the alpha channel from one image\n","\n","Once we have transformed the image to a 3D numpy image it will be easier to manipulate the data. In this case just removing the fourth slot from the third dimension will suffice."]},{"cell_type":"code","metadata":{"id":"Fv62glRQWSkO"},"source":["# convert image to numpy array\n","truth = np.asarray(truth_PIL)\n","\n","#numpy array metadata\n","print('Before removing alpha channel:')\n","print(type(truth))\n","print(str(truth.shape) + '\\n')\n","\n","#removing alpha channel\n","truth = np.delete(arr=truth, obj=-1, axis=2)\n","\n","#check\n","print('After removing alpha channel')\n","print(type(truth))\n","print(str(truth.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVwipeX-qBle"},"source":["## Binarize one image\n","\n","Each mask is a full color image, even if there's only two colors in it: white (background) and purple (cat). It will be handy to transform it into a simple 0/1 numeric matrix, thus losing the third dimension. Let's do that on the truth mask."]},{"cell_type":"code","metadata":{"id":"VqYPpFPrWSkR"},"source":["#taking a look, just to keep in mind our target\n","pyplot.imshow(truth)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sT8BH2YSWSkU"},"source":["#printing a white pixel (background) and a purple pixel (cat)\n","print('A background pixel:')\n","print(truth[0, 0])\n","print('\\nA cat pixel:')\n","print(truth[300, 400])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RUxmXQZTWSkW"},"source":["#each pixel is reduced to its minimum value, so that only white\n","#pixel stay at 255. Also, we are loosing the third dimension here.\n","truth = truth.min(2)\n","\n","#selecting all background pixels\n","background_selector = truth == 255\n","non_background_selector = truth != 255\n","\n","#background goes to zero, everything else to one\n","truth[background_selector] = 0\n","truth[non_background_selector] = 1\n","\n","#printing again a white pixel (background) and a purple pixel (cat)\n","print('A background pixel:')\n","print(truth[0, 0])\n","print('\\nA cat pixel:')\n","print(truth[300, 400])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6eIVi1STsWZQ"},"source":["## Preprocess all the images\n","\n","We create a function to remove the alpha channel and binarize images so we don't have to copypaste much code."]},{"cell_type":"code","metadata":{"id":"aLlRzg_xWSkY"},"source":["#put everything above in a function, so that we don't repeat for each image\n","def preprocess_img(PIL_image):\n","    #to numpy\n","    np_image = np.asarray(PIL_image)\n","    #remove alpha, if present\n","    np_image = np.delete(arr=np_image, obj=-1, axis=2)\n","    #to zero/one, losing color\n","    np_image = np_image.min(2)\n","    background_selector     = np_image == 255\n","    non_background_selector = np_image != 255\n","    np_image[background_selector] = 0\n","    np_image[non_background_selector] = 1\n","\n","    #done\n","    return(np_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"utthwV1JWSka"},"source":["#preprocess the remaining images (and redo truth, for completeness)\n","truth = preprocess_img(truth_PIL)\n","segm1 = preprocess_img(segm1_PIL)\n","segm2 = preprocess_img(segm2_PIL)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uZIp2iKUtZnB"},"source":["# Pixel Accuracy\n","\n","The first metric we consider is pixel accuracy, defined as the number of pixel correctly classified divided by the total number of pixels."]},{"cell_type":"code","metadata":{"id":"0JJwmZyaWSki"},"source":["#defining a function to ease our job\n","def accuracy(img1, img2):\n","    #total number of pixels\n","    tot_px = img1.shape[0] * img1.shape[1]\n","\n","    #number of matching pixels\n","    diff = np.count_nonzero(img1 == img2)\n","\n","    #return the ratio\n","    return(diff / tot_px)\n","\n","#comparing the algorithms to the truth mask\n","print('Accuracy')\n","print('truth vs. segm1: ' + str(accuracy(truth, segm1)))\n","print('truth vs. segm2: ' + str(accuracy(truth, segm2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MAXDZsNOuSpa"},"source":["Algorithm 2 is clearly better. However accuracy has many limitations, in particular for very unbalanced datasets (e.g. when there are much more background pixels than object ones)."]},{"cell_type":"markdown","metadata":{"id":"k9IROXy7utYq"},"source":["# Mean Intersection over Union (IoU) - Jaccard index\n","\n","This is a better metric than accuracy, and it's widely used in machine learning. For details please refer to [the wiki page](https://en.wikipedia.org/wiki/Jaccard_index).\n","\n","In the field of image segmentation it's common to compute the IoU for each object class, and then mediate the values. In our example we have only two classes: zero (background) and one (cat).\n","\n","We'll compute it explicitly and then using keras library."]},{"cell_type":"markdown","metadata":{"id":"ZxZk2fWivw3v"},"source":["## Mean IoU by hand\n","\n","Rembember the definition of IoU for a single class:\n","\n","```\n","for each class compute:\n","  IoU = |A intersect B| / |A union B|\n","\n","Then average over classes\n","```\n","\n","It's easier if we first compute the confusion matrix. Since our example is binary classification(only two classes), if we consider class 1 as \"positive\" and class 0 as \"negative\" a confusion matrix will look like the following:\n","\n","```\n","           predicted\n","            values\n","             1    0\n","          +----+----+\n","true    1 | TP | FN |\n","values    +----+----+\n","        0 | FP | TN |\n","          +----+----+\n","```\n","\n","You can then see that:\n","\n","```\n","IoU (class 1) = TP / (TP + FN + FP)\n","IoU (class 0) = TN / (TN + FN + FP)\n","```\n","\n","Let's code it. For this example we'll compute truth-segm1 IoU. For confusion matrix we'll use sklearn module."]},{"cell_type":"code","metadata":{"id":"5YwcOa0QWSkm"},"source":["#confusion matrix\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(truth.flatten(), segm1.flatten())\n","print(\"Truth-algo1 confusion matrix\")\n","print(cm)\n","\n","#using standard statistical nomenclature (true positive, false negatives...)\n","TP = cm[1,1]\n","TN = cm[0,0]\n","FP = cm[0,1]\n","FN = cm[1,0]\n","\n","#computing IoU for both classes, then pringing average\n","IoU_cl0 = TN / (TN + FP + FN)\n","IoU_cl1 = TP / (TP + FP + FN)\n","\n","print(\"\\nTruth-algo1 Mean IoU, by hand\")\n","print((IoU_cl0 + IoU_cl1) / 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8GOtiQskyovV"},"source":["## Mean IoU using keras\n","\n","Keras api contains a [MeanIoU](https://keras.io/api/metrics/segmentation_metrics/#meaniou-class) metric that greatly simplifies our work."]},{"cell_type":"code","metadata":{"id":"E7UIxS_nWSkn"},"source":["#import tensorflow as tf\n","from keras.metrics import MeanIoU\n","\n","m = MeanIoU(num_classes=2)\n","m.update_state(truth.flatten(), segm1.flatten())\n","print(\"\\nTruth-algo1 Mean IoU, using Keras\")\n","print(m.result().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOWZupG7zUzy"},"source":["Same results as the one obtained by hand. Very good!\n","\n","Let's use a function for easier comparisons."]},{"cell_type":"code","metadata":{"id":"qWY6bD-GWSkp"},"source":["def my_IoU (img1, img2):\n","    m = MeanIoU(num_classes=2)\n","    m.update_state(img1.flatten(), img2.flatten())\n","    return(m.result().numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95FEZ-VXWSkq"},"source":["print('Mean Intersection over Union:')\n","print('truth vs. segm1: ' + str(my_IoU(truth, segm1)))\n","print('truth vs. segm2: ' + str(my_IoU(truth, segm2)))\n","print('truth vs. truth: ' + str(my_IoU(truth, truth)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vX16Mh2K0CHR"},"source":["As expected the second algorithm works better than the first. Moreover we confirm that if we could get a perfect result (a perfect copy of the truth mask) the IoU would be 1."]},{"cell_type":"markdown","metadata":{"id":"bRGOh8frCcDh"},"source":["# Mean Dice coefficient\n","\n","Definition: twice intersection over sum of areas.\n","\n","```\n","for each class compute:\n","  IoU = 2 x |A intersect B| / (|A| + |B|)\n","\n","Then average over classes\n","```\n","\n","As done for IoU compute the confusion matrix:\n","\n","```\n","           predicted\n","            values\n","             1    0\n","          +----+----+\n","true    1 | TP | FN |\n","values    +----+----+\n","        0 | FP | TN |\n","          +----+----+\n","```\n","\n","It follows that:\n","\n","```\n","Dice (class 1) = 2 * TP / (TP + FN + TN + FP) = 2 * TP / (all confusion matrix)\n","Dice (class 0) = 2 * TN / (TN + FP + TP + FN) = 2 * TN / (all confusion matrix)\n","```\n","\n","Let's code it. For this example we'll use the truth-segm1 confusion matrix already computed for IoU."]},{"cell_type":"code","metadata":{"id":"BVyzl6uADzEL"},"source":["#confusion matrix, already computed for IoU\n","print(\"Truth-algo1 confusion matrix\")\n","print(cm)\n","\n","#using standard statistical nomenclature (true positive, false negatives...)\n","TP = cm[1,1]\n","TN = cm[0,0]\n","FP = cm[0,1]\n","FN = cm[1,0]\n","\n","#computing dice coefficinent for both classes, then printing the average\n","total = (TP + FN + TN + FP)\n","dice_cl0 = 2 * TN / total\n","dice_cl1 = 2 * TP / total\n","\n","print(\"\\nTruth-algo1 Dice coefficient, by hand\")\n","print((dice_cl0 + dice_cl1) / 2)"],"execution_count":null,"outputs":[]}]}