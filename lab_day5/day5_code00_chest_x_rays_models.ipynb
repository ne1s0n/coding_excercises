{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vU5b1TdjL81b"},"source":["# Conf"]},{"cell_type":"code","metadata":{"id":"kHO2k6zGL-o7","executionInfo":{"status":"ok","timestamp":1664461295732,"user_tz":-120,"elapsed":391,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"source":["#where the data are stored\n","data_url = 'http://www.jackdellequerce.com/data/reduced_chest_xray.zip'\n","\n","#where to place the data\n","download_target_imgs = '/content/data/'\n","\n","#Keras constants\n","BATCH_SIZE = 32\n","IMAGE_SIZE = [256, 256]\n","IMAGE_SHAPE = (IMAGE_SIZE[0], IMAGE_SIZE[1] , 3)\n","EPOCHS = 12\n","\n","###############################################\n","# https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb\n","# https://github.com/Bixi81/Python-ml/blob/master/keras_pretrained_imagerec_multiclass.py\n","\n","# DIR with training images\n","base_dir = '/content/data/reduced_chest_xray/'\n","# Number training images\n","ntrain = 300\n","# Number validation images\n","nval  = 100\n","# Batch size\n","batch_size = 20 #20\n","# Epochs \n","ep = 50\n","# Number of classes (for training, output layer)\n","nclasses = 2\n","###############################################"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8AQ1yCg1LeII"},"source":["# Data setup"]},{"cell_type":"code","metadata":{"id":"XQ19yZi9L2Yb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664461296548,"user_tz":-120,"elapsed":23,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"outputId":"23c7fe0a-6670-4064-bb89-a41dda3279f0"},"source":["import os\n","import time\n","import subprocess\n","from urllib.request import urlopen\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","import glob\n","import requests\n","import zipfile\n","\n","#these two lists should contain the full paths of all train and validation images\n","train_filenames = glob.glob(download_target_imgs + 'reduced_chest_xray/train/*/*')\n","val_filenames   = glob.glob(download_target_imgs + 'reduced_chest_xray/test/*/*')\n","\n","#let's check that we actually have the data\n","if len(train_filenames) == 0 or len(val_filenames) == 0:\n","  #either the data was never downloaded or something bad happened\n","  #in any case, we donwload and unzip everything\n","\n","  #room for data\n","  os.makedirs(download_target_imgs, exist_ok=True)\n","\n","  #downloading\n","  r = requests.get(data_url)\n","  open(download_target_imgs + 'local_archive.zip', 'wb').write(r.content)\n","\n","  #unpacking\n","  z = zipfile.ZipFile(download_target_imgs + 'local_archive.zip')\n","  z.extractall(path = download_target_imgs)\n","\n","  #at this point data is there, we are ready to get the list of files\n","  train_filenames = glob.glob(download_target_imgs + 'reduced_chest_xray/train/*/*')\n","  val_filenames   = glob.glob(download_target_imgs + 'reduced_chest_xray/test/*/*')\n","\n","#whatever the original case, at this point we have the files\n","print('Available images for train: ' + str(len(train_filenames)))\n","print('Available images for validation: ' + str(len(val_filenames)))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Available images for train: 300\n","Available images for validation: 100\n"]}]},{"cell_type":"markdown","metadata":{"id":"s3yjy8MMLjEU"},"source":["# CNN"]},{"cell_type":"markdown","metadata":{"id":"n3RzxKIBbq9N"},"source":["## Import"]},{"cell_type":"code","metadata":{"id":"BnegHgsyblgQ","executionInfo":{"status":"ok","timestamp":1664461296550,"user_tz":-120,"elapsed":16,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"source":["\n","import os, datetime\n","import numpy as np\n","from keras.applications.vgg16 import VGG16 \n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras import models, layers, optimizers, regularizers\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.layers.core import Dense, Dropout, Activation\n","from PIL import ImageFile\n","import statistics\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVp-9jJ2cJT4"},"source":["## Data setup"]},{"cell_type":"markdown","metadata":{"id":"fnfRDAi_ctLz"},"source":["## Data generators"]},{"cell_type":"code","metadata":{"id":"mPIDnGqCcpAy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664461296950,"user_tz":-120,"elapsed":413,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"outputId":"7d52e4b0-83b0-4d9c-dcfc-ea7a1ae63048"},"source":["#why rescale: https://github.com/Arsey/keras-transfer-learning-for-oxford102/issues/1\n","\n","start = datetime.datetime.now()\n","\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'test')\n","\n","train_datagen = ImageDataGenerator(\n","      rescale=1./255,\n","      rotation_range=10,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=False,\n","      fill_mode='nearest')\n","\n","\n","# Note that the validation data should not be augmented!\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","        # This is the target directory\n","        train_dir,\n","        # All images will be resized to 150x150\n","        target_size=(150, 150),\n","        batch_size=batch_size,\n","        # Since we use categorical_crossentropy loss, we need binary labels\n","        class_mode='categorical')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","        validation_dir,\n","        target_size=(150, 150),\n","        batch_size=batch_size,\n","        class_mode='categorical')"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 300 images belonging to 2 classes.\n","Found 100 images belonging to 2 classes.\n"]}]},{"cell_type":"markdown","metadata":{"id":"llxaVRnFcXL7"},"source":["## Model - architecture"]},{"cell_type":"code","metadata":{"id":"Mm5JzQ3Rca1T","executionInfo":{"status":"ok","timestamp":1664461296953,"user_tz":-120,"elapsed":21,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"source":["conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n","conv_base.trainable = True\n","\n","model = models.Sequential()\n","model.add(conv_base)\n","model.add(layers.Flatten())\n","model.add(layers.Dense(nclasses, activation='softmax'))"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DR3EEQkUcdun"},"source":["## Model - compile"]},{"cell_type":"code","metadata":{"id":"wW5dhe4vc8Ln","executionInfo":{"status":"ok","timestamp":1664461296955,"user_tz":-120,"elapsed":17,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}}},"source":["# Model compile / fit\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.RMSprop(learning_rate=2e-5),\n","              metrics=['acc'])\n","\n","#here we configure two callbacks, early stopping moniroting the loss, and\n","#a learning rate reduction for fine tuning\n","#more on callbacks: https://keras.io/api/callbacks/\n","\n","es = EarlyStopping(monitor='loss', mode='min', min_delta=0.001, verbose=1, patience=40, restore_best_weights=True)\n","reduce_lr = ReduceLROnPlateau(monitor='loss', mode='min', factor=0.9, patience=15, min_lr=1e-20, verbose=1, cooldown=3)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLza80Khc_ey"},"source":["## Model - fit"]},{"cell_type":"code","metadata":{"id":"zrk7tjNwdCfQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664461655944,"user_tz":-120,"elapsed":359003,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"outputId":"52a283ea-2155-4911-be96-e95fd100ced5"},"source":["history = model.fit(\n","      train_generator,\n","      steps_per_epoch=round(ntrain/batch_size,0),\n","      epochs=ep,\n","      validation_data=validation_generator,\n","      validation_steps=round(nval/batch_size,0),\n","      verbose=2,\n","      callbacks=[es, reduce_lr])"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","15/15 - 14s - loss: 0.4758 - acc: 0.7967 - val_loss: 0.4712 - val_acc: 0.8000 - lr: 2.0000e-05 - 14s/epoch - 949ms/step\n","Epoch 2/50\n","15/15 - 7s - loss: 0.2711 - acc: 0.9000 - val_loss: 0.3409 - val_acc: 0.8500 - lr: 2.0000e-05 - 7s/epoch - 450ms/step\n","Epoch 3/50\n","15/15 - 7s - loss: 0.1829 - acc: 0.9400 - val_loss: 0.4212 - val_acc: 0.8300 - lr: 2.0000e-05 - 7s/epoch - 445ms/step\n","Epoch 4/50\n","15/15 - 7s - loss: 0.2517 - acc: 0.9067 - val_loss: 0.2748 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 450ms/step\n","Epoch 5/50\n","15/15 - 7s - loss: 0.1726 - acc: 0.9367 - val_loss: 0.2673 - val_acc: 0.8800 - lr: 2.0000e-05 - 7s/epoch - 450ms/step\n","Epoch 6/50\n","15/15 - 7s - loss: 0.1257 - acc: 0.9600 - val_loss: 0.2828 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 446ms/step\n","Epoch 7/50\n","15/15 - 7s - loss: 0.1427 - acc: 0.9500 - val_loss: 0.3322 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 441ms/step\n","Epoch 8/50\n","15/15 - 7s - loss: 0.1449 - acc: 0.9467 - val_loss: 0.2894 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 441ms/step\n","Epoch 9/50\n","15/15 - 7s - loss: 0.1379 - acc: 0.9467 - val_loss: 0.3799 - val_acc: 0.8700 - lr: 2.0000e-05 - 7s/epoch - 443ms/step\n","Epoch 10/50\n","15/15 - 7s - loss: 0.0683 - acc: 0.9833 - val_loss: 0.2749 - val_acc: 0.9200 - lr: 2.0000e-05 - 7s/epoch - 443ms/step\n","Epoch 11/50\n","15/15 - 7s - loss: 0.0604 - acc: 0.9900 - val_loss: 0.3311 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 442ms/step\n","Epoch 12/50\n","15/15 - 7s - loss: 0.0841 - acc: 0.9567 - val_loss: 0.2510 - val_acc: 0.9000 - lr: 2.0000e-05 - 7s/epoch - 441ms/step\n","Epoch 13/50\n","15/15 - 7s - loss: 0.0789 - acc: 0.9733 - val_loss: 0.2556 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 446ms/step\n","Epoch 14/50\n","15/15 - 7s - loss: 0.0441 - acc: 0.9867 - val_loss: 0.3275 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 446ms/step\n","Epoch 15/50\n","15/15 - 7s - loss: 0.0685 - acc: 0.9700 - val_loss: 0.3008 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 445ms/step\n","Epoch 16/50\n","15/15 - 7s - loss: 0.0696 - acc: 0.9733 - val_loss: 0.2862 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 443ms/step\n","Epoch 17/50\n","15/15 - 7s - loss: 0.0447 - acc: 0.9767 - val_loss: 0.3884 - val_acc: 0.9000 - lr: 2.0000e-05 - 7s/epoch - 446ms/step\n","Epoch 18/50\n","15/15 - 7s - loss: 0.0842 - acc: 0.9667 - val_loss: 0.2636 - val_acc: 0.9100 - lr: 2.0000e-05 - 7s/epoch - 448ms/step\n","Epoch 19/50\n","15/15 - 7s - loss: 0.0266 - acc: 0.9867 - val_loss: 0.4388 - val_acc: 0.8900 - lr: 2.0000e-05 - 7s/epoch - 447ms/step\n","Epoch 20/50\n","15/15 - 7s - loss: 0.0367 - acc: 0.9900 - val_loss: 0.3264 - val_acc: 0.9000 - lr: 2.0000e-05 - 7s/epoch - 435ms/step\n","Epoch 21/50\n","15/15 - 7s - loss: 0.0656 - acc: 0.9767 - val_loss: 0.2668 - val_acc: 0.9400 - lr: 2.0000e-05 - 7s/epoch - 437ms/step\n","Epoch 22/50\n","15/15 - 7s - loss: 0.0500 - acc: 0.9833 - val_loss: 0.3658 - val_acc: 0.9500 - lr: 2.0000e-05 - 7s/epoch - 439ms/step\n","Epoch 23/50\n","15/15 - 7s - loss: 0.0096 - acc: 0.9967 - val_loss: 0.2967 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 439ms/step\n","Epoch 24/50\n","15/15 - 7s - loss: 0.0732 - acc: 0.9733 - val_loss: 0.2512 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 463ms/step\n","Epoch 25/50\n","15/15 - 7s - loss: 0.0271 - acc: 0.9900 - val_loss: 0.2297 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 444ms/step\n","Epoch 26/50\n","15/15 - 9s - loss: 0.0334 - acc: 0.9800 - val_loss: 0.2434 - val_acc: 0.9300 - lr: 2.0000e-05 - 9s/epoch - 595ms/step\n","Epoch 27/50\n","15/15 - 8s - loss: 0.0509 - acc: 0.9933 - val_loss: 0.2442 - val_acc: 0.9500 - lr: 2.0000e-05 - 8s/epoch - 563ms/step\n","Epoch 28/50\n","15/15 - 8s - loss: 0.0092 - acc: 1.0000 - val_loss: 0.3418 - val_acc: 0.9300 - lr: 2.0000e-05 - 8s/epoch - 539ms/step\n","Epoch 29/50\n","15/15 - 8s - loss: 0.0637 - acc: 0.9733 - val_loss: 0.2720 - val_acc: 0.9300 - lr: 2.0000e-05 - 8s/epoch - 522ms/step\n","Epoch 30/50\n","15/15 - 8s - loss: 0.0083 - acc: 1.0000 - val_loss: 0.3137 - val_acc: 0.9300 - lr: 2.0000e-05 - 8s/epoch - 543ms/step\n","Epoch 31/50\n","15/15 - 7s - loss: 0.0017 - acc: 1.0000 - val_loss: 0.3452 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 475ms/step\n","Epoch 32/50\n","15/15 - 7s - loss: 0.0756 - acc: 0.9767 - val_loss: 0.3093 - val_acc: 0.9100 - lr: 2.0000e-05 - 7s/epoch - 460ms/step\n","Epoch 33/50\n","15/15 - 7s - loss: 0.0051 - acc: 1.0000 - val_loss: 0.3067 - val_acc: 0.9500 - lr: 2.0000e-05 - 7s/epoch - 443ms/step\n","Epoch 34/50\n","15/15 - 8s - loss: 0.0497 - acc: 0.9900 - val_loss: 0.3932 - val_acc: 0.9100 - lr: 2.0000e-05 - 8s/epoch - 524ms/step\n","Epoch 35/50\n","15/15 - 7s - loss: 0.0032 - acc: 1.0000 - val_loss: 0.4131 - val_acc: 0.9000 - lr: 2.0000e-05 - 7s/epoch - 475ms/step\n","Epoch 36/50\n","15/15 - 7s - loss: 0.0124 - acc: 0.9967 - val_loss: 0.5882 - val_acc: 0.8800 - lr: 2.0000e-05 - 7s/epoch - 439ms/step\n","Epoch 37/50\n","15/15 - 7s - loss: 0.0358 - acc: 0.9800 - val_loss: 0.4028 - val_acc: 0.9000 - lr: 2.0000e-05 - 7s/epoch - 441ms/step\n","Epoch 38/50\n","15/15 - 7s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4141 - val_acc: 0.9100 - lr: 2.0000e-05 - 7s/epoch - 444ms/step\n","Epoch 39/50\n","15/15 - 7s - loss: 0.0548 - acc: 0.9867 - val_loss: 0.3064 - val_acc: 0.9600 - lr: 2.0000e-05 - 7s/epoch - 490ms/step\n","Epoch 40/50\n","15/15 - 7s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.3979 - val_acc: 0.9200 - lr: 2.0000e-05 - 7s/epoch - 443ms/step\n","Epoch 41/50\n","15/15 - 7s - loss: 0.0058 - acc: 0.9967 - val_loss: 0.5379 - val_acc: 0.8800 - lr: 2.0000e-05 - 7s/epoch - 465ms/step\n","Epoch 42/50\n","15/15 - 7s - loss: 0.0221 - acc: 0.9933 - val_loss: 0.3383 - val_acc: 0.9500 - lr: 2.0000e-05 - 7s/epoch - 438ms/step\n","Epoch 43/50\n","15/15 - 7s - loss: 0.0025 - acc: 1.0000 - val_loss: 0.5249 - val_acc: 0.9100 - lr: 2.0000e-05 - 7s/epoch - 446ms/step\n","Epoch 44/50\n","15/15 - 7s - loss: 0.0362 - acc: 0.9833 - val_loss: 0.3549 - val_acc: 0.9400 - lr: 2.0000e-05 - 7s/epoch - 448ms/step\n","Epoch 45/50\n","15/15 - 7s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3527 - val_acc: 0.9300 - lr: 2.0000e-05 - 7s/epoch - 491ms/step\n","Epoch 46/50\n","15/15 - 7s - loss: 7.8745e-04 - acc: 1.0000 - val_loss: 0.3768 - val_acc: 0.9500 - lr: 2.0000e-05 - 7s/epoch - 449ms/step\n","Epoch 47/50\n","15/15 - 7s - loss: 0.0177 - acc: 0.9967 - val_loss: 0.3587 - val_acc: 0.9200 - lr: 2.0000e-05 - 7s/epoch - 439ms/step\n","Epoch 48/50\n","15/15 - 7s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.3524 - val_acc: 0.9200 - lr: 2.0000e-05 - 7s/epoch - 442ms/step\n","Epoch 49/50\n","15/15 - 7s - loss: 0.0581 - acc: 0.9900 - val_loss: 0.5000 - val_acc: 0.9000 - lr: 2.0000e-05 - 7s/epoch - 440ms/step\n","Epoch 50/50\n","15/15 - 7s - loss: 0.0187 - acc: 0.9900 - val_loss: 0.3126 - val_acc: 0.9200 - lr: 2.0000e-05 - 7s/epoch - 438ms/step\n"]}]},{"cell_type":"markdown","metadata":{"id":"BPzmYOmANGRY"},"source":["## Closing remarks"]},{"cell_type":"code","metadata":{"id":"tUxHoegINMcG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664461696804,"user_tz":-120,"elapsed":381,"user":{"displayName":"Nelson Nazzicari","userId":"06091096357258353861"}},"outputId":"19fdd39e-db87-4b85-dfe0-7b4475f1c251"},"source":["# Save model\n","model.save(os.path.join(download_target_imgs, 'keras_multiclass_model.hdf5'))\n","end = datetime.datetime.now()\n","delta = str(end-start)\n","\n","# Metrics\n","acc = history.history['acc']\n","acc = acc[-5:]\n","val_acc = history.history['val_acc']\n","val_acc = val_acc[-5:]\n","loss = history.history['loss']\n","loss = loss[-5:]\n","val_loss = history.history['val_loss']\n","val_loss = val_loss[-5:]\n","\n","# End statement\n","print(\"============================================\")\n","print(\"Time taken (h/m/s): %s\" %delta[:7])\n","print(\"============================================\")\n","print(\"Metrics (average last five steps)\")\n","print(\"--------------------------------------------\")\n","print(\"Loss       %.3f\" %statistics.mean(loss))\n","print(\"Val. Loss  %.3f\" %statistics.mean(val_loss))\n","print(\"--------------------------------------------\")\n","print(\"Acc.       %.3f\" %statistics.mean(acc))\n","print(\"Val. Acc.  %.3f\" %statistics.mean(val_acc))\n","print(\"============================================\")\n","print(\"Epochs:    %s\" %(ep))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================\n","Time taken (h/m/s): 0:06:40\n","============================================\n","Metrics (average last five steps)\n","--------------------------------------------\n","Loss       0.019\n","Val. Loss  0.380\n","--------------------------------------------\n","Acc.       0.995\n","Val. Acc.  0.922\n","============================================\n","Epochs:    50\n"]}]}]}