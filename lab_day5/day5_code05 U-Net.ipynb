{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KrQmGgTK8rb"
      },
      "source": [
        "This notebook defines and instantiates a U-Net architecture for image segmentation, which is then trained on the [Oxford pet database](https://www.robots.ox.ac.uk/~vgg/data/pets/).\n",
        "\n",
        "Code is taken from [here](https://blog.paperspace.com/unet-architecture-image-segmentation/), [here](https://blog.paperspace.com/understanding-canet-architecture/), [here](https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96) and [here](https://keras.io/examples/vision/oxford_pets_image_segmentation/) and tweaked for better explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voDKK2ApxFap"
      },
      "source": [
        "# U-Net architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MnCCmIUxJtb"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70lmarAXdURq"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, ReLU\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2DTranspose, Concatenate\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w73xyu5CxMN0"
      },
      "source": [
        "## Convolution block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1242ivT_da4L"
      },
      "source": [
        "def convolution_operation(entered_input, filters=64):\n",
        "    # Taking first input and implementing the first conv block\n",
        "    conv1 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(entered_input)\n",
        "    batch_norm1 = BatchNormalization()(conv1)\n",
        "    act1 = ReLU()(batch_norm1)\n",
        "\n",
        "    # Taking first input and implementing the second conv block\n",
        "    conv2 = Conv2D(filters, kernel_size = (3,3), padding = \"same\")(act1)\n",
        "    batch_norm2 = BatchNormalization()(conv2)\n",
        "    act2 = ReLU()(batch_norm2)\n",
        "\n",
        "    return act2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_V66RgIxQ25"
      },
      "source": [
        "## Downsampling block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqnKwmhVddi1"
      },
      "source": [
        "def encoder(entered_input, filters=64):\n",
        "    # Collect the start and end of each sub-block for normal pass and skip connections\n",
        "    enc1 = convolution_operation(entered_input, filters)\n",
        "    MaxPool1 = MaxPooling2D(strides = (2,2))(enc1)\n",
        "    return enc1, MaxPool1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHEH_DEYxTKZ"
      },
      "source": [
        "## Upsampling block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPYpokwXdgb2"
      },
      "source": [
        "def decoder(entered_input, skip, filters=64):\n",
        "    # Upsampling and concatenating the essential features\n",
        "    Upsample = Conv2DTranspose(filters, (2, 2), strides=2, padding=\"same\")(entered_input)\n",
        "    Connect_Skip = Concatenate()([Upsample, skip])\n",
        "    out = convolution_operation(Connect_Skip, filters)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfr1xB5ixWAk"
      },
      "source": [
        "## Whole architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHrZgTHZdhZt"
      },
      "source": [
        "def U_Net(Image_Size, num_classes):\n",
        "    # Take the image size and shape\n",
        "    input1 = Input(Image_Size)\n",
        "\n",
        "    # Construct the encoder blocks\n",
        "    skip1, encoder_1 = encoder(input1, 64)\n",
        "    skip2, encoder_2 = encoder(encoder_1, 64*2)\n",
        "    skip3, encoder_3 = encoder(encoder_2, 64*4)\n",
        "    skip4, encoder_4 = encoder(encoder_3, 64*8)\n",
        "\n",
        "    # Preparing the next block\n",
        "    conv_block = convolution_operation(encoder_4, 64*16)\n",
        "\n",
        "    # Construct the decoder blocks\n",
        "    decoder_1 = decoder(conv_block, skip4, 64*8)\n",
        "    decoder_2 = decoder(decoder_1, skip3, 64*4)\n",
        "    decoder_3 = decoder(decoder_2, skip2, 64*2)\n",
        "    decoder_4 = decoder(decoder_3, skip1, 64)\n",
        "\n",
        "    out = Conv2D(num_classes, 1, padding=\"same\", activation=\"sigmoid\")(decoder_4)\n",
        "\n",
        "    model = Model(input1, out)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cmi04UcxcYY"
      },
      "source": [
        "## Taking a look at the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFEkVckJdoD_"
      },
      "source": [
        "input_shape = (160, 160, 3)\n",
        "model = U_Net(input_shape, num_classes = 20)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agarpHfZdrFb"
      },
      "source": [
        "tf.keras.utils.plot_model(model, \"model.png\", show_shapes=False, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMy2Ks6ixheK"
      },
      "source": [
        "# Pet dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQTnxUs8xn4p"
      },
      "source": [
        "## Download\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdkpvWgbe0XY"
      },
      "source": [
        "import os\n",
        "\n",
        "#data from: https://academictorrents.com/details/b18bbd9ba03d50b0f7f479acc9f4228a408cecc1\n",
        "#if the links on-the-fly (here: https://www.robots.ox.ac.uk/~vgg/data/pets/) do not work\n",
        "#we can use the previously downloaded data from jackdellequerce.com\n",
        "#if one of the file is there we assume everything was already downloaded\n",
        "if not os.path.isfile('annotations/README'):\n",
        "  #downloading the archives\n",
        "  #!curl -O https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
        "  #!curl -O https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
        "  !curl -O http://www.jackdellequerce.com/data/oxford-iiit-pet/images.tar.gz\n",
        "  !curl -O http://www.jackdellequerce.com/data/oxford-iiit-pet/annotations.tar.gz\n",
        "  #unpacking\n",
        "  !tar -xf images.tar.gz\n",
        "  !tar -xf annotations.tar.gz\n",
        "  #deleting the archives\n",
        "  !rm images.tar.gz\n",
        "  !rm annotations.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gsl4hrx4zX"
      },
      "source": [
        "## Data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKI2pABuh0ap"
      },
      "source": [
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "img_size = (160, 160)\n",
        "num_pet_classes = 3\n",
        "batch_size = 32\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(input_dir, fname)\n",
        "        for fname in os.listdir(input_dir)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "#A bit of interface: printing number of images and the first 10 file names\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ9ibUMxyCZF"
      },
      "source": [
        "## Taking a look at one of the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wheUXWWph0yh"
      },
      "source": [
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Display input image #7\n",
        "display(Image(filename=input_img_paths[9]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = PIL.ImageOps.autocontrast(load_img(target_img_paths[9]))\n",
        "display(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsEgam5EyI_P"
      },
      "source": [
        "## Sequence helper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yQC-pn8h6mU"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            y[j] = np.expand_dims(img, 2)\n",
        "            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n",
        "            y[j] -= 1\n",
        "        return x, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzBwozpNyScO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b39MwiODyT2n"
      },
      "source": [
        "## Train/validatin split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wkY6du_iKD1"
      },
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 1000\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_img_paths)\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = OxfordPets(\n",
        "    batch_size, img_size, train_input_img_paths, train_target_img_paths\n",
        ")\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO7VhNMGJ_5Z"
      },
      "source": [
        "## Installing segmentation_models module (via pip)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqj44K3tJ8qg"
      },
      "source": [
        "#This module is required for IoU and Dice scores\n",
        "!pip install segmentation_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFfs6prpe6I5"
      },
      "source": [
        "## Defining the Dice Loss (Jaccard index)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBMw3GYJZxm4"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
        "    \"\"\"\n",
        "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
        "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
        "\n",
        "    The jaccard distance loss is usefull for unbalanced datasets. This has been\n",
        "    shifted so it converges on 0 and is smoothed to avoid exploding or disapearing\n",
        "    gradient.\n",
        "\n",
        "    Ref: https://en.wikipedia.org/wiki/Jaccard_index\n",
        "\n",
        "    @url: https://gist.github.com/wassname/f1452b748efcbeb4cb9b1d059dce6f96\n",
        "    @author: wassname\n",
        "    \"\"\"\n",
        "    #added these two casts to force same type\n",
        "    y_true = K.cast(y_true, 'float32')\n",
        "    y_pred = K.cast(y_pred, 'float32')\n",
        "\n",
        "    #this below is wassname's original code\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "\n",
        "    return (1 - jac) * smooth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rItDP0wypH7"
      },
      "source": [
        "## Model instantiation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env SM_FRAMEWORK=tf.keras\n",
        "import tensorflow as tf\n",
        "import segmentation_models as sm\n",
        "from tensorflow.keras import models\n",
        "from segmentation_models.metrics import iou_score"
      ],
      "metadata": {
        "id": "qP8ZWoHiJKKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qJt53JsH339"
      },
      "source": [
        "num_pet_classes = 3\n",
        "input_shape = (160, 160, 3)\n",
        "model = U_Net(input_shape, num_classes = num_pet_classes)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=jaccard_distance_loss, metrics=[iou_score])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBVgjSJOH6eI"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YXNw-jhiPay"
      },
      "source": [
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 3\n",
        "\n",
        "history = model.fit(train_gen, epochs=epochs, validation_data=val_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnkJC7ZtKYCp"
      },
      "source": [
        "## Plot loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBgAgVxlSr-C"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#a handy function to plot loss and metrics\n",
        "def plot_loss_history(h, title):\n",
        "  for metric in h.history.keys():\n",
        "    #ignoring metrics on validation set, which are implied when\n",
        "    #plotting on training set\n",
        "    if metric.startswith('val_'):\n",
        "      continue\n",
        "\n",
        "    #if we get here we found a metric on the training set,\n",
        "    #let's plot it\n",
        "    plt.plot(h.history[metric], label = \"Train set\")\n",
        "    plt.plot(h.history[\"val_\" + metric], label = \"Validation set\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title + ' - ' + metric)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "#let's use the function we just defined\n",
        "plot_loss_history(history, title = 'U-Net pet segmentation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZQM8Tahy_ne"
      },
      "source": [
        "## Predict the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfVegkCci76c"
      },
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
        "val_preds = model.predict(val_gen)\n",
        "\n",
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "\n",
        "\n",
        "# Display results for validation image #10\n",
        "i = 10\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=val_input_img_paths[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "display(img)\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i)  # Note that the model only sees inputs at 150x150."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlnNVPgZ7u-n"
      },
      "source": [
        "# Alternative loss function\n",
        "\n",
        "Keras offers a different loss function which could be used for segmentation: [sparse_categorical_crossentropy()](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class)\n",
        "\n",
        "It's:\n",
        "\n",
        "* categorical: that's expected, since segmentation is intrinsically a categorical problem\n",
        "* crossentropy: we already met this\n",
        "* sparse: useful for very unbalanced classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRmHp-cS6Uh7"
      },
      "source": [
        "model2 = U_Net(input_shape, num_classes = num_pet_classes)\n",
        "model2.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[iou_score])\n",
        "history2 = model2.fit(train_gen, epochs=epochs, validation_data=val_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPlE5ZKF7ABS"
      },
      "source": [
        "#let's use the function we just defined\n",
        "plot_loss_history(history2, title = 'U-Net pet segmentation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Z-XTXg6q5P"
      },
      "source": [
        "val_preds = model2.predict(val_gen)\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=val_input_img_paths[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "display(img)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}