{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro_to_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcJRPG-aWj3b"
      },
      "source": [
        "## Recurrent Neural Networks (RNN) with Keras\n",
        "\n",
        "- Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language: e.g. speech recognition, speech synthesis, text generation\n",
        "- Schematically, a RNN layer uses a `for loop` to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far\n",
        "\n",
        "### with Keras\n",
        "\n",
        "There are three built-in RNN layers in Keras: \n",
        "1. [keras.layers.RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN): `keras.layers.SimpleRNN`\n",
        "2. [keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Long Short-Term Memory layer (Hochreiter, 1997)\n",
        "3. [keras.layers.GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU): Gated Recurrent Unit (Cho et al, 2014)\n",
        "\n",
        "RNN suffer from short-term memory problems: if a time series is long, RNN have difficulties in carrying information from earlier timepoints over to later timepoints. Specifically, in back propagation RNN experience the **vanishing gradient problem**, i.e. the gradient (values used to update NN weights) shrinks over successive backpropagation steps, and if it becomes too small it won't contribute to learning:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\alpha \\cdot \\frac{\\partial}{\\partial w_t}J(w) = 2.1 - 0.1 \\cdot 0.001 = 2.0999\n",
        "$$\n",
        "\n",
        "(not much of a difference!)\n",
        "\n",
        "Therefore, RNN can forget what they saw early in the sequence $\\rightarrow$ **short-term memory!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhwJoRtPTc3m"
      },
      "source": [
        "#### Gates in RNN\n",
        "\n",
        "To address issues with short-term memory, RNN use internal structures (sublayers) called **gates**. Gates can learn which data in a sequence are important to keep or throw away and pass them on along the chain of sequences.\n",
        "It is s bit like remembering only those words in an ad that struck your memory (e.g. the price, the deadline, the main characteristics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXeI6VxNWQPw"
      },
      "source": [
        "#### Inside a RNN\n",
        "\n",
        "- words (or sounds) transformed to vectors\n",
        "- the RNN processes each sequence of vectors one by one, passing hidden states (units) sequentially to the next steps: in this way, the RNN holds information seen in the each previous step\n",
        "- the input vector (word) and previous hidden state are combined to form a new vector that has information on the current and previous inputs\n",
        "- the combined vector goes through the activation function (e.g. `tanh`) and the output is the new hidden state to be combined with the input to be processed by he next unit in the layer\n",
        "- `tanh` squeashes values from the linear combination of the combined vector values in input (input + hidden state) to the range $[-1,1]$   \n",
        "\n",
        "Simple RNN (no *gates*, or better only one gate) use much less computational resources than the evolved variants: **LSTM** and **GRU**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRytvTALjVBr"
      },
      "source": [
        "#### LSTM\n",
        "\n",
        "Unlike simple RNN, the operations inside a LSTM cell (sublayer/unit) allow to keep or forget pieces of information. In this way, also information from earlier time steps can make its way to later time steps, reducing the effects of short-term memory. In this journey, information can be added or removed through **gates** (typically **4** in LST layers).\n",
        "LSTM gates (sublayers) use the **sigmoid** activation function, in $[0,1]$, which permits to 'forget' information by returning 0, or to keep it by returning 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F_C2ZjiXxli"
      },
      "source": [
        "## Setting up a RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uE-pZgiWYzm"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnxRbaYX-Zc"
      },
      "source": [
        "Simple example of a `Sequential` model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a `LSTM` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJHoW8RWfOFq"
      },
      "source": [
        "## parameters\n",
        "embed_input_size = 1000\n",
        "embed_output_size = 64\n",
        "lstm_units = 128\n",
        "dense_units = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWBPNPwpepzU",
        "outputId": "5d0d8654-50a2-4908-b91f-cc3c95813c95"
      },
      "source": [
        "model = keras.Sequential() ## not unlike usual NN models (including CNN)\n",
        "\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=embed_input_size, output_dim=embed_output_size)) ## this is unlike dense NN models (model.add(Dense()))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(lstm_units))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(dense_units))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBgXUqtrgnAa"
      },
      "source": [
        "Let's calculate the n. of parameters in LSTM layers:\n",
        "- insight about how LSTM handles time dependent or sequence input data\n",
        "- model capacity and complexity:\n",
        "    - handle overfitting or underfitting\n",
        "    - adjust the number of parameters of each layer\n",
        "\n",
        "`LSTM` expects input data to be a `3D tensor` such that:\n",
        "\n",
        "`[batch_size, timesteps, feature]`\n",
        "\n",
        "1. `batch_size`: how many samples in each batch during training and testing\n",
        "2. `timesteps`: how many values in a sequence, e.g. in `[4, 7, 8, 4]` there are 4 timesteps\n",
        "3. `features`: how many dimensions to represent data in one time step; e.g. if each value in the sequence is one hot encoded with 9 zero and 1 one then feature is 10\n",
        "\n",
        "`LSTM` layers have **4 dense layers** in its internal structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtqF8cbwtw__"
      },
      "source": [
        "#### Illustration of a LTM layer\n",
        "\n",
        "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPdXJlyhwoBZ"
      },
      "source": [
        "- 3 inputs\n",
        "- 4 dense layers within the LSTM layer\n",
        "- 2 LSTM units (hidden/cell state)\n",
        "\n",
        "The LSTM units (e.g. 128 from the example above) are added to the input units (64, from the example above) and multiplied by the number of LSTM units; the corresponding bias terms are then added (128, one per LSTM unit):\n",
        "\n",
        "$$\n",
        "(64+128) \\cdot 128+128 = 24\\,704\n",
        "$$\n",
        "\n",
        "This is multiplied by the number of internal dense layers (4):\n",
        "\n",
        "$$\n",
        "24\\,704 \\cdot 4 = 98\\,816\n",
        "$$\n",
        "\n",
        "This is how the number of parameters in a LSTM layer is calculated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjrMGCHtKRJJ"
      },
      "source": [
        "### GRU layers\n",
        "\n",
        "- "
      ]
    }
  ]
}