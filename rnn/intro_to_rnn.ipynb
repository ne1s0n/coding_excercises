{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "intro_to_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcJRPG-aWj3b"
      },
      "source": [
        "## Recurrent Neural Networks (RNN) with Keras\n",
        "\n",
        "- Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language: e.g. speech recognition, speech synthesis, text generation\n",
        "- Schematically, a RNN layer uses a `for loop` to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far\n",
        "\n",
        "### with Keras\n",
        "\n",
        "There are three built-in RNN layers in Keras: \n",
        "1. [keras.layers.RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN): `keras.layers.SimpleRNN`\n",
        "2. [keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Long Short-Term Memory layer (Hochreiter, 1997)\n",
        "3. [keras.layers.GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU): Gated Recurrent Unit (Cho et al, 2014)\n",
        "\n",
        "RNN suffer from short-term memory problems: if a time series is long, RNN have difficulties in carrying information from earlier timepoints over to later timepoints. Specifically, in back propagation RNN experience the **vanishing gradient problem**, i.e. the gradient (values used to update NN weights) shrinks over successive backpropagation steps, and if it becomes too small it won't contribute to learning:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\alpha \\cdot \\frac{\\partial}{\\partial w_t}J(w) = 2.1 - 0.1 \\cdot 0.001 = 2.0999\n",
        "$$\n",
        "\n",
        "(not much of a difference!)\n",
        "\n",
        "Therefore, RNN can forget what they saw early in the sequence $\\rightarrow$ **short-term memory!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhwJoRtPTc3m"
      },
      "source": [
        "#### Gates in RNN\n",
        "\n",
        "To address issues with short-term memory, RNN use internal structures (sublayers) called **gates**. Gates can learn which data in a sequence are important to keep or throw away and pass them on along the chain of sequences.\n",
        "It is s bit like remembering only those words in an ad that struck your memory (e.g. the price, the deadline, the main characteristics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXeI6VxNWQPw"
      },
      "source": [
        "#### Inside a RNN\n",
        "\n",
        "- words (or sounds) transformed to vectors\n",
        "- the RNN processes each sequence of vectors one by one, passing hidden states (units) sequentially to the next steps: in this way, the RNN holds information seen in the each previous step\n",
        "- the input vector (word) and previous hidden state are combined to form a new vector that has information on the current and previous inputs\n",
        "- the combined vector goes through the activation function (e.g. `tanh`) and the output is the new hidden state to be combined with the input to be processed by he next unit in the layer\n",
        "- `tanh` squeashes values from the linear combination of the combined vector values in input (input + hidden state) to the range $[-1,1]$   \n",
        "\n",
        "Simple RNN (no *gates*, or better only one gate) use much less computational resources than the evolved variants: **LSTM** and **GRU**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRytvTALjVBr"
      },
      "source": [
        "#### LSTM\n",
        "\n",
        "Unlike simple RNN, the operations inside a LSTM cell (sublayer/unit) allow to keep or forget pieces of information. In this way, also information from earlier time steps can make its way to later time steps, reducing the effects of short-term memory. In this journey, information can be added or removed through **gates** (typically **4** in LST layers).\n",
        "LSTM gates (sublayers) use the **sigmoid** activation function, in $[0,1]$, which permits to 'forget' information by returning 0, or to keep it by returning 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbjClprWwNZ7"
      },
      "source": [
        "- **forget gate**: previous hidden state + input vector $\\rightarrow$ `sigmoid` activation: `if sigmoid(h+x) ~ 0 then forget; else if sigmoid(h+x) ~ 1 then keep`\n",
        "- **input gate(s)**: this typically has two units, one with `sigmoid` and one with `tanh` activation functions; this is where the updating of model weights happens. Each unit receives the previous hidden state + the input vector: \n",
        "  1. one unit decides on importance (which values to update)\n",
        "  2. the other unit updates the weights\n",
        "- **output gate**: this unit combines results from the forget and input gates to produce the output that is passed on to the next times step (next sublayer/unit in the LSTM layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHoc18fOymku"
      },
      "source": [
        "### GRU\n",
        "\n",
        "GRU (gated recurrent unit), compared to LSTM gets rid of `cell state` and use only the `hidden state` to transfer information (LSTM combines cell states and hidden states to pass on information through layers):\n",
        "\n",
        "- **reset gate**: controls how much past information to forget\n",
        "- **update gate**: controls what information to throw away and what new information to add\n",
        "- **output gate**: passes the new hidden state on to the next time step (next unit in the GRU layer)\n",
        "\n",
        "GRU layers are similar to LSTM layers, but a little faster (fewer tensor operations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F_C2ZjiXxli"
      },
      "source": [
        "## Setting up a RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uE-pZgiWYzm"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnxRbaYX-Zc"
      },
      "source": [
        "Simple example of a `Sequential` model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a `LSTM` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJHoW8RWfOFq"
      },
      "source": [
        "## parameters\n",
        "embed_input_size = 1000\n",
        "embed_output_size = 64\n",
        "lstm_units = 128\n",
        "dense_units = 10"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWBPNPwpepzU",
        "outputId": "002f57c1-8c8b-48a1-893f-d699f762d732"
      },
      "source": [
        "model = keras.Sequential() ## not unlike usual NN models (including CNN)\n",
        "\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 64.\n",
        "model.add(layers.Embedding(input_dim=embed_input_size, output_dim=embed_output_size)) ## this is unlike dense NN models (model.add(Dense()))\n",
        "\n",
        "# Add a LSTM layer with 128 internal units.\n",
        "model.add(layers.LSTM(lstm_units))\n",
        "\n",
        "# Add a Dense layer with 10 units.\n",
        "model.add(layers.Dense(dense_units))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 164,106\n",
            "Trainable params: 164,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBgXUqtrgnAa"
      },
      "source": [
        "Let's calculate the n. of parameters in LSTM layers:\n",
        "- insight about how LSTM handles time dependent or sequence input data\n",
        "- model capacity and complexity:\n",
        "    - handle overfitting or underfitting\n",
        "    - adjust the number of parameters of each layer\n",
        "\n",
        "`LSTM` expects input data to be a `3D tensor` such that:\n",
        "\n",
        "`[batch_size, timesteps, feature]`\n",
        "\n",
        "1. `batch_size`: how many samples in each batch during training and testing\n",
        "2. `timesteps`: how many values in a sequence, e.g. in `[4, 7, 8, 4]` there are 4 timesteps\n",
        "3. `features`: how many dimensions to represent data in one time step; e.g. if each value in the sequence is one hot encoded with 9 zero and 1 one then feature is 10\n",
        "\n",
        "`LSTM` layers have **4 dense layers** in its internal structure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtqF8cbwtw__"
      },
      "source": [
        "#### Illustration of a LTM layer\n",
        "\n",
        "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPdXJlyhwoBZ"
      },
      "source": [
        "- 3 inputs\n",
        "- 4 dense layers within the LSTM layer\n",
        "- 2 LSTM units (hidden/cell state)\n",
        "\n",
        "The LSTM units (e.g. 128 from the example above) are added to the input units (64, from the example above) and multiplied by the number of LSTM units; the corresponding bias terms are then added (128, one per LSTM unit):\n",
        "\n",
        "$$\n",
        "(64+128) \\cdot 128+128 = 24\\,704\n",
        "$$\n",
        "\n",
        "This is multiplied by the number of internal dense layers (4):\n",
        "\n",
        "$$\n",
        "24\\,704 \\cdot 4 = 98\\,816\n",
        "$$\n",
        "\n",
        "This is how the number of parameters in a LSTM layer is calculated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjrMGCHtKRJJ"
      },
      "source": [
        "### GRU layers\n",
        "\n",
        "1. first, you sum the size of the input vector (original data) and the hidden state (n. of units in the GRU layer): `input_vector` + `hidden_state`: $(h+i)$\n",
        "2. then you multiply the total imput size by the number of unit in the GRU layer: $h \\cdot (h+i)$\n",
        "3. then you add the bias terms, one per unit: $h \\cdot (h+i) + h$\n",
        "4. then you multiply by the number of gates (typically three in GRU layers): $g \\cdot [h \\cdot (h+i)+h]$\n",
        "5. finally, you add additional bias terms, as many as the product of gates and units: $g \\cdot [h \\cdot (h+i)+h]+g \\cdot h$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujq_gVQs02Ss",
        "outputId": "f81c1b88-84d4-46db-c49a-696d3c94e4bd"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)\n",
        "model.add(layers.GRU(256, return_sequences=True))\n",
        "\n",
        "# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)\n",
        "model.add(layers.SimpleRNN(128))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 64)          64000     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, None, 256)         247296    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 128)               49280     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 361,866\n",
            "Trainable params: 361,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7zGJ9fN5Uh7"
      },
      "source": [
        "$(64+256) \\cdot 256 = 81920$\n",
        "\n",
        "$81920 + 256 = 82176$\n",
        "\n",
        "$3 \\cdot 82176 = 246528$\n",
        "\n",
        "$246528 + 3 \\cdot 256 = 247296$"
      ]
    }
  ]
}