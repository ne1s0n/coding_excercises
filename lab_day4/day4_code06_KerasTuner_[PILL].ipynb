{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KerasTuner\n",
        "\n",
        "KerasTuner is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search. KerasTuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms.\n",
        "In this notebook we show how to use the **KerasTuner** for automatic network optimization (and, in general, hyperparameter tuning). This example uses the breast cancer dataset which we have already seen in the course and is completely self contained. However if you want to further understand what's going on please refer to:\n",
        "\n",
        "* [Official website](https://keras.io/keras_tuner/)\n",
        "* [Tutorial at Tensorflow](https://www.tensorflow.org/tutorials/keras/keras_tuner)\n",
        "* [Tutorial at medium.com](https://haneulkim.medium.com/hyperparameter-tuning-with-keras-tuner-full-tutorial-f8128397e857)"
      ],
      "metadata": {
        "id": "UCAXsNUl8vi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup(s)"
      ],
      "metadata": {
        "id": "gIsUPNka9iyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard libraries setup"
      ],
      "metadata": {
        "id": "rp0Rk8YU96Qq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKEvHzQW8oF1"
      },
      "outputs": [],
      "source": [
        "#very common libraries, that we for sure are using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KerasTuner setup"
      ],
      "metadata": {
        "id": "2VF13QL2-a0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making sure KerasTuner is installed.\n",
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "wYA-0aeu-cbx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "dn37MY_MA0cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seed setup"
      ],
      "metadata": {
        "id": "OInn7vWK98f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#resetting the seeds\n",
        "!wget -O support_code.py https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/lab_day1/support_code.py\n",
        "%run support_code.py\n",
        "n1 = 0\n",
        "reset_random_seeds(n1)"
      ],
      "metadata": {
        "id": "bxUUBoWD9_i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data setup"
      ],
      "metadata": {
        "id": "Gl9Z_VYG-Bsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#libraries for this block\n",
        "import sklearn.datasets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# loading data\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "bcancer = load_breast_cancer()\n",
        "y = bcancer.target\n",
        "X = pd.DataFrame(bcancer.data, columns=bcancer.feature_names)\n",
        "\n",
        "# normalizing\n",
        "X = (X - X.mean())/X.std()"
      ],
      "metadata": {
        "id": "MB-txeIB-DtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#declaring a sss object\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
        "\n",
        "#sss.split() returns two iterables over the two pieces of data\n",
        "for train_index, val_index in sss.split(X=X, y=y):\n",
        "  x_train = X.iloc[train_index, :]\n",
        "  x_val   = X.iloc[val_index, :]\n",
        "\n",
        "  y_train = y[train_index]\n",
        "  y_val   = y[val_index]"
      ],
      "metadata": {
        "id": "eWBINuoXEaAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KerasTuner workflow"
      ],
      "metadata": {
        "id": "5r2sb5SC-ETn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the (hyper)model\n",
        "\n",
        "When you build a model for the tuning of the hyperparameters, you also define the **hyperparameter search space** in addition to the model architecture.\n",
        "The model you set up for fine-tuning is called a **hypermodel**.\n",
        "\n",
        "With **KerasTuner**, you can define a hypermodel through two approaches:\n",
        "\n",
        "- by using a **model builder function**\n",
        "- by subclassing the **HyperModel class** of the KerasTuner API\n",
        "\n",
        "Here we use a model builder function to define the classification model.\n",
        "The model builder function returns a compiled model and uses the hyperparameters you define to finetune the model:\n",
        "\n",
        "- n. of units in the first dense layer\n",
        "- learning rate"
      ],
      "metadata": {
        "id": "YsItSK8NCr25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (X.shape[1],)\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "P4DYDONySx0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "\n",
        "  input_shape = (30,)\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Input(input_shape))\n",
        "  #model.add(keras.layers.Flatten(input_shape=(30,))) ## n. of features in the BreastCancer dataset\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 16-128\n",
        "  hp_units = hp.Int('units', min_value=16, max_value=128, step=16)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "VZ4YSk0RCexA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparamaters to be explored\n",
        "\n",
        "The Keras Tuner has four tuners available: i) RandomSearch, ii) Hyperband, iii) BayesianOptimization, iv) Sklearn.\n",
        "In this tutorial, you use the `Hyperband tuner`.\n",
        "\n",
        "To instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (`max_epochs`)."
      ],
      "metadata": {
        "id": "NSpo_yEOC7Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3, ## factor: Integer, the reduction factor for the number of epochs and number of models for each bracket. Defaults to 3.\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')"
      ],
      "metadata": {
        "id": "JDpkmFwuDKUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a callback to stop training early after reaching a certain value for the validation loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "gACF8BFaIRjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "1NgARx_yDXvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(x_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "0x-tyNQyDeM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "SNK7hOf7FhNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(x_train, y_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "id": "9zYigxGmFi8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(x_train, y_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "metadata": {
        "id": "BDJ7clSfFqmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = hypermodel.evaluate(x_val, y_val)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "id": "CRj0LdcUFwFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further steps\n",
        "\n",
        "The code above is a **very** minimal example and works as a starting point. Stuff to consider:\n",
        "\n",
        "* each combination of hyperparameter is trained once, with a 70/30 default split. Using `.evaluate_models()` it's possible to do a proper k-fold crossvalidation (see [scan documentation](https://autonomio.github.io/talos/#/Scan), search \"evaluate_models\")\n",
        "* the default approach of trying all the combinations can become unfeasible very quickly. The `Scan` function supports several policies for sampling a subset of the hyperparameter space. See the [Towardsdatascience's tutorial](https://towardsdatascience.com/tune-the-hyperparameters-of-your-deep-learning-networks-in-python-using-keras-and-talos-2a2a38c5ac31) for a more in-depth example"
      ],
      "metadata": {
        "id": "miy4ArJAAF7d"
      }
    }
  ]
}