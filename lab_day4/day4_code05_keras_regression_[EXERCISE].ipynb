{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0nH5XpufRUJ"
      },
      "source": [
        "# A neural network model for regression problems\n",
        "\n",
        "We will see here how to use Keras to implement a simple neural network model for a **regression problem** (continuous target variable).\n",
        "For this practical session we are using the [California house princing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), exploring the relationship between house prices (in thousands of dollars) and a series of thirteen numerical properties of houses in Boston suburbs.\n",
        "\n",
        "Fortunately for us, is a very well known dataset, handily included in the [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwtDeoiyuo-X"
      },
      "source": [
        "# The usual seed priming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HI3hUEIundz"
      },
      "source": [
        "#general random seed\n",
        "from numpy.random import seed\n",
        "seed(10)\n",
        "\n",
        "#tensorflow-specific seed. If you want to know why\n",
        "#we need to set two separate random seeds see\n",
        "#https://blog.cmgresearch.com/2020/09/04/tensorflow-has-to-random-seeds.html\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1iAUVwiDd_Z"
      },
      "source": [
        "# Loading libraries and getting the data\n",
        "\n",
        "We first load some necessary libraries and get the data from the `sklearn` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2HnsV5C2iXS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#(features, target_variable) = datasets.fetch_california_housing(return_X_y=True)\n",
        "cal = datasets.fetch_california_housing(return_X_y=False)\n",
        "\n",
        "#for data description and manipulation a pandas dataframe is handier\n",
        "#than a numpy matrix. Moreover, you can always go back using .to_numpy()\n",
        "#method\n",
        "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html\n",
        "#features = pd.DataFrame(features)\n",
        "features = pd.DataFrame(cal.data, columns=cal.feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7NeZtClA40o"
      },
      "source": [
        "# Data description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOeaRofAA_yr"
      },
      "source": [
        "#taking a look at the features\n",
        "features.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sIQEGu0Ak2E"
      },
      "source": [
        "#taking a look at the target variable\n",
        "target_variable = cal.target\n",
        "plt.hist(target_variable, bins = 15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKnMwA7ATyFv"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlxbDIHCr6oJ"
      },
      "source": [
        "## Missing values removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju6OFDkYpaNu"
      },
      "source": [
        "It's always useful to check if something is missing, since usually regression models don't like missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZSvuQnWr5K5"
      },
      "source": [
        "features.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLg360RiM5nj"
      },
      "source": [
        "No values are missing, but just in case the code belows takes care of holes in the features data frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0TJQDZPC--D"
      },
      "source": [
        "#fast way, but only for one data frame...\n",
        "#features = features.dropna()\n",
        "\n",
        "#finding features with at least one missing data point\n",
        "missing = features.isnull().sum(axis = 1) > 0\n",
        "\n",
        "#you cannot simply use the \"not\" operator on a series\n",
        "#it's ambigous. We need to pass through a proper\n",
        "#negation function from numpy\n",
        "not_missing = np.logical_not(missing)\n",
        "\n",
        "#subsetting\n",
        "features = features.loc[not_missing, :]\n",
        "target_variable = target_variable[not_missing]\n",
        "\n",
        "#just checking\n",
        "print(\"Size of target variable: \", target_variable.shape)\n",
        "print(\"Size of feature matrix: \", features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8UGOPjRNPuS"
      },
      "source": [
        "## Discussion: what if we are missing target values?\n",
        "\n",
        "The code above shows us how to remove samples with missing values in the `features` data frame. What if we have missing values in the target array?\n",
        "\n",
        "Pay attention: `target_variable` is a NumPy array. What kind of data should we expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR0PzRTZC1x0"
      },
      "source": [
        "## Discussion: outliers removal?\n",
        "\n",
        "Should we remove outliers data? If yes, do it in the space below. If no, tell us why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-XcGq-oge2s"
      },
      "source": [
        "######## YOUR CODE HERE ########\n",
        "if True:\n",
        "  #some very refined logic...\n",
        "  outlier = target_variable > 4\n",
        "\n",
        "  #subsetting\n",
        "  features = features.loc[np.logical_not(outlier), :]\n",
        "  target_variable = target_variable[np.logical_not(outlier)]\n",
        "\n",
        "  #just checking\n",
        "  print(\"Size of target variable: \", target_variable.shape)\n",
        "  print(\"Size of feature matrix: \", features.shape)\n",
        "################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r99y-Y0zk6KK"
      },
      "source": [
        "## Discussion/Exercise: data normalization?\n",
        "\n",
        "Should we normalize the features? If yes, do it in the space below. If no, tell us why."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwoVhYDvlXmz"
      },
      "source": [
        "######## YOUR CODE HERE ########\n",
        "if True:\n",
        "  #getting mean and standard deviation for train set\n",
        "  avg = features.mean()\n",
        "  std = features.std()\n",
        "\n",
        "  #normalizing\n",
        "  features = (features - avg)/std\n",
        "\n",
        "  #checking if we are doing everything fine\n",
        "  print(pd.DataFrame(features).describe())\n",
        "################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_a2pcQcJ-o5"
      },
      "source": [
        "# Training and validation sets\n",
        "\n",
        "We have a resonable-sized dataset, we can go with one of the standard split-validation cuts, e.g. 10%\n",
        "\n",
        "> **[PRO TIP]** In very small dataset you may be forced to do \"leave one out\" crossvalidation. For N samples you cycle selecting one sample for predictions (this would be your validation set) and keeping N-1 for training. Unfortunately in this case you cannot compute correlations (can you guess why?) and must use error-related metrics like Mean Squared Error.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgI8cFrlhwNK"
      },
      "source": [
        "#we import a function to perform the split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_val, target_train, target_val = train_test_split(features, target_variable, test_size=0.10)\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o93lPZozLjrz"
      },
      "source": [
        "# Building the neural networks model\n",
        "\n",
        "We are now ready to build our neural networks model for regression. First, we set some hyperparameters:\n",
        "\n",
        "- the activation function in the output layer in this case is **linear**: we get directly the results from $\\sum wx + b$\n",
        "- the loss function is MSE (**Mean Squared Error**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHjBOQDRLJD1"
      },
      "source": [
        "# Configuration options\n",
        "\n",
        "#Keras wants the data shape as a tuple. In case of images is something\n",
        "#like (image width, image height, number of channels).\n",
        "#In our case data is tabular so each data point is an\n",
        "#array (a row in the table) and we just need to specify\n",
        "#a single number, i.e. the number of columns. However, we still need\n",
        "#to input a tuple, so...\n",
        "input_shape = (features.shape[1],)\n",
        "\n",
        "#other standard features describing our network\n",
        "hidden_nodes = 16\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'linear'\n",
        "loss_function = 'mean_squared_error'\n",
        "optimizer_used = 'rmsprop'\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMyVxYiDMINV"
      },
      "source": [
        "print(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Hiz909JIRn"
      },
      "source": [
        "As we learnt to do throughout the course, we now use `keras` to build our sequential neural networks model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI2dJT2AMPSq"
      },
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT.\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes.\n",
        "from keras.layers import Dense\n",
        "\n",
        "#adding the hidden layer with the required number of nodes, plus the final\n",
        "#layer with a single node (since we want to output a single number)\n",
        "model = Sequential()\n",
        "model.add(Dense(units=hidden_nodes, input_shape=input_shape, activation=hidden_activation))\n",
        "model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJ5agM7JMVsE"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FwozHIJakU"
      },
      "source": [
        "The `model.summary()` tells us that we now have 161 model parameters to learn:\n",
        "\n",
        "- hidden layer: 8 feature weights x 16 units + 16 bias terms $\\rightarrow$ 144 parameters\n",
        "- output layer: 16 unit weights + 1 bias term $\\rightarrow$ 17 parameters  \n",
        "- 144 + 17 = 161 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhXMeMyyMbAd"
      },
      "source": [
        "# Training the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rFoY_ehMf7g"
      },
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(\n",
        "    features_train, target_train,\n",
        "    epochs=num_epochs, validation_data=(features_val, target_val), verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwVjN65mMo6f"
      },
      "source": [
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9cZN6G9MvTc"
      },
      "source": [
        "# Predictions\n",
        "\n",
        "We can now see the predictions our neural networks model produced for the house prices. Below we report also the known target values from the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YrKJ0sYMxFK"
      },
      "source": [
        "#predicting, and reporting the shape of the obtained object (also known as y hat)\n",
        "predictions = model.predict(features_val)\n",
        "print(predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-l-44KDm_Pq"
      },
      "source": [
        "#a couple of print on true target values (also known as y)\n",
        "print(target_val)\n",
        "print(target_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jb_stcJLaR7"
      },
      "source": [
        "In regression problems you can not use the same accuracy metrics as in classification problems (e.g. error rate, confusion matrix, etc.): in stead, other metrics are used like:\n",
        "\n",
        "- **Pearson linear correlation**\n",
        "- **Spearman rank correlation**\n",
        "- **RMSE** (root mean squared error)\n",
        "- **MAE** (mean absolute error)\n",
        "- etc. (there are many more)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhs_MUYANn_r"
      },
      "source": [
        "#changing forma to pandas Series, for handiness\n",
        "y_pred = pd.Series(predictions[:,0])\n",
        "y = pd.Series(target_val)\n",
        "\n",
        "#computing correlation\n",
        "accuracy = y.corr(y_pred, method='pearson')\n",
        "print(\"accuracy (measured as Pearson's correlation) is: \", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgMMwGmYC31l"
      },
      "source": [
        "\n",
        "A scatter plot of predicted vs true values can also be useful to visualise the result of the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UDWONAIO2Ly"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "\n",
        "plt.scatter(y, y_pred)\n",
        "#equivalent: plt.plot(y, y_pred, \"o\")\n",
        "\n",
        "# draw the main diagonal, for reference\n",
        "plt.plot([0, 6], [0, 6], color='b')\n",
        "\n",
        "#labelling the axis\n",
        "plt.xlabel(\"observed values\")\n",
        "plt.ylabel(\"predicted values\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zihYASJUcVgr"
      },
      "source": [
        "#Root Mean Square Error\n",
        "print('Root Mean Square Error : ' + str(np.sqrt(sum((y-y_pred)**2)/len(y))))\n",
        "print('Target mean            : ' + str(y.mean()))\n",
        "print('Predictions mean       : ' + str(y_pred.mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CK3scgQuT6A"
      },
      "source": [
        "# Exercise: improve the network\n",
        "\n",
        "The predictions are not terrible but for sure not excellent. How to improve the performances?"
      ]
    }
  ]
}