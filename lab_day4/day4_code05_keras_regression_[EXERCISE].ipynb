{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0nH5XpufRUJ"
      },
      "source": [
        "# A neural network model for regression problems\n",
        "\n",
        "We will see here how to use Keras to implement a simple neural network model for a **regression problem** (continuous target variable).\n",
        "For this practical session we are using the [California house princing dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), exploring the relationship between house prices (in thousands of dollars) and a series of thirteen numerical properties of houses in Boston suburbs.\n",
        "\n",
        "Fortunately for us, is a very well known dataset, handily included in the [sklearn library](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwtDeoiyuo-X"
      },
      "source": [
        "# The usual seed priming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1HI3hUEIundz"
      },
      "outputs": [],
      "source": [
        "#resetting the seeds\n",
        "!wget -O support_code.py https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/lab_day1/support_code.py\n",
        "%run support_code.py\n",
        "n_seed = 10\n",
        "\n",
        "reset_random_seeds(n_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1iAUVwiDd_Z"
      },
      "source": [
        "# Loading libraries and getting the data\n",
        "\n",
        "We first load some necessary libraries and get the data from the `sklearn` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2HnsV5C2iXS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#(features, target_variable) = datasets.fetch_california_housing(return_X_y=True)\n",
        "cal = datasets.fetch_california_housing(return_X_y=False)\n",
        "\n",
        "#for data description and manipulation a pandas dataframe is handier\n",
        "#than a numpy matrix. Moreover, you can always go back using .to_numpy()\n",
        "#method\n",
        "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_numpy.html\n",
        "#features = pd.DataFrame(features)\n",
        "features = pd.DataFrame(cal.data, columns=cal.feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7NeZtClA40o"
      },
      "source": [
        "# Data description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOeaRofAA_yr"
      },
      "outputs": [],
      "source": [
        "#taking a look at the features\n",
        "features.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sIQEGu0Ak2E"
      },
      "outputs": [],
      "source": [
        "#taking a look at the target variable\n",
        "target_variable = cal.target\n",
        "plt.hist(target_variable, bins = 15)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKnMwA7ATyFv"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlxbDIHCr6oJ"
      },
      "source": [
        "## Missing values removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ju6OFDkYpaNu"
      },
      "source": [
        "It's always useful to check if something is missing, since usually regression models don't like missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZSvuQnWr5K5"
      },
      "outputs": [],
      "source": [
        "features.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLg360RiM5nj"
      },
      "source": [
        "No values are missing, but just in case the code belows takes care of holes in the features data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0TJQDZPC--D"
      },
      "outputs": [],
      "source": [
        "#fast way, but only for one data frame...\n",
        "#features = features.dropna()\n",
        "\n",
        "#finding features with at least one missing data point\n",
        "missing = features.isnull().sum(axis = 1) > 0\n",
        "\n",
        "#you cannot simply use the \"not\" operator on a series\n",
        "#it's ambigous. We need to pass through a proper\n",
        "#negation function from numpy\n",
        "not_missing = np.logical_not(missing)\n",
        "\n",
        "#subsetting\n",
        "features = features.loc[not_missing, :]\n",
        "target_variable = target_variable[not_missing]\n",
        "\n",
        "#just checking\n",
        "print(\"Size of target variable: \", target_variable.shape)\n",
        "print(\"Size of feature matrix: \", features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8UGOPjRNPuS"
      },
      "source": [
        "## Discussion: what if we are missing target values?\n",
        "\n",
        "The code above shows us how to remove samples with missing values in the `features` data frame. What if we have missing values in the target array?\n",
        "\n",
        "Pay attention: `target_variable` is a NumPy array. What kind of data should we expect?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR0PzRTZC1x0"
      },
      "source": [
        "## Discussion: outliers removal?\n",
        "\n",
        "Should we remove outliers data? If yes, do it in the space below. If no, tell us why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-XcGq-oge2s"
      },
      "outputs": [],
      "source": [
        "######## YOUR CODE HERE ########\n",
        "if True:\n",
        "  #some very refined logic...\n",
        "\n",
        "  #subsetting\n",
        "\n",
        "\n",
        "  #just checking\n",
        "\n",
        "################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r99y-Y0zk6KK"
      },
      "source": [
        "## Discussion/Exercise: data normalization?\n",
        "\n",
        "Should we normalize the features? If yes, do it in the space below. If no, tell us why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwoVhYDvlXmz"
      },
      "outputs": [],
      "source": [
        "######## YOUR CODE HERE ########\n",
        "if True:\n",
        "  #getting mean and standard deviation for train set\n",
        "\n",
        "\n",
        "  #normalizing\n",
        "\n",
        "\n",
        "  #checking if we are doing everything fine\n",
        "\n",
        "################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_a2pcQcJ-o5"
      },
      "source": [
        "# Training and validation sets\n",
        "\n",
        "We have a resonable-sized dataset, we can go with one of the standard split-validation cuts, e.g. 10%\n",
        "\n",
        "> **[PRO TIP]** In very small dataset you may be forced to do \"leave one out\" crossvalidation. For N samples you cycle selecting one sample for predictions (this would be your validation set) and keeping N-1 for training. Unfortunately in this case you cannot compute correlations (can you guess why?) and must use error-related metrics like Mean Squared Error.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgI8cFrlhwNK"
      },
      "outputs": [],
      "source": [
        "#we import a function to perform the split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## YOUR CODE\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o93lPZozLjrz"
      },
      "source": [
        "# Building the neural networks model\n",
        "\n",
        "We are now ready to build our neural networks model for regression. First, we set some hyperparameters:\n",
        "\n",
        "- the activation function in the output layer in this case is **linear**: we get directly the results from $\\sum wx + b$\n",
        "- the loss function is MSE (**Mean Squared Error**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHjBOQDRLJD1"
      },
      "outputs": [],
      "source": [
        "# Configuration options\n",
        "\n",
        "#Keras wants the data shape as a tuple. In case of images is something\n",
        "#like (image width, image height, number of channels).\n",
        "#In our case data is tabular so each data point is an\n",
        "#array (a row in the table) and we just need to specify\n",
        "#a single number, i.e. the number of columns. However, we still need\n",
        "#to input a tuple, so...\n",
        "input_shape =\n",
        "\n",
        "#other standard features describing our network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMyVxYiDMINV"
      },
      "outputs": [],
      "source": [
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Hiz909JIRn"
      },
      "source": [
        "As we learnt to do throughout the course, we now use `keras` to build our sequential neural networks model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI2dJT2AMPSq"
      },
      "outputs": [],
      "source": [
        "reset_random_seeds(n_seed)\n",
        "\n",
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT.\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes.\n",
        "from keras.layers import Dense\n",
        "\n",
        "#adding the hidden layer with the required number of nodes, plus the final\n",
        "#layer with a single node (since we want to output a single number)\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ5agM7JMVsE"
      },
      "outputs": [],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FwozHIJakU"
      },
      "source": [
        "The `model.summary()` tells us that we now have 161 model parameters to learn:\n",
        "\n",
        "- hidden layer: 8 feature weights x 16 units + 16 bias terms $\\rightarrow$ 144 parameters\n",
        "- output layer: 16 unit weights + 1 bias term $\\rightarrow$ 17 parameters  \n",
        "- 144 + 17 = 161 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhXMeMyyMbAd"
      },
      "source": [
        "# Training the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rFoY_ehMf7g"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwVjN65mMo6f"
      },
      "outputs": [],
      "source": [
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9cZN6G9MvTc"
      },
      "source": [
        "# Predictions\n",
        "\n",
        "We can now see the predictions our neural networks model produced for the house prices. Below we report also the known target values from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YrKJ0sYMxFK"
      },
      "outputs": [],
      "source": [
        "#predicting, and reporting the shape of the obtained object (also known as y hat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-l-44KDm_Pq"
      },
      "outputs": [],
      "source": [
        "#a couple of print on true target values (also known as y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jb_stcJLaR7"
      },
      "source": [
        "In regression problems you can not use the same accuracy metrics as in classification problems (e.g. error rate, confusion matrix, etc.): in stead, other metrics are used like:\n",
        "\n",
        "- **Pearson linear correlation**\n",
        "- **Spearman rank correlation**\n",
        "- **RMSE** (root mean squared error)\n",
        "- **MAE** (mean absolute error)\n",
        "- etc. (there are many more)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhs_MUYANn_r"
      },
      "outputs": [],
      "source": [
        "#changing forma to pandas Series, for handiness\n",
        "\n",
        "\n",
        "#computing correlation\n",
        "\n",
        "print(\"accuracy (measured as Pearson's correlation) is: \", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgMMwGmYC31l"
      },
      "source": [
        "\n",
        "A scatter plot of predicted vs true values can also be useful to visualise the result of the model:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UDWONAIO2Ly"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "\n",
        "\n",
        "#equivalent: plt.plot(y, y_pred, \"o\")\n",
        "\n",
        "# draw the main diagonal, for reference\n",
        "\n",
        "\n",
        "#labelling the axis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zihYASJUcVgr"
      },
      "outputs": [],
      "source": [
        "#Root Mean Square Error\n",
        "print('Root Mean Square Error : ' + str(np.sqrt(sum((y-y_pred)**2)/len(y))))\n",
        "print('Target mean            : ' + str(y.mean()))\n",
        "print('Predictions mean       : ' + str(y_pred.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CK3scgQuT6A"
      },
      "source": [
        "# Exercise: improve the network\n",
        "\n",
        "The predictions are not terrible but for sure not excellent. How to improve the performances?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}