{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xcJRPG-aWj3b",
        "tLOzrgCOw_Ud",
        "4IT60noT2DJY",
        "EXeI6VxNWQPw",
        "tHoc18fOymku",
        "-cbkT1xNy2mY",
        "wURar9IS6iqi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcJRPG-aWj3b"
      },
      "source": [
        "## Recurrent Neural Networks (RNN) with Keras\n",
        "\n",
        "- Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language: e.g. speech recognition, speech synthesis, text generation, stock price prediction\n",
        "- Schematically, a RNN layer uses a `for loop` to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far\n",
        "\n",
        "### with Keras\n",
        "\n",
        "There are three built-in RNN layers in Keras:\n",
        "1. [keras.layers.RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN): `keras.layers.SimpleRNN`\n",
        "2. [keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Long Short-Term Memory layer (Hochreiter, 1997)\n",
        "3. [keras.layers.GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU): Gated Recurrent Unit (Cho et al, 2014)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F_C2ZjiXxli"
      },
      "source": [
        "## Setting up a simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uE-pZgiWYzm"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Input\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVGCPhfDuC8V"
      },
      "source": [
        "We start with a **hypothetical example**:\n",
        "\n",
        "- audio sequence data\n",
        "- $1\\,000$ audio tracks\n",
        "- time dimension, e.g. max length = 30 seconds (30 time slices)\n",
        "- at each time slice and for each audio track we have data on *frequency*, *decibel* (intensity), *zero-crossing rate* (number of times a waveform crosses the horizontal time axis: e.g. recognition of percussive vs pitched sounds, etc.)\n",
        "\n",
        "The **input data shape** would be:\n",
        "\n",
        "$$\n",
        "(1000, 30, 3)\n",
        "$$\n",
        "\n",
        "1000 records x 30 seconds x 3 feature (**3D array**)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try with a **dense NN**:\n",
        "\n",
        "- for each audio sequence, we have $3 \\cdot 30$ features (frequency, intensity and crossing-rate for each of 30 one-second time slices)"
      ],
      "metadata": {
        "id": "TB7U8m7ISisF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_sequences = 1000\n",
        "n_features = 3\n",
        "n_time_slices = 30\n",
        "n_units = [16, 1] ## the last layers is the output (binary classification --> sigmoid)\n",
        "output_activation = 'sigmoid'"
      ],
      "metadata": {
        "id": "-iR2dTFAS6AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(tf.keras.Input(shape=(n_sequences, n_features*n_time_slices)))\n",
        "model.add(tf.keras.layers.Dense(n_units[0], activation='relu'))\n",
        "model.add(Dense(n_units[1], activation=output_activation))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7RwM2FR3RgEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (90 features x 16 units) + 16 bias_terms = 1456\n",
        "- 16 unit_outputs + 1 bias_term = 17\n",
        "\n",
        "And now with a **simple RNN model**:"
      ],
      "metadata": {
        "id": "rNwR6IiMTjuh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-guXsrOOq_6n"
      },
      "source": [
        "rnn_activation = 'tanh'\n",
        "input_shape=(n_time_slices,n_features)\n",
        "\n",
        "# SimpleRNN model\n",
        "model = Sequential()\n",
        "model.add(Input(input_shape))\n",
        "model.add(SimpleRNN(units=n_units[0], activation=rnn_activation))\n",
        "model.add(Dense(n_units[1], activation=output_activation))\n",
        "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLOzrgCOw_Ud"
      },
      "source": [
        "#### Calculating number of parameters to train\n",
        "\n",
        "We have **16 units** (**+ 1 output unit**) and **3 features** which enter in the calculations for the number of trainable parameters\n",
        "\n",
        "$$\n",
        "\\mathbf{W_u} \\left[ \\mathbf{WA^{<t-1>}_{(u,u)}}, \\mathbf{WX^{<t>}_{(u,m)}} \\right]\n",
        "$$\n",
        "\n",
        "For each time slice of each record we have:\n",
        "\n",
        "- matrix $\\mathbf{WA}$ is a square matrix of weights with dimensions (*u* x *u*), where *u* is the number of units in the RNN layer (remember: $a^{<t>}$ is an element of the sequence)\n",
        "- matrix $\\mathbf{WX}$ has dimensions (*u* x *m*), where *m* is the number of features (remember: matrix $\\mathbf{X}_{(n,m)}$ is the matrix of features, e.g. a word dictionary, or in this case the features of the sound sequence)\n",
        "\n",
        "This gives the following trainable parameters from the weight matrix $\\mathbf{W_{(u,u+m)}}$:\n",
        "\n",
        "- recurrent weights: $u \\cdot u = u^2 = 16^2 = 256$ weights\n",
        "- input weights: $u \\cdot m = 16 \\cdot 3 = 48$ weights\n",
        "- bias terms: 16 (one per unit in the RNN layer)\n",
        "- dense (output) layer: one unit x 16 input features + 1 bias term = 17\n",
        "\n",
        "**Total weights**: $256+48+16+16+1=337$\n",
        "\n",
        "<font color=\"yellow\">**¡Important!: notice that the length of the sequence does not affect the number of parameters (unlike dense layers)**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWnxRbaYX-Zc"
      },
      "source": [
        "## RNN model with embeddings\n",
        "\n",
        "!! Remember !! basic **vocabulary-based input-word representation**:\n",
        "- **vocabulary** of size `n` (n. of words in the vocabulary)\n",
        "- n-length vectors with **0's** and (a single) **1** e.g. [0, 0, 0, ..., 1, 0 , 0, ... 0]\n",
        "- one such vector per word\n",
        "- **sparse representation** (but huge matrix $\\rightarrow$ high dimensional arrays)\n",
        "\n",
        "E.g.: 15k-word vocabulary, 100 sentences, each sentence max 15-word long: array size = (15k x 15 x 100) $\\rightarrow$ over $20 \\cdot 10^6$ data points\n",
        "\n",
        "Word **embeddings** also represent words in an **array**: $\\rightarrow$ **continuous vectors**:\n",
        "- represent any word in *few dimensions* (mostly based on the number of unique words in the text)\n",
        "- **dense representation** $→$ low dimensional vectors\n",
        "- not hardcoded but **learned** from the data $\\rightarrow$ this is a **key feature** of **embeddings** (like *autoencoders*, automatic extraction of features from the data)\n",
        "\n",
        "E.g. 15k-word vocabulary, 32 NN units $\\rightarrow$ (15k, 32) array (dense) $480\\,000$ data points\n",
        "\n",
        "embedding of a single word over *m* (e.g. 32) dimensions: [-0.054, 0.768, 0.003, 0.832, -0.101, ..., 0.923, -0.509]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://github.com/ne1s0n/coding_excercises/blob/master/data/embeddings.png?raw=true\">\n",
        "From: https://towardsdatascience.com/word-embeddings-and-the-chamber-of-secrets-lstm-gru-tf-keras-de3f5c21bf16\n",
        "\n",
        "- geometric relationship (in the m-dimensional hyperspace) between words in word embeddings can represent **semantic relationships**\n",
        "- **words closer to each other** (m-dimensional distances) have **stronger relation** compared to words away from each other\n",
        "- there could be vector 'male to female' which represents the relation between a word and its feminine: may help predict 'king' when 'he' is encountered and 'queen' when 'she' is encountered in the sentence.\n"
      ],
      "metadata": {
        "id": "-zryD80NsdjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple example of a `Sequential` model that processes text sequences (e.g. words), embeds each sequence into a 15000-dimensional vector (vocabulary size), then processes the sequence of vectors using a `Simple RNN` layer:\n",
        "\n",
        "- 100 sentences (text sequences) [n. of records]\n",
        "- max length of each sequence: 30 words [\"time\" (longitudinal) dimension]\n",
        "- vocabulary size: $15\\,000$ words [n. of features]"
      ],
      "metadata": {
        "id": "WluSztBZkGG_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJHoW8RWfOFq"
      },
      "source": [
        "## parameters\n",
        "embed_input_size = 15000 ## input vocabulary size\n",
        "embed_output_size = 32 ## n. of hidden units (nodes)\n",
        "rnn_units = 16\n",
        "dense_units = 1 ## e.g. classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWBPNPwpepzU"
      },
      "source": [
        "model = keras.Sequential() ## not unlike usual NN models (including CNN)\n",
        "\n",
        "model.add(Input((100,))) ## Input sentences of max length 100 words (this does not affect the n. of parameters)\n",
        "# Add an Embedding layer expecting input vocab of size XXX, and\n",
        "# output embedding dimension of size ZZZ.\n",
        "model.add(layers.Embedding(input_dim=embed_input_size, output_dim=embed_output_size)) ## this is unlike dense NN models (model.add(Dense()))\n",
        "\n",
        "# Add a RNN layer with 16 internal units.\n",
        "model.add(layers.SimpleRNN(rnn_units))\n",
        "\n",
        "# Add a Dense layer with 1 units.\n",
        "model.add(layers.Dense(dense_units))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IT60noT2DJY"
      },
      "source": [
        "#### Calculating number of parameters to train\n",
        "\n",
        "- **embedding layer**: $15\\,000 \\cdot 32 = 480\\,000$\n",
        "- **RNN layer**: $16 \\cdot 16 + 16 \\cdot 32 + 16 = 784$\n",
        "- **output layer**: $16 \\cdot 1 + 1 = 17$\n",
        "\n",
        "**Total weights:** $480\\,000 + 784 + 17 = 480\\,801$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmOnaYL4czoL"
      },
      "source": [
        "---\n",
        "# FROM HERE ON, IT IS ONLY FOR THE VERY BRAVE!\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhwJoRtPTc3m"
      },
      "source": [
        "### Short-term memory\n",
        "\n",
        "RNN suffer from short-term memory problems: if a time series is long, RNN have difficulties in carrying information from earlier timepoints over to later timepoints. Specifically, in back propagation RNN experience the **vanishing gradient problem**, i.e. the gradient (values used to update NN weights) shrinks over successive backpropagation steps, and if it becomes too small it won't contribute to learning:\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\alpha \\cdot \\frac{\\partial}{\\partial w_t}J(w) = 2.1 - 0.1 \\cdot 0.001 = 2.0999\n",
        "$$\n",
        "\n",
        "(not much of a difference!)\n",
        "\n",
        "Therefore, RNN can forget what they saw early in the sequence $\\rightarrow$ **short-term memory!**\n",
        "\n",
        "#### Gates in RNN\n",
        "\n",
        "To address issues with short-term memory, RNN use internal structures (sublayers) called **gates**. Gates can learn which data in a sequence are important to keep or throw away and pass them on along the chain of sequences.\n",
        "It is s bit like remembering only those words in an ad that struck your memory (e.g. the price, the deadline, the main characteristics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXeI6VxNWQPw"
      },
      "source": [
        "#### Inside a RNN\n",
        "\n",
        "- words (or sounds) transformed to vectors\n",
        "- the RNN processes each sequence of vectors one by one, passing hidden states (units) sequentially to the next steps: in this way, the RNN holds information seen in the each previous step\n",
        "- the input vector (word) and previous hidden state are combined to form a new vector that has information on the current and previous inputs\n",
        "- the combined vector goes through the activation function (e.g. `tanh`) and the output is the new hidden state to be combined with the input to be processed by he next unit in the layer\n",
        "- `tanh` squeashes values from the linear combination of the combined vector values in input (input + hidden state) to the range $[-1,1]$   \n",
        "\n",
        "Simple RNN (no *gates*, or better only one gate) use much less computational resources than the evolved variants: **LSTM** and **GRU**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRytvTALjVBr"
      },
      "source": [
        "#### LSTM\n",
        "\n",
        "Unlike simple RNN, the operations inside a LSTM cell (sublayer/unit) allow to keep or forget pieces of information. In this way, also information from earlier time steps can make its way to later time steps, reducing the effects of short-term memory. In this journey, information can be added or removed through **gates** (typically **4** in LST layers).\n",
        "LSTM gates (sublayers) use the **sigmoid** activation function, in $[0,1]$, which permits to 'forget' information by returning 0, or to keep it by returning 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbjClprWwNZ7"
      },
      "source": [
        "- **forget gate**: previous hidden state + input vector $\\rightarrow$ `sigmoid` activation: `if sigmoid(h+x) ~ 0 then forget; else if sigmoid(h+x) ~ 1 then keep`\n",
        "- **input gate(s)**: this typically has two units, one with `sigmoid` and one with `tanh` activation functions; this is where the updating of model weights happens. Each unit receives the previous hidden state + the input vector:\n",
        "  1. one unit decides on importance (which values to update)\n",
        "  2. the other unit updates the weights\n",
        "- **output gate**: this unit combines results from the forget and input gates to produce the output that is passed on to the next times step (next sublayer/unit in the LSTM layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHoc18fOymku"
      },
      "source": [
        "### GRU\n",
        "\n",
        "GRU (gated recurrent unit), compared to LSTM gets rid of `cell state` and use only the `hidden state` to transfer information (LSTM combines cell states and hidden states to pass on information through layers):\n",
        "\n",
        "- **reset gate**: controls how much past information to forget\n",
        "- **update gate**: controls what information to throw away and what new information to add\n",
        "- **output gate**: passes the new hidden state on to the next time step (next unit in the GRU layer)\n",
        "\n",
        "GRU layers are similar to LSTM layers, but a little faster (fewer tensor operations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkdfnSD_25-O"
      },
      "source": [
        "## LSTM layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBgXUqtrgnAa"
      },
      "source": [
        "Let's calculate the n. of parameters in LSTM layers:\n",
        "- insight about how LSTM handles time dependent or sequence input data\n",
        "- model capacity and complexity:\n",
        "    - handle overfitting or underfitting\n",
        "    - adjust the number of parameters of each layer\n",
        "\n",
        "`LSTM` expects input data to be a `tensor` such that:\n",
        "\n",
        "`[batch_size, timesteps, feature]`\n",
        "\n",
        "1. `n. of records` (`batch_size`): how many samples (in each batch) during training and testing, e.g. number of sequences to be processed\n",
        "2. `timesteps`: how many values in a sequence, e.g. in `[4, 7, 8, 4]` there are 4 timesteps (30 words max in our text processing example)\n",
        "3. `features`: how many dimensions to represent data in one time step; e.g. if each digit value in the sequence is one hot encoded with 9 zero and 1 one then feature is 10, or size of the vocabulary for sentences/words (or output of previous layer)\n",
        "\n",
        "`LSTM` layers have **4 gates** in their internal structure:\n",
        "\n",
        "- *forget* gate\n",
        "- *update* gate\n",
        "- *candidate* gate\n",
        "- *output* gate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtqF8cbwtw__"
      },
      "source": [
        "#### Illustration of a LSTM layer\n",
        "\n",
        "<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUt_W5SJWdGh"
      },
      "source": [
        "embed_input_size = 15000 ## input vocabulary size\n",
        "embed_output_size = 32 ## n. of hidden units (nodes)\n",
        "lstm_units = 16\n",
        "lstm_activation = 'relu'\n",
        "dense_units = 1\n",
        "output_acrivation = 'sigmoid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaDdstjyWVBU"
      },
      "source": [
        "model = keras.Sequential() ## not unlike usual NN models (including CNN)\n",
        "\n",
        "# Add an Embedding layer expecting input vocab of size 1000, and\n",
        "# output embedding dimension of size 32.\n",
        "model.add(layers.Embedding(input_dim=embed_input_size, output_dim=embed_output_size)) ## this is unlike dense NN models (model.add(Dense()))\n",
        "\n",
        "# Add a RNN layer with 16 internal units.\n",
        "model.add(layers.LSTM(lstm_units, activation = lstm_activation))\n",
        "\n",
        "# Add a Dense layer with 1 units.\n",
        "model.add(layers.Dense(dense_units, activation = output_acrivation))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPdXJlyhwoBZ"
      },
      "source": [
        "- **i**: 32 (output from the embedding layer)\n",
        "- **h**: 16 LSTM units\n",
        "- **g**: number of gates in each LSTM unit\n",
        "\n",
        "$ [((i+h)×h) + (h)] \\cdot g$\n",
        "\n",
        "The LSTM units (e.g. 16 from the example above) are added to the input units (32, from the example above) and multiplied by the number of LSTM units; the corresponding bias terms are then added (16, one per LSTM unit):\n",
        "\n",
        "$$\n",
        "(32+16) \\cdot 16 + 16 = 784\n",
        "$$\n",
        "\n",
        "This is multiplied by the number of internal gates (4):\n",
        "\n",
        "$$\n",
        "784 \\cdot 4 = 3\\,136\n",
        "$$\n",
        "\n",
        "This is how the number of parameters in a LSTM layer is calculated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omOzPYf1tvVv"
      },
      "source": [
        "You can also think in terms of gates: each gate within a unit receives the 32 + 16 input data and has 1 bias term\n",
        "\n",
        "$(32+16)+1 = 49$\n",
        "\n",
        "There are 4 gates in a LSTM unit: 49 x 4 = 196\n",
        "\n",
        "We have 16 LSTM units (nodes) in our LSTM layer: 196 x 16 = 3136"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjrMGCHtKRJJ"
      },
      "source": [
        "### GRU layers\n",
        "\n",
        "\n",
        "A similar way of reasoning as with LSTM layers is used also for GRU layers: one important difference is that GRU units have **3 gates** (*candidate*, *update*, *relevance*) instead of 4.\n",
        "\n",
        "We can therefore calculate the number of trainable parameters as:\n",
        "\n",
        "$$\n",
        "[((i+h)×h)+(h)] \\cdot g\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pluralsight2.imgix.net/guides/c02c6196-7452-4095-9215-c4d57a9dd1a4_1.JPG\" width=\"700\">"
      ],
      "metadata": {
        "id": "65r62Kk0013B"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8a_c9CzubnR"
      },
      "source": [
        "embed_input_size = 15000 ## input vocabulary size\n",
        "embed_output_size = 32 ## n. of hidden units (nodes)\n",
        "gru_units = 16\n",
        "gru_activation = 'tanh'\n",
        "dense_units = 1\n",
        "output_acrivation = 'sigmoid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujq_gVQs02Ss"
      },
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(input_dim=embed_input_size, output_dim=embed_output_size))\n",
        "\n",
        "# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 16)\n",
        "model.add(layers.GRU(gru_units, activation=gru_activation, return_sequences=True))\n",
        "\n",
        "model.add(layers.Dense(dense_units, activation=output_acrivation))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7zGJ9fN5Uh7"
      },
      "source": [
        "$3 \\cdot [(32+16) \\cdot 16 + 16] = 2\\,352$\n",
        "\n",
        "We see that the calculations give 2352 parameters, which is less that the 2400 parameters computed by `Keras`: there are **48 parameters missing**\n",
        "\n",
        "This is because some RNN implementations, like `Keras` and `PyTorch` for efficiency reasons overparameterise the model by adding an extra bias term to each GRU gate:\n",
        "\n",
        "$$\n",
        "3 \\cdot 16 = 48\\\\\n",
        "2352 + 48 = 2400\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cbkT1xNy2mY"
      },
      "source": [
        "#### To recap GRU layers parameters\n",
        "\n",
        "1. first, you sum the size of the input vector (original data) and the hidden state (n. of units in the GRU layer): `input_vector` + `hidden_state`: $(h+i)$\n",
        "2. then you multiply the total imput size by the number of unit in the GRU layer: $h \\cdot (h+i)$\n",
        "3. then you add the bias terms, one per unit: $h \\cdot (h+i) + h$\n",
        "4. then you multiply by the number of gates (typically three in GRU layers): $g \\cdot [h \\cdot (h+i)+h]$\n",
        "5. finally, you add additional bias terms, as many as the product of gates and units: $g \\cdot [h \\cdot (h+i)+h]+g \\cdot h$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wURar9IS6iqi"
      },
      "source": [
        "## A little excercise: train yourself on trainable parameters\n",
        "\n",
        "Hypothetical example: ECG data (electrocardiography)\n",
        "\n",
        "- 100 patients\n",
        "- 60 seconds\n",
        "- 1 features: voltage\n",
        "\n",
        "Build a RNN model for heart disease diagnosis:\n",
        "\n",
        "- 1 simple RNN layer\n",
        "- 1 GRU layer\n",
        "- 1 LSTM layer\n",
        "- 1 output layer\n",
        "\n",
        "Work out the number of parameters"
      ]
    }
  ]
}