{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m57JSLtUSdeg"
      },
      "source": [
        "# Underfitting and overfitting\n",
        "\n",
        "![under_over_fitting](https://i.imgur.com/eP0gppr.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI8VJjRKc7h-"
      },
      "source": [
        "![under_over_fit](https://i.imgur.com/eUF6mfo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-zgnkwvdAIW"
      },
      "source": [
        "## Illustration with examples\n",
        "\n",
        "This dataset contains 13 attributes and one target variable, that refers to the presence of heart disease in the patient as an integer value from 0 (no presence) to 4 (severe heart disease)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-rcMLd4azQF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.metrics\n",
        "\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SktJxwudSaDz"
      },
      "source": [
        "DATASET_URL = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/processed.cleveland.data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH0q52Rub_4-"
      },
      "source": [
        "### Reading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EudCRCqGSP-"
      },
      "source": [
        "heart_data = pd.read_csv(DATASET_URL)\n",
        "heart_data.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','disease_severity']\n",
        "# heart_data = heart_data.iloc[:,[2,3,4,5,6,7,8,9,10,11,12]]\n",
        "print(heart_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPVAbCp1cN6t"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Remove missing data, clean and subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fugitldsRaEF"
      },
      "source": [
        "heart_data = heart_data.dropna()\n",
        "heart_data = heart_data[(heart_data['ca'] != '?') & (heart_data['thal'] != '?')]\n",
        "print(len(heart_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thi1ZgImSKzu"
      },
      "source": [
        "heart_data['disease_severity'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfQekY7rL9Bj"
      },
      "source": [
        "#### Normalize features\n",
        "\n",
        "(this is suboptimal: do you remember why?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI_2slL2BENF"
      },
      "source": [
        "target_variable = heart_data[[\"disease_severity\"]]\n",
        "features = heart_data.loc[:, heart_data.columns != 'disease_severity']\n",
        "print(\"Size of target variable: \", target_variable.shape)\n",
        "print(\"Size of feature matrix: \", features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0iQOCOKPAsI"
      },
      "source": [
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yvWeigaW5uc"
      },
      "source": [
        "#the \"utils\" subpackage is very useful, take a look to it when you have time\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#converting to categorical\n",
        "target_multi_cat = to_categorical(target_variable)\n",
        "\n",
        "#since everything else is a Pandas dataframe, let's stick to the format\n",
        "#for consistency\n",
        "# target_multi_cat = pd.DataFrame(target_multi_cat)\n",
        "\n",
        "#let's take a look\n",
        "print(target_multi_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x44DPPqkQdX6"
      },
      "source": [
        "features = features.astype({'ca':float, 'thal':float})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTiClDBnL-_r"
      },
      "source": [
        "features=(features-features.mean())/features.std()\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrGiHb7mBHtU"
      },
      "source": [
        "### Training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## setting seeds\n",
        "np.random.seed(600)\n",
        "\n",
        "import random\n",
        "random.seed(13)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(13) ## 166\n",
        "tf.config.experimental.enable_op_determinism()"
      ],
      "metadata": {
        "id": "Pb2MRHEhL1ZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqio8Oc2BKDC"
      },
      "source": [
        "#we import a function to perform the split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(features, target_multi_cat, test_size=0.20)\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_test.shape)\n",
        "print(target_train.shape)\n",
        "print(target_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(target_train, columns = ['0','1','2','3','4'])\n",
        "df.sum()"
      ],
      "metadata": {
        "id": "hEUrS9DsIQ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(target_test, columns = ['0','1','2','3','4'])\n",
        "df.sum()"
      ],
      "metadata": {
        "id": "U9HTXwNFT3d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUim6pRehwv4"
      },
      "source": [
        "### Tiny model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxMEW8biBW4d"
      },
      "source": [
        "## # Configuration options\n",
        "input_shape = (features_train.shape[1],) ## tuple that specifies the number of features\n",
        "num_classes = 5\n",
        "loss_function = 'categorical_crossentropy'\n",
        "optimizer_used = 'rmsprop' ## or keras.optimizers.adam(lr=0.001)? maybe for softmax regression?\n",
        "num_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjEl_BViBeR0"
      },
      "source": [
        "# softmax regression shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(units=4, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fDGt85hhxvo"
      },
      "source": [
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function, metrics=keras.metrics.CategoricalAccuracy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I-jdGAmh1G-"
      },
      "source": [
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_test, target_test), verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb491nmpCPU8"
      },
      "source": [
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znnsAG_ZCUJW"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "predictions = model.predict(features_test)\n",
        "print(\"predictions:\")\n",
        "print(predictions[0:4])\n",
        "\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n",
        "\n",
        "target_classes = np.argmax(target_test, axis=1)\n",
        "\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2,3,4])\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Ubwg74ChZl"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(target_classes, predicted_classes)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "confusion_matrix(target_classes, predicted_classes, normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnRERBpDSVN"
      },
      "source": [
        "## Small model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYzq9sZ3DTvU"
      },
      "source": [
        "num_epochs = 100\n",
        "\n",
        "# softmax regression shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(units=4, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(units=8, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz76JoL2DyJ8"
      },
      "source": [
        "model.compile(optimizer=optimizer_used, loss=loss_function, metrics=keras.metrics.CategoricalAccuracy())\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_test, target_test), verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ2tJN3TD5Qd"
      },
      "source": [
        "plot_loss_history(history, 'Logistic ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYuhn80ND_EW"
      },
      "source": [
        "predictions = model.predict(features_test)\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n",
        "\n",
        "target_classes = np.argmax(target_test, axis=1)\n",
        "\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2,3,4])\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqiYKT02ECuL"
      },
      "source": [
        "accuracy = accuracy_score(target_classes, predicted_classes)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "confusion_matrix(target_classes, predicted_classes, normalize='true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At2rMEUGEG8k"
      },
      "source": [
        "### Medium model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik-dGCCdEGhO"
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=16, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(units=32, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(units=16, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q6IV_BMETIF"
      },
      "source": [
        "model.compile(optimizer=optimizer_used, loss=loss_function, metrics=keras.metrics.CategoricalAccuracy())\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_test, target_test), verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDd_v73lEWoF"
      },
      "source": [
        "plot_loss_history(history, 'Softmax ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKlMlpKlEZn-"
      },
      "source": [
        "predictions = model.predict(features_test)\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n",
        "\n",
        "target_classes = np.argmax(target_test, axis=1)\n",
        "\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2,3,4])\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(target_classes, predicted_classes)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "confusion_matrix(target_classes, predicted_classes, normalize='true')"
      ],
      "metadata": {
        "id": "lp4WU7W4DjYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANTLkTzfEodW"
      },
      "source": [
        "### Large model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lo7yiO1EqCF"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(units=32, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(units=64, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(units=128, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(units=64, input_shape=input_shape, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukzPOy04E2jE"
      },
      "source": [
        "num_epochs = 100\n",
        "\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function, metrics=keras.metrics.CategoricalAccuracy())\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_test, target_test), verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OioRxFlE5CW"
      },
      "source": [
        "plot_loss_history(history, 'Softmax ({} epochs)'.format(num_epochs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbWP2mMuE8En"
      },
      "source": [
        "predictions = model.predict(features_test)\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n",
        "\n",
        "target_classes = np.argmax(target_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(target_classes, predicted_classes)\n",
        "print(\"Overall accuracy is: \", accuracy)\n",
        "\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2,3,4])\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2,3,4], normalize='true')\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "metadata": {
        "id": "0FCLSxEcWbzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A second example [OPTIONAL]"
      ],
      "metadata": {
        "id": "LIKCqHFlEEBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_URL = 'https://raw.githubusercontent.com/ne1s0n/coding_excercises/master/data/DNA_methylation_data.csv'"
      ],
      "metadata": {
        "id": "MsO1Q9XTEFd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas can read a csv directly from a url\n",
        "bat_data = pd.read_csv(DATASET_URL)\n",
        "print(bat_data)"
      ],
      "metadata": {
        "id": "qmfUnyt7EJi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bat_data = bat_data.iloc[:,[1,3,4,5,6,7,8,9]]\n",
        "print(bat_data.head())\n",
        "print(\"N. of records is: \",len(bat_data))"
      ],
      "metadata": {
        "id": "9j1i774uES76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Removing missing data"
      ],
      "metadata": {
        "id": "y1E4vYStEiC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bat_data = bat_data.dropna()"
      ],
      "metadata": {
        "id": "JnAYKD_gEufW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}