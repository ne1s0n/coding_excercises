{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-TykfLxN6_L"
      },
      "source": [
        "# Chest X ray problem\n",
        "\n",
        "Binary classification problem, we are asked to classify chest X rays from patients and tell which are sick and which are healty.\n",
        "\n",
        "The dataset is deposited [here](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia).\n",
        "\n",
        "For a \"heavy guns\" solution for this problem (with data augmentation, learning rate decay, memory optimization and other neat advanced stuff) see [here](https://www.kaggle.com/amyjang/tensorflow-pneumonia-classification-on-x-rays)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "066Hk1V5_AP2"
      },
      "source": [
        "# Config\n",
        "\n",
        "These constants are given for the exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiaFDmq3-2qq"
      },
      "source": [
        "#where the data are stored\n",
        "data_url = 'http://www.jackdellequerce.com/data/reduced_chest_xray.zip'\n",
        "\n",
        "#where to place the data\n",
        "download_target_imgs = '/content/data/'\n",
        "base_dir = download_target_imgs + 'reduced_chest_xray/'\n",
        "\n",
        "#Keras constants\n",
        "BATCH_SIZE = 20\n",
        "IMAGE_SIZE = [128, 128]\n",
        "IMAGE_SHAPE = (IMAGE_SIZE[0], IMAGE_SIZE[1] , 3)\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkcf_AVX_l9D"
      },
      "source": [
        "# Data setup\n",
        "\n",
        "The following code ensures that the images are present in `base_dir` folder. If the data is not there it is downloaded and unpacked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWSI4K5-_nG1"
      },
      "source": [
        "import glob     #for checking dir content\n",
        "import os       #for dir creation\n",
        "import requests #for data download\n",
        "import zipfile  #for unpacking zipped files\n",
        "\n",
        "#these two lists should contain the full paths of all train and test images\n",
        "train_filenames = glob.glob(base_dir + 'train/*/*')\n",
        "validation_filenames   = glob.glob(base_dir + 'test/*/*')\n",
        "\n",
        "#let's check that we actually have the data\n",
        "if len(train_filenames) == 0 or len(validation_filenames) == 0:\n",
        "  #either the data was never downloaded or something bad happened\n",
        "  #in any case, we donwload and unzip everything\n",
        "\n",
        "  #room for data\n",
        "  os.makedirs(download_target_imgs, exist_ok=True)\n",
        "\n",
        "  #downloading\n",
        "  r = requests.get(data_url)\n",
        "  open(download_target_imgs + 'local_archive.zip', 'wb').write(r.content)\n",
        "\n",
        "  #unpacking\n",
        "  z = zipfile.ZipFile(download_target_imgs + 'local_archive.zip')\n",
        "  z.extractall(path = download_target_imgs)\n",
        "\n",
        "  #at this point data is there, we are ready to get the list of files\n",
        "  train_filenames = glob.glob(base_dir + 'train/*/*')\n",
        "  validation_filenames   = glob.glob(base_dir + 'test/*/*')\n",
        "\n",
        "#whatever the original case, at this point we have the files\n",
        "print('Available images for train: ' + str(len(train_filenames)))\n",
        "print('Available images for validation: ' + str(len(validation_filenames)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVybJWNWBKdZ"
      },
      "source": [
        "# Data loading - ImageDataGenerator\n",
        "\n",
        "We are going to use [ImageDataGenerator](https://keras.io/api/preprocessing/image/#imagedatagenerator-class) for loading the images from the local memory.\n",
        "\n",
        "`ImageDataGenerator` class is defined in `keras.preprocessing.image` submodule and can be used for data augmentation and general data handling.\n",
        "\n",
        "When working with images it is important to rescale them in from the 0-255 range to 0-1, as explained in details [here](https://github.com/Arsey/keras-transfer-learning-for-oxford102/issues/1\n",
        ").\n",
        "\n",
        "**ASSIGNMENT**: In the next snippet you need to import `ImageDataGenerator` and then declare two objects named `train_datagen` and `validation_datagen`. Both need a rescaling factor or 1.0/255<br>\n",
        "The \".0\" part is important so python will do a floating point division and not an integer division, in fact:\n",
        "\n",
        "- 1.0/255 → 0.003921569\n",
        "- 1/255 → 0\n",
        "\n",
        "PS: the above was strictly true for Python 2.xx; with Python $\\ge$ 3.0 integer division is solved automatically if fractional (still it is a good practice to be explicit with the type-casting of variables)\n",
        "\n",
        "**OPTIONAL ASSIGNMENT**: do some data augmentation telling `train_datagen` to do horizontal flips. Important: never do data augmentation on validation set. Can you guess what the consequences would be?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A8XsNRC_xZF"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#declare two objects\n",
        "######################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh56-KGTbMpu"
      },
      "source": [
        "In so far you have declared the `ImageDataGenerator` objects, but you have not linked them to the actual files. It's now time to do so.\n",
        "\n",
        "**ASSIGNMENT** use the [flow_from_directory()](https://keras.io/api/preprocessing/image/#flowfromdirectory-method) method to link the `validation_datagen` object to its folder.\n",
        "\n",
        "**NOTE**: parameter `batch_size` on train dataset influences the amount of memory required. Usually the bigger the better, but the system can easily become overloaded. On validation dataset it is **important** to give a number that exactly divides the number of available samples, otherwise some samples will never be used. When in doubt put it to 1 (only for validation).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28NTVc-UBWK"
      },
      "source": [
        "#let's put the dirs in a handy place\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "#linking the actual data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # This is the target directory\n",
        "        train_dir,\n",
        "        # All images will be resized\n",
        "        target_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        # We have two classes, we'll work in binary mode\n",
        "        class_mode='binary')\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsJe3QdjFQFg"
      },
      "source": [
        "# Architecture - Define\n",
        "\n",
        "It's now time to define an architecture. You'll use the usual [Sequential model](https://keras.io/guides/sequential_model/).\n",
        "\n",
        "**ASSIGNMENT**: Declare a model with the following layers:\n",
        "\n",
        "1. [Conv2D layer](https://keras.io/api/layers/convolution_layers/convolution2d/), 32 nodes, 3x3 kernel, \"same\" padding, \"relu\" activation\n",
        "2. [MaxPooling2D layer](https://keras.io/api/layers/pooling_layers/max_pooling2d/), 2x2 pool size\n",
        "3. another Conv2D layer, this time 64 nodes, everything else same as above\n",
        "4. another MaxPooling2D, same as above\n",
        "5. [Flatten layer](https://keras.io/api/layers/reshaping_layers/flatten/)\n",
        "6. [Dense layer](https://keras.io/api/layers/core_layers/dense/), used as output, \"sigmoid\" activation function. Can you guess the number of nodes?\n",
        "\n",
        "The first layer is the input layer and requires also an `input_shape`, which you find in the `IMAGE_SHAPE` constant you declared above.\n",
        "\n",
        "**OPTIONAL ASSIGNMENT** put a Dropout layer after the first MaxPooling2D with a rate of 0.2. Do you need to change something else in the code?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "824q9YfpCyvz"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "#let's declare an empty model\n",
        "model = Sequential()\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eko67WVQFYvR"
      },
      "source": [
        "# Architecture - Take a look\n",
        "\n",
        "You have defined the architecture, it's time to take a look at your work. Keras offers two options:\n",
        "\n",
        "* the [.summary()](https://keras.io/api/models/model/#summary-method) method built-in your model object\n",
        "* [plot_model()](https://keras.io/api/utils/model_plotting_utils/#plotmodel-function) function from `keras.utils.vis_utils` package.\n",
        "\n",
        "**ASSIGNMENT** invoke either `.summary()` (easy) or `plot_model()` (more complicated), take a look inside your model and verify that everything is as expected in terms of number of layers, output size and so forth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd0J3rW9FaN8"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "# 1. summary\n",
        "\n",
        "# 2.\n",
        "######################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv6BvycrG16o"
      },
      "source": [
        "# Architecture - Compile\n",
        "\n",
        "So far you have defined the topology of your network, it's now time to specify how you are going to measure its performance.\n",
        "\n",
        "**ASSIGNMENT** invoke the [.compile()](https://keras.io/api/models/model_training_apis/#compile-method)  method for your model, specifying the [loss function](https://keras.io/api/losses/) (we are doing binary classification, so 'binary_crossentropy' is the standard choice)\n",
        "\n",
        "**OPTIONAL ASSIGNMENT 1**: ask keras to keep track of an extra metric, 'accuracy'. Keep in mind that `.compile()` expects a list of strings when specifying metrics, even if only one element is present.\n",
        "\n",
        "**OPTIONAL ASSIGNMENT 2**: default optimizer is [RMSprop](https://keras.io/api/optimizers/rmsprop/), with a default learning rate of 0.001. Declare the optimizer so that the used learning rate is 0.00002."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKB2yjhDG4hy"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmZxgrc2GNTE"
      },
      "source": [
        "# Train\n",
        "\n",
        "It's finally time to train your model using the [.fit()](https://keras.io/api/models/model_training_apis/#fit-method) method.\n",
        "\n",
        "**ASSIGNMENT**: train your model speifying that:\n",
        "\n",
        "* your train data (argument `x`) is in `train_generator`\n",
        "* your validation data (argument `validation_data`) is in `validation_generator`\n",
        "* the desider number of epochs (argument `epochs`) is in the declared constant `EPOCHS`\n",
        "* `verbose` level = 2 so that we can take a look at what's happening\n",
        "\n",
        "The returned object should go in a new variable called `train_log`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTGAZgNkGFJt"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yltaSPLFLV0X"
      },
      "source": [
        "# Support function for plotting metrics\n",
        "\n",
        "The following function is a small utility that allows for plotting loss and all the metrics returned by a `.fit()` call. Just execute the snippet so that the function is declared. Or, if you are curious, take a look at the code :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57KmwRHKpN8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss_history(h, title):\n",
        "  for metric in h.history.keys():\n",
        "    #ignoring metrics on validation set, which are implied when\n",
        "    #plotting on training set\n",
        "    if metric.startswith('val_'):\n",
        "      continue\n",
        "\n",
        "    #if we get here we found a metric on the training set,\n",
        "    #let's plot it\n",
        "    plt.plot(h.history[metric], label = \"Train set\")\n",
        "    plt.plot(h.history[\"val_\" + metric], label = \"Validation set\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title + ' - ' + metric)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHr7PqtFLd0E"
      },
      "source": [
        "# Plotting your model performances\n",
        "\n",
        "As a final step, plot loss (and metrics, if present) of your training.\n",
        "\n",
        "**ASSIGNMENT**: use the `plot_loss_history()` you just declared to plot the evolution of your training. What consideration can you do?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHMOMfuKLRd_"
      },
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}